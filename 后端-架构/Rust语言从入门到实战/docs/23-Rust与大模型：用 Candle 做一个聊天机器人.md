ä½ å¥½ï¼Œæˆ‘æ˜¯Mikeã€‚ä»Šå¤©æˆ‘ä»¬æ¥èŠä¸€èŠå¦‚ä½•ç”¨Ruståšä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººã€‚

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯2023å¹´æœ€ç«çš„é¢†åŸŸï¼Œæ²¡æœ‰ä¹‹ä¸€ã€‚è¿™æ³¢çƒ­æ½®æ˜¯ç”±OpenAIçš„ChatGPTåœ¨ä»Šå¹´ä¸ŠåŠå¹´å‘å¸ƒåå¼•èµ·çš„ï¼Œä¹‹åå…¨ä¸–ç•Œçš„ç ”ç©¶æœºæ„å’Œç§‘æŠ€å…¬å¸éƒ½å·å…¥äº†å¤§æ¨¡å‹çš„ç«äº‰ä¸­ã€‚ç›®å‰ä¸šç•Œåº”ç”¨å¤§æ¨¡å‹è®­ç»ƒåŠæ¨ç†çš„ä¸»è¦è¯­è¨€æ˜¯Pythonå’ŒC/C++ã€‚Pythonä¸€èˆ¬ç”¨æ¥å®ç°ä¸Šå±‚æ¡†æ¶ï¼Œè€ŒC/C++ä¸€èˆ¬èµ·åº•å±‚é«˜æ€§èƒ½æ‰§è¡Œçš„ä½œç”¨ï¼Œæ¯”å¦‚è‘—åçš„æ¡†æ¶ PyTorchï¼Œå®ƒçš„ä¸Šå±‚ä¸šåŠ¡å±‚é¢æ˜¯ç”¨Pythonå†™çš„ï¼Œä¸‹å±‚æ‰§è¡Œå±‚é¢ç”±Cæ‰§è¡Œï¼Œå› ä¸ºGPUåŠ é€Ÿçš„éƒ¨åˆ†åªèƒ½ç”±C/C++æ¥è°ƒç”¨ã€‚

çœ‹èµ·æ¥å¤§è¯­è¨€æ¨¡å‹å¥½åƒå’ŒRustæ²¡ä»€ä¹ˆå…³ç³»ï¼Œå¿…é¡»æ‰¿è®¤ï¼Œç”±äºå†å²ç§¯ç´¯çš„åŸå› ï¼Œåœ¨AIè¿™ä¸€å—å„¿Rustçš„å½±å“åŠ›è¿˜éå¸¸å°ã€‚ä½†ä»å¦ä¸€æ–¹é¢æ¥è®²å‘¢ï¼ŒRustæ˜¯ç›®å‰ä¸šç•Œé™¤Pythonã€C++ å¤–ï¼Œå”¯ä¸€æœ‰æ½œåŠ›åœ¨æœªæ¥20å¹´çš„AI å‘å±•ä¸­å‘æŒ¥é‡è¦ä½œç”¨çš„è¯­è¨€äº†ã€‚ä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿ

é¦–å…ˆRustçš„æ€§èƒ½ä¸C/C++ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨è°ƒç”¨GPUèƒ½åŠ›æ–¹é¢ä¹ŸåŒæ ·æ–¹ä¾¿ï¼›å…¶æ¬¡ï¼ŒRustå¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¸è¾“äºPythonï¼Œè¿™è®©äººä»¬ä½¿ç”¨Ruståšä¸šåŠ¡å¹¶ä¸éš¾ï¼›ç„¶åï¼ŒRustçš„cargoç¼–è¯‘æˆå•æ–‡ä»¶çš„èƒ½åŠ›ï¼Œä»¥åŠå¯¹WebAssemblyå®Œå–„çš„æ”¯æŒï¼Œéƒ¨ç½²åº”ç”¨çš„æ—¶å€™éå¸¸æ–¹ä¾¿ï¼Œè¿™æ¯”Py + C/C++ç»„åˆéœ€è¦å®‰è£…çš„ä¸€å †ä¾èµ–å’Œæ•°Gçš„åº“æ–¹ä¾¿å¤ªå¤šã€‚

ç›®å‰Rustç”Ÿæ€ä¸­å…¶å®å·²ç»æœ‰å¾ˆå¤šAIç›¸å…³çš„åŸºç¡€è®¾æ–½äº†ï¼Œä½ å¯ä»¥ä»æˆ‘ç»™å‡ºçš„[é“¾æ¥](https://www.arewelearningyet.com/)é‡Œæ‰¾åˆ°ã€‚

ä¸–ç•Œä¸Šæœ€å¤§çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä»“åº“å¹³å° HuggingFaceï¼ˆæœºå™¨å­¦ä¹ é¢†åŸŸçš„Githubï¼‰ æ¨å‡ºäº† Rust æœºå™¨å­¦ä¹ æ¡†æ¶ [Candle](https://github.com/huggingface/candle)ã€‚åœ¨è¿™ä¸ªå®˜æ–¹ä»£ç ä»“åº“ä¸Šï¼ŒHuggingFaceä¸Šè§£é‡Šäº†ä¸ºä»€ä¹ˆè¦åšä¸€ä¸ªRustçš„æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚

1. Candleä¸Šäº‘ç«¯çš„ Serverless æ¨ç†å¯è¡Œã€‚PyTorché‚£ä¸€å¥—ä½“ç§¯å¤ªå¤§ï¼Œå®‰è£…å®Œå¾—å‡ ä¸ªGï¼Œè€ŒCandleç¼–è¯‘åçš„å¯æ‰§è¡Œæ–‡ä»¶æ‰åå‡ Måˆ°å‡ åMã€‚
2. Candleå¯ä»¥è®©ä½ é¿å…Pythonçš„ [GIL](https://www.backblaze.com/blog/the-python-gil-past-present-and-future/)ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚
3. HuggingFaceå·²ç»ç”¨Rustå†™äº†ä¸å°‘åŸºç¡€å·¥å…·äº†ï¼Œæ¯”å¦‚ [safetensors](https://github.com/huggingface/safetensors) å’Œ [tokenizers](https://github.com/huggingface/tokenizers)ã€‚

Elon Muskçš„ [x.ai](https://x.ai/) å‘å¸ƒåï¼Œé¡µé¢ä¸Šä¹Ÿæœ‰ä¸€æ®µå¯¹Rustçš„æº¢ç¾ä¹‹è¯ï¼šRustè¢«è¯æ˜æ˜¯ä¸€ä¸ªç†æƒ³çš„é€‰æ‹©ï¼Œç”¨äºæ„å»ºå¯æ‰©å±•çš„ã€å¯é çš„ã€å¯ç»´æŠ¤æ€§çš„åŸºç¡€è®¾æ–½ã€‚å®ƒæä¾›äº†é«˜æ€§èƒ½ã€ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿï¼Œå®ƒèƒ½é˜»æ­¢å¤§éƒ¨åˆ†é”™è¯¯ï¼Œè¿™äº›é”™è¯¯åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ç»å¸¸ä¼šç¢°åˆ°ã€‚ç”±äºæˆ‘ä»¬çš„å›¢é˜Ÿè§„æ¨¡å¾ˆå°ï¼ŒåŸºç¡€è®¾æ–½çš„å¯é æ€§å°±æ˜¾å¾—è‡³å…³é‡è¦ï¼Œå¦åˆ™ç»´æŠ¤å·¥ä½œä¼šæµªè´¹å¤§é‡åˆ›æ–°çš„æ—¶é—´ã€‚Rustç»™æˆ‘ä»¬æä¾›äº†ä¿¡å¿ƒï¼Œä»»ä½•çš„ä»£ç ä¿®æ”¹æˆ–é‡æ„éƒ½å¯ä»¥äº§å‡ºå¯å·¥ä½œçš„ç¨‹åºï¼Œå¹¶ä¸”åœ¨æœ€å°ç›‘ç®¡ä¸‹å¯ä»¥æŒç»­è¿è¡Œå¥½å‡ ä¸ªæœˆï¼ˆè€Œä¸ä¼šå‡ºé—®é¢˜ï¼‰ã€‚

Muskç”šè‡³è¯´Rustè¯­è¨€æ˜¯æœªæ¥[æ„å»º AGI](https://zhuanlan.zhihu.com/p/648565007) çš„è¯­è¨€ã€‚

åœ¨ä½¿ç”¨Rustå°è¯•åšèŠå¤©æœºå™¨äººä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹ç›¸å…³çš„èƒŒæ™¯çŸ¥è¯†ã€‚

## å¤§è¯­è¨€æ¨¡å‹èƒŒæ™¯çŸ¥è¯†

è¿™èŠ‚è¯¾æˆ‘ä»¬è¿˜æ˜¯ä¸»è¦è®²RuståŠRustçš„åº”ç”¨ï¼Œæ‰€ä»¥ç›¸å…³çš„èƒŒæ™¯çŸ¥è¯†æˆ‘ä»¬å°±ç®€å•æ¦‚æ‹¬ä¸€ä¸‹ã€‚

æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼ŒMLï¼‰æ³›æŒ‡ç”¨è®¡ç®—æœºå¯¹æ•°æ®é›†æŒ‰ç…§ä¸€å®šçš„ç®—æ³•è¿›è¡Œæ•°æ®å¤„ç†ã€åˆ†æã€èšç±»ã€å›å½’ç­‰ã€‚åœ¨æ‰§è¡Œå‰ï¼Œäººå¾€å¾€ä¸çŸ¥é“ç»“æœæ˜¯ä»€ä¹ˆï¼Œæ‰€ä»¥å«æœºå™¨å­¦ä¹ ã€‚æœºå™¨å­¦ä¹ å¯ä»¥ç”¨äºè‡ªåŠ¨æå–ä¿¡æ¯ï¼Œè‡ªåŠ¨æˆ–è¾…åŠ©äººç±»åšå†³ç­–ã€‚

è€Œç¥ç»ç½‘ç»œï¼ˆNeural Networkï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç±»ç®—æ³•ï¼Œå®ƒæ˜¯æ¨¡æ‹Ÿäººçš„å¤§è„‘ç¥ç»å…ƒå’Œè¿æ¥çš„ä¸€ç§ç®—æ³•ã€‚ç›®å‰æ•´ä¸ªä¸šç•ŒæŠ•å…¥èµ„æºæœ€å¤šçš„å°±æ˜¯åœ¨è¿™ç±»ç®—æ³•ä¸Šé¢ï¼Œåœ¨è¿™ä¸ªç±»åˆ«ä¸­çš„åˆ›æ–°ä¹Ÿæ˜¯æœ€å¤šçš„ï¼Œæœ‰ç§è§‚ç‚¹è®¤ä¸ºç¥ç»ç½‘ç»œç®—æ³•æ˜¯é€šå‘çœŸæ­£çš„AIæœ€å¯èƒ½æ­£ç¡®çš„è·¯å¾„ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/6b/91/6bc0b61202yy14dfb11a834d0a649b91.png?wh=1525x811 "å›¾ç‰‡æ¥æºï¼šhttps://victorzhou.com/media/nn-series/network.svg ")

è€Œæ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰ï¼Œå…¶å®å°±æ˜¯å±‚æ¬¡å¾ˆå¤šå¾ˆæ·±çš„ç¥ç»ç½‘ç»œã€‚ä½ å¯ä»¥æƒ³è±¡ï¼Œå±‚æ¬¡è¶Šæ·±ï¼ŒèŠ‚ç‚¹æ•°ï¼ˆå›¾é‡Œé‚£äº›åœˆï¼‰è¶Šå¤šï¼Œå°±è¶Šèƒ½æ¨¡æ‹Ÿå¤§è„‘ã€‚ä½†æ˜¯æ·±åº¦å­¦ä¹ å¸¦æ¥çš„é—®é¢˜å°±æ˜¯ï¼Œå±‚æ¬¡è¶Šæ·±ï¼ŒèŠ‚ç‚¹æ•°è¶Šå¤šï¼Œåˆ™é‚£äº›çº¿ï¼ˆå°±æ˜¯æƒé‡å€¼ï¼‰å°±ä¼šè¶Šå¤šï¼Œå‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œé‚£ä¹ˆè®¡ç®—é‡å°±ä¼šè¶Šæ¥è¶Šå¤§ã€‚å…¶å®ä¹‹å‰åå‡ ã€äºŒåå¹´AIè¿›å±•ä¸å¤§ï¼Œä¸»è¦å°±æ˜¯å› ä¸ºè¿™ä¸ªè®¡ç®—èƒ½åŠ›é™åˆ¶äº†ã€‚

é€šè¿‡æ— æ•°äººçš„æ¢ç´¢ï¼Œæˆ‘ä»¬å‘ç°å¯ä»¥æŠŠæ·±åº¦å­¦ä¹ çš„è®¡ç®—å¹¶è¡ŒåŒ–ï¼Œä¼˜åŒ–äº†å¾ˆå¤šå·¥ç¨‹ä¸Šçš„ç®—æ³•ã€‚è¿™æ ·å°±èƒ½å¤Ÿåœ¨å¤§å®¶ç†Ÿæ‚‰çš„ä¸»è¦ç”¨æ¥ç©æ¸¸æˆçš„GPUæ˜¾å¡ä¸Šè¿è¡Œï¼Œå¤§å¤§æé«˜äº†è®¡ç®—é€Ÿåº¦ã€‚è¿™ä¸ªè§†è§’ï¼ŒäºŒåå¤šå¹´å‰Nvidiaçš„è€é»„å°±çœ‹æ˜ç™½äº†ï¼Œæ—©æ—©åœ°æä¾›å¥½äº†åŸºç¡€è®¾æ–½ï¼ŒCUDAã€CUDNNç­‰ã€‚æ‰€ä»¥ç°åœ¨ä½ å°±çœ‹åˆ°äº†ï¼Œä¸€å¡éš¾æ±‚ï¼Œå·²ç»ä¸¥é‡å½±å“åˆ°äº†æ¸¸æˆç©å®¶çš„ç”Ÿå­˜ã€‚

### ChatGPTä¸LLaMA

é‚£ä¹ˆChatGPTæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿè‹±æ–‡æ˜¯Chat Generative Pre-trained Transformerï¼Œæ˜¯OpenAIæä¾›çš„åœ¨çº¿AIå¯¹è¯æœåŠ¡ï¼Œå…·æœ‰ä»¤äººæƒŠå¹çš„ç†è§£èƒ½åŠ›å’Œå›ç­”èƒ½åŠ›ã€‚ä½ å¯ä»¥æŠŠChatGPTç†è§£æˆä¸€ä¸ªä¸ºå¯¹è¯è°ƒä¼˜çš„é¢„è®­ç»ƒè½¬æ¢æ¨¡å‹ã€‚GPT 3 æœ‰ 1750 äº¿å‚æ•°ï¼Œæœ‰ä¼ è¨€GPT 4 æœ‰ 1.76ä¸‡äº¿å‚æ•°ã€‚å‚æ•°æ˜¯ä»€ä¹ˆï¼Œå°±æ˜¯å›¾ä¸­èŠ‚ç‚¹ä¹‹é—´çš„è¿çº¿ï¼Œä½ å¯ä»¥æƒ³è±¡ä¸€ä¸‹1750äº¿æ ¹çº¿çš„åœºæ™¯ã€‚

OpenAIæå‡ºäº†ChatGPTï¼Œè®©å…¨ä¸–ç•ŒæƒŠæ‰äº†ä¸‹å·´ï¼Œä½†æ˜¯å®ƒæ˜¯é—­æºçš„ã€‚äºæ˜¯Metaå…¬å¸ï¼ˆå‰Facebookï¼‰çš„Yann LeCunï¼ˆå›¾çµå¥–å¾—ä¸»ï¼‰å›¢é˜Ÿï¼Œæäº†ä¸€ä¸ªå¼€æºç‰ˆæœ¬çš„GPTï¼Œå« LLaMAã€‚å®ƒä¹Ÿæ˜¯åœ¨å·¨é‡çš„æºæ•°æ®é›†ä¸Šè¿›è¡Œçš„è®­ç»ƒï¼Œç”Ÿæˆäº† 70äº¿ï¼ˆ7Bï¼‰ï¼Œ130äº¿ï¼ˆ13Bï¼‰ï¼Œ650äº¿ï¼ˆ65Bï¼‰ä¸‰ä¸ªç‰ˆæœ¬å‚æ•°çš„å¤§æ¨¡å‹æ–‡ä»¶ï¼Œå¯ä»¥ä¾›ä¸šç•Œåšå­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚ç°åœ¨å·²æ¨å‡ºLLaMA 2ï¼Œæ­£åœ¨åšLLaMA 3ã€‚

LLaMAæå‡ºæ¥åï¼Œæ€èµ·äº†LLMç•Œçš„ç‹‚æ¬¢ã€‚å…¨ä¸–ç•Œçš„å›¢é˜Ÿåœ¨LLaMAçš„åŸºç¡€ä¸Šï¼Œç»§ç»­è°ƒä¼˜ï¼Œæ¨å‡ºäº†å„ç§å„æ ·çš„å¤§æ¨¡å‹ã€‚æ¯”å¦‚å›½å†…æ¸…åçš„ ChatGLM, é›¶ä¸€ä¸‡ç‰©çš„ Yiã€‚å›½å¤–çš„ Mistralã€OpenChatã€Starling ç­‰ã€‚è¿™å—å„¿éå¸¸å·ï¼Œå¤§å®¶éƒ½åœ¨äº‰ç›¸æ¨å‡ºè‡ªå·±è°ƒä¼˜åçš„ç‰ˆæœ¬ï¼Œæ¯å¤©æ—©ä¸Šç¡é†’èµ·æ¥ï¼Œéƒ½å‘ç°åˆæ¨å‡ºäº†å‡ ä¸ªæ–°çš„LLMã€‚

è¿™äº›è®­ç»ƒå¥½åçš„æ–‡ä»¶ä¸€èˆ¬ä»å‡ ä¸ªGåˆ°å‡ åGä¸ç­‰ï¼Œä¹Ÿæœ‰å‡ ç™¾Gçš„ã€‚è¦è¿è¡Œå®ƒä»¬ï¼Œå¾—æœ‰éå¸¸å¼ºå¤§çš„æœºå™¨æ‰è¡Œã€‚æ¯”å¦‚7B çš„ LLaMA 2 æ–‡ä»¶ï¼Œæ¯ä¸ªæƒé‡ä¸ºä¸€ä¸ª f16 æµ®ç‚¹æ•°ï¼Œå ä¸¤ä¸ªå­—èŠ‚ï¼Œæ‰€ä»¥å¯ä»¥ä¼°ç®—å‡ºè¦è¿è¡Œ LLaMA 2 æ¨¡å‹ï¼Œèµ·ç å¾—æœ‰ 14G çš„å†…å­˜æˆ–æ˜¾å­˜ã€‚å†…å­˜è¿˜å¥½ï¼Œæ˜¾å­˜è¶…è¿‡14Gçš„ä¸ªäººç”¨æˆ·çœŸä¸å¤šã€‚å¹¶ä¸”ï¼ŒLLaMA2çš„æ¨¡å‹æ–‡ä»¶æ˜¯PyTorchå¯¼å‡ºçš„ï¼Œåªèƒ½ç”±PyTorchæ¡†æ¶æ¥è¿è¡Œã€‚

### llama.cppä¸é‡å­åŒ–æ–¹æ³•

è½¦åˆ°å±±å‰å¿…æœ‰è·¯ï¼Œå¤§ç¥ Georgi Gerganov æäº†ä¸€ä¸ªé¡¹ç›®ï¼š[llama.cpp](https://github.com/ggerganov/llama.cpp)ã€‚å®ƒæ˜¯ä¸€ä¸ªç”¨C/C++é‡æ–°å®ç°å¼•æ“çš„ç‰ˆæœ¬ï¼Œä¸éœ€è¦å®‰è£…PyTorchï¼Œå°±å¯ä»¥è¿è¡ŒLLaMA 2æ¨¡å‹æ–‡ä»¶ã€‚æœ€å…³é”®çš„æ˜¯ï¼Œå®ƒæå‡ºäº†ä¸€ç§é‡å­åŒ–ï¼ˆquantizationï¼‰æ–¹æ³•ï¼Œå¯ä»¥å°†æƒé‡ä» 16 ä½é‡å­åŒ–åˆ°8ä½ã€6ä½ã€5ä½ã€4ä½ï¼Œç”šè‡³2ä½ã€‚è¿™æ ·ï¼Œå°±ç›¸å½“äºç­‰æ¯”ç¼©å°äº†å ç”¨å†…å­˜çš„è§„æ¨¡ã€‚æ¯”å¦‚ï¼Œä¸€ä¸ª4ä½é‡å­åŒ–ç‰ˆæœ¬çš„LLaMA 2 7Bæ¨¡å‹ï¼Œå°±åªéœ€è¦ä¸åˆ°4Gçš„å†…å­˜/æ˜¾å­˜å°±èƒ½è¿è¡Œã€‚è¿™æ ·ï¼Œå°±èƒ½é€‚é…å¤§å¤šæ•°çš„ä¸ªäººè®¡ç®—æœºäº†ã€‚

è¿™ç§é‡å­åŒ–æ–¹æ³•æ˜¯ä¸ªé‡å¤§åˆ›æ–°ï¼Œå®ƒç›´æ¥ä¿ƒè¿›äº†LLMç”Ÿæ€çš„è¿›ä¸€æ­¥ç¹è£ã€‚ç°åœ¨HuggingFaceä¸Šæœ‰å¤§é‡é‡å­åŒ–åçš„æ¨¡å‹ï¼Œæ¯”å¦‚ [openchat\_3.5.Q4\_K\_M.gguf](https://huggingface.co/TheBloke/openchat_3.5-GGUF/blob/main/openchat_3.5.Q4_K_M.gguf) å°±æ˜¯ä¸€ä¸ªOpenChatçš„4ä½é‡å­åŒ–çš„ç‰ˆæœ¬ã€‚æˆ‘ä»¬ä¸‹è½½çš„æ—¶å€™ï¼Œç›´æ¥ä¸‹è½½è¿™äº›é‡å­åŒ–åçš„æ¨¡å‹æ–‡ä»¶å°±å¯ä»¥äº†ã€‚

è¯·æ³¨æ„ï¼Œè¿™äº›æ–‡ä»¶æ˜¯è®­ç»ƒåçš„æˆå“ï¼Œæˆ‘ä»¬ä¸‹è½½å®ƒæ˜¯ç”¨æ¥åšæ¨ç†ï¼ˆinferï¼‰çš„ï¼Œè€Œä¸æ˜¯è®­ç»ƒï¼ˆtrainï¼‰çš„ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™äº›æˆå“æ¨¡å‹ä¸Šè¿è¡Œè°ƒä¼˜ï¼ˆfine tuneï¼‰ã€‚

### å¤§æ¨¡å‹æ–‡ä»¶æ ¼å¼

ç›®å‰HuggingFaceä¸Šæœ‰å‡ ç§å¸¸è§çš„LLMæ–‡ä»¶æ ¼å¼ã€‚

- binæ ¼å¼ï¼šPytorchå¯¼å‡ºçš„æ¨¡å‹æ–‡ä»¶æ ¼å¼
- safetensorsæ ¼å¼ï¼šHuggingFaceå®šä¹‰çš„ä¸€ç§æ–°çš„æ¨¡å‹æ–‡ä»¶æ ¼å¼ï¼Œæœ‰å¯èƒ½æˆä¸ºæœªæ¥çš„ä¸»æµæ ¼å¼ã€‚HuggingFaceç”¨Rustå®ç°safetensorsæ ¼å¼çš„è§£æï¼Œå¹¶å¯¼å‡ºä¸ºPyæ¥å£ï¼Œè¯·å‚è§[é“¾æ¥](https://huggingface.co/docs/safetensors/index)ã€‚
- ggmlæ ¼å¼ï¼šllama.cpp é¡¹ç›®é‡å­åŒ–æ¨¡å‹çš„å‰æœŸæ¨¡å‹æ ¼å¼ã€‚
- ggufæ ¼å¼ï¼šllama.cppé¡¹ç›®é‡å­åŒ–æ¨¡å‹çš„åæœŸæ¨¡å‹æ ¼å¼ï¼Œä¹Ÿæ˜¯ç°åœ¨ä¸»æµçš„é‡å­åŒ–LLMæ ¼å¼ã€‚

### Rustçš„æœºå™¨å­¦ä¹ æ¡†æ¶

Rustç”Ÿæ€ç°åœ¨æœ‰å‡ ä¸ªæ¯”è¾ƒä¸é”™çš„MLæ¡†æ¶ï¼Œæœ€å¥½çš„ä¸¤ä¸ªæ˜¯ï¼š[Candle](https://github.com/huggingface/candle) å’Œ [burn](https://github.com/Tracel-AI/burn)ã€‚åç»­ï¼Œæˆ‘ä»¬ä»¥Candleä¸ºä¾‹æ¥ä»‹ç»ã€‚

## Candleä»‹ç»

æ®Candleå®˜ç½‘ä»‹ç»ï¼Œå®ƒæ˜¯ä¸€ä¸ªæå°ä¸»ä¹‰æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œä¹Ÿå°±æ˜¯æ²¡ä»€ä¹ˆä¾èµ–ï¼Œä¸åƒPytorché‚£æ ·è£…ä¸€å †ä¸œè¥¿ï¼Œéƒ¨ç½²èµ·æ¥å¾ˆéº»çƒ¦ã€‚ä½†å…¶å®å®ƒä¹Ÿèƒ½ç”¨æ¥è®­ç»ƒã€‚

å®ƒæœ‰ä¸‹é¢è¿™äº›ç‰¹æ€§ï¼š

- HuggingFaceå‡ºå“ã€‚è¿‘æ°´æ¥¼å°å…ˆå¾—æœˆï¼ŒCandleå‡ ä¹èƒ½æ”¯æŒHuggingFaceä¸Šæ‰€æœ‰çš„æ¨¡å‹ï¼ˆæœ‰çš„éœ€è¦ç»è¿‡è½¬æ¢ï¼‰ã€‚
- è¯­æ³•ç®€å•ï¼Œè·ŸPyTorchå·®ä¸å¤šã€‚
- CPUã€Cudaã€Metalçš„æ”¯æŒã€‚
- è®©serverlesså’Œå¿«é€Ÿéƒ¨ç½²æˆä¸ºå¯èƒ½ã€‚
- æ¨¡å‹è®­ç»ƒã€‚
- åˆ†å¸ƒå¼è®¡ç®—ï¼ˆé€šè¿‡NCCLï¼‰ã€‚
- å¼€ç®±å³ç”¨çš„æ¨¡å‹æ”¯æŒï¼ŒLLaMAã€Whisperã€ Falcon ç­‰ç­‰ã€‚

Candleä¸ä»…ä»…æ˜¯å¤§æ¨¡å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒè¿˜æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå› æ­¤å®ƒä¹Ÿæ”¯æŒå…¶ä»–çš„æœºå™¨å­¦ä¹ ç®—æ³•å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆreinforcement learningï¼‰ã€‚ä¸‹é¢æˆ‘ä»¬å°±æ¥çœ‹çœ‹å¦‚ä½•åˆ©ç”¨Candleæ¡†æ¶åšä¸€ä¸ªèŠå¤©æœºå™¨äººã€‚

æ³¨ï¼šè¿™èŠ‚è¯¾çš„ä»£ç é€‚ç”¨äº candle\_core v0.3 ç‰ˆæœ¬ã€‚

## ä½¿ç”¨Candleåšä¸€ä¸ªèŠå¤©æœºå™¨äºº

### ä¸‹è½½æ¨¡å‹æ–‡ä»¶

æˆ‘å¯¹ä¸€äº›å¤§æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°OpenChatçš„å¯¹è¯æ•ˆæœæ¯”è¾ƒå¥½ï¼Œæ‰€ä»¥ä¸‹é¢æˆ‘ä»¬ç”¨OpenChat LLMæ¥è¿›è¡Œå±•ç¤ºã€‚æˆ‘ä»¬ä¼šç”¨ [quantized 8bit](https://huggingface.co/TheBloke/openchat_3.5-GGUF/blob/main/openchat_3.5.Q8_0.gguf) çš„ç‰ˆæœ¬ã€‚å…¶å® 4bit çš„ç‰ˆæœ¬ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œæ•ˆæœä¹Ÿéå¸¸å¥½ã€‚

Candle å®˜æ–¹çš„ç¤ºä¾‹æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä¸ºè¿™ä¸ªè¯¾ç¨‹å®šåˆ¶äº†ä¸€ä¸ªæ›´ç®€å•çš„ç‹¬ç«‹è¿è¡Œçš„[ç¤ºä¾‹](https://github.com/miketang84/jikeshijian/tree/master/23-candle_chat)ã€‚ä½ å¯ä»¥å°†è¿™ä¸ªä»“åº“å…‹éš†ä¸‹æ¥ï¼Œè¿›å…¥ç›®å½•ã€‚åœ¨è¿è¡Œä»£ç ä¹‹å‰ï¼Œä¸‹è½½æ¨¡å‹æ–‡ä»¶å’Œtokenizer.jsonæ–‡ä»¶ã€‚

```plain
ä¸ä»£ç ç›®å½•åŒçº§çš„ä½ç½®ï¼Œåˆ›å»ºä¸€ä¸ªç›®å½•
mkdir hf_hub
è¿›å…¥è¿™ä¸ªç›®å½•ï¼Œè¯·ä¸‹è½½
https://huggingface.co/TheBloke/openchat_3.5-GGUF/blob/main/openchat_3.5.Q8_0.gguf
å’Œ
https://huggingface.co/openchat/openchat_3.5/blob/main/tokenizer.json
å°†è¿™ä¸ª tokenizer.json é‡å‘½åä¸º openchat_3.5_tokenizer.json
```

ç›®å½•ç»“æ„ï¼š

```plain
23-candle_chat/
    Cargo.toml
    src/
hf_hub/
    openchat_3.5_tokenizer.json
    openchat_3.5.Q8_0.gguf
```

### è¿è¡Œæ¼”ç¤º

ç„¶åï¼Œè¿›å…¥ 23-candle\_chat/ è¿è¡Œï¼š

```plain
cargo run --release --bin simple
```

å‡ºç°å¦‚ä¸‹ç•Œé¢ï¼Œå°±å¯ä»¥èŠå¤©äº†ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/18/27/18d6de5afca0b6ffb61e1bc309877127.png?wh=1531x255)

ä¸‹é¢æ˜¯æˆ‘é—®çš„ä¸€ä¸ªé—®é¢˜ï¼Œbotçš„å›ç­”å¥½åƒæœ‰ç‚¹é—®é¢˜ï¼Œè¿™ä¸ªæ¨¡å‹ç”¨è‹±æ–‡é—®çš„æ•ˆæœä¼šå¥½ä¸€äº›ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/6e/c1/6e420065087c82b10813d2694a67c6c1.png?wh=1920x791)

### ä»£ç è®²è§£

ä½ å¯ä»¥çœ‹ä¸€ä¸‹ä»£ç ã€‚

```plain
#![allow(unused)]

use std::fs::File;
use std::io::Write;
use std::path::PathBuf;
use tokenizers::Tokenizer;

use candle_core::quantized::gguf_file;
use candle_core::utils;
use candle_core::{Device, Tensor};
use candle_transformers::generation::LogitsProcessor;
use candle_transformers::models::quantized_llama as quantized_model;

use anyhow::Result;

mod token_output_stream;
use token_output_stream::TokenOutputStream;

struct Args {
Â  Â  tokenizer: String,
Â  Â  model: String,
Â  Â  sample_len: usize,
Â  Â  temperature: f64,
Â  Â  seed: u64,
Â  Â  repeat_penalty: f32,
Â  Â  repeat_last_n: usize,
Â  Â  gqa: usize,
}

impl Args {
Â  Â  fn tokenizer(&self) -> Result<Tokenizer> {
Â  Â  Â  Â  let tokenizer_path = PathBuf::from(&self.tokenizer);
Â  Â  Â  Â  Tokenizer::from_file(tokenizer_path).map_err(anyhow::Error::msg)
Â  Â  }

Â  Â  fn model(&self) -> Result<PathBuf> {
Â  Â  Â  Â  Ok(std::path::PathBuf::from(&self.model))
Â  Â  }
}

fn main() -> anyhow::Result<()> {
Â  Â  println!(
Â  Â  Â  Â  "avx: {}, neon: {}, simd128: {}, f16c: {}",
Â  Â  Â  Â  utils::with_avx(),
Â  Â  Â  Â  utils::with_neon(),
Â  Â  Â  Â  utils::with_simd128(),
Â  Â  Â  Â  utils::with_f16c()
Â  Â  );

Â  Â  let args = Args {
Â  Â  Â  Â  tokenizer: String::from("../hf_hub/openchat_3.5_tokenizer.json"),
Â  Â  Â  Â  model: String::from("../hf_hub/openchat_3.5.Q8_0.gguf"),
Â  Â  Â  Â  sample_len: 1000,
Â  Â  Â  Â  temperature: 0.8,
Â  Â  Â  Â  seed: 299792458,
Â  Â  Â  Â  repeat_penalty: 1.1,
Â  Â  Â  Â  repeat_last_n: 64,
Â  Â  Â  Â  gqa: 8,
Â  Â  };

Â  Â  // load model
Â  Â  let model_path = args.model()?;
Â  Â  let mut file = File::open(&model_path)?;
Â  Â  let start = std::time::Instant::now();

Â  Â  // This is the model instance
Â  Â  let model = gguf_file::Content::read(&mut file)?;
Â  Â  let mut total_size_in_bytes = 0;
Â  Â  for (_, tensor) in model.tensor_infos.iter() {
Â  Â  Â  Â  let elem_count = tensor.shape.elem_count();
Â  Â  Â  Â  total_size_in_bytes +=
Â  Â  Â  Â  Â  Â  elem_count * tensor.ggml_dtype.type_size() / tensor.ggml_dtype.blck_size();
Â  Â  }
Â  Â  println!(
Â  Â  Â  Â  "loaded {:?} tensors ({}bytes) in {:.2}s",
Â  Â  Â  Â  model.tensor_infos.len(),
Â  Â  Â  Â  total_size_in_bytes,
Â  Â  Â  Â  start.elapsed().as_secs_f32(),
Â  Â  );
Â  Â  let mut model = quantized_model::ModelWeights::from_gguf(model, &mut file)?;
Â  Â  println!("model built");

Â  Â  // load tokenizer
Â  Â  let tokenizer = args.tokenizer()?;
Â  Â  let mut tos = TokenOutputStream::new(tokenizer);
Â  Â  // left for future improvement: interactive
Â  Â  for prompt_index in 0.. {
Â  Â  Â  Â  print!("> ");
Â  Â  Â  Â  std::io::stdout().flush()?;
Â  Â  Â  Â  let mut prompt = String::new();
Â  Â  Â  Â  std::io::stdin().read_line(&mut prompt)?;
Â  Â  Â  Â  if prompt.ends_with('\n') {
Â  Â  Â  Â  Â  Â  prompt.pop();
Â  Â  Â  Â  Â  Â  if prompt.ends_with('\r') {
Â  Â  Â  Â  Â  Â  Â  Â  prompt.pop();
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  Â  Â  let prompt_str = format!("User: {prompt} <|end_of_turn|> Assistant: ");
Â  Â  Â  Â  print!("bot: ");

Â  Â  Â  Â  let tokens = tos
Â  Â  Â  Â  Â  Â  .tokenizer()
Â  Â  Â  Â  Â  Â  .encode(prompt_str, true)
Â  Â  Â  Â  Â  Â  .map_err(anyhow::Error::msg)?;

Â  Â  Â  Â  let prompt_tokens = tokens.get_ids();
Â  Â  Â  Â  let mut all_tokens = vec![];
Â  Â  Â  Â  let mut logits_processor = LogitsProcessor::new(args.seed, Some(args.temperature), None);

Â  Â  Â  Â  let start_prompt_processing = std::time::Instant::now();
Â  Â  Â  Â  let mut next_token = {
Â  Â  Â  Â  Â  Â  let input = Tensor::new(prompt_tokens, &Device::Cpu)?.unsqueeze(0)?;
Â  Â  Â  Â  Â  Â  let logits = model.forward(&input, 0)?;
Â  Â  Â  Â  Â  Â  let logits = logits.squeeze(0)?;
Â  Â  Â  Â  Â  Â  logits_processor.sample(&logits)?
Â  Â  Â  Â  };
Â  Â  Â  Â  let prompt_dt = start_prompt_processing.elapsed();
Â  Â  Â  Â  all_tokens.push(next_token);
Â  Â  Â  Â  if let Some(t) = tos.next_token(next_token)? {
Â  Â  Â  Â  Â  Â  print!("{t}");
Â  Â  Â  Â  Â  Â  std::io::stdout().flush()?;
Â  Â  Â  Â  }

Â  Â  Â  Â  let eos_token = "<|end_of_turn|>";
Â  Â  Â  Â  let eos_token = *tos.tokenizer().get_vocab(true).get(eos_token).unwrap();
Â  Â  Â  Â  let start_post_prompt = std::time::Instant::now();
Â  Â  Â  Â  let to_sample = args.sample_len.saturating_sub(1);
Â  Â  Â  Â  let mut sampled = 0;
Â  Â  Â  Â  for index in 0..to_sample {
Â  Â  Â  Â  Â  Â  let input = Tensor::new(&[next_token], &Device::Cpu)?.unsqueeze(0)?;
Â  Â  Â  Â  Â  Â  let logits = model.forward(&input, prompt_tokens.len() + index)?;
Â  Â  Â  Â  Â  Â  let logits = logits.squeeze(0)?;
Â  Â  Â  Â  Â  Â  let logits = if args.repeat_penalty == 1. {
Â  Â  Â  Â  Â  Â  Â  Â  logits
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  let start_at = all_tokens.len().saturating_sub(args.repeat_last_n);
Â  Â  Â  Â  Â  Â  Â  Â  candle_transformers::utils::apply_repeat_penalty(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  &logits,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  args.repeat_penalty,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  &all_tokens[start_at..],
Â  Â  Â  Â  Â  Â  Â  Â  )?
Â  Â  Â  Â  Â  Â  };
Â  Â  Â  Â  Â  Â  next_token = logits_processor.sample(&logits)?;
Â  Â  Â  Â  Â  Â  all_tokens.push(next_token);
Â  Â  Â  Â  Â  Â  if let Some(t) = tos.next_token(next_token)? {
Â  Â  Â  Â  Â  Â  Â  Â  print!("{t}");
Â  Â  Â  Â  Â  Â  Â  Â  std::io::stdout().flush()?;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  sampled += 1;
Â  Â  Â  Â  Â  Â  if next_token == eos_token {
Â  Â  Â  Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  Â  Â  };
Â  Â  Â  Â  }
Â  Â  Â  Â  if let Some(rest) = tos.decode_rest().map_err(candle_core::Error::msg)? {
Â  Â  Â  Â  Â  Â  print!("{rest}");
Â  Â  Â  Â  }
Â  Â  Â  Â  std::io::stdout().flush()?;
Â  Â  Â  Â  let dt = start_post_prompt.elapsed();
Â  Â  Â  Â  println!(
Â  Â  Â  Â  Â  Â  "\n\n{:4} prompt tokens processed: {:.2} token/s",
Â  Â  Â  Â  Â  Â  prompt_tokens.len(),
Â  Â  Â  Â  Â  Â  prompt_tokens.len() as f64 / prompt_dt.as_secs_f64(),
Â  Â  Â  Â  );
Â  Â  Â  Â  println!(
Â  Â  Â  Â  Â  Â  "{sampled:4} tokens generated: {:.2} token/s",
Â  Â  Â  Â  Â  Â  sampled as f64 / dt.as_secs_f64(),
Â  Â  Â  Â  );
Â  Â  }

Â  Â  Ok(())
}
```

æˆ‘ä»¬åˆ†æ®µè®²è§£è¿™100å¤šè¡Œä»£ç ã€‚

ç¬¬19è¡Œï¼Œå®šä¹‰äº†Argså‚æ•°ï¼Œè¿™æ˜¯æ¨¡å‹å¿…è¦çš„å‚æ•°é…ç½®å®šä¹‰ã€‚ç¬¬42ï½48è¡Œï¼Œçœ‹çœ‹æœ‰å“ªäº›CPUç‰¹æ€§æ”¯æŒã€‚ç¬¬50ï½59è¡Œï¼Œå®ä¾‹åŒ–Argsï¼Œè¿™äº›å‚æ•°éƒ½æ˜¯ç¡¬ç¼–ç è¿›å»çš„ï¼Œé™¤äº†ä¸¤ä¸ªæ–‡ä»¶çš„è·¯å¾„å¤–ï¼Œéœ€è¦LLMç›¸å…³çŸ¥è¯†æ‰èƒ½ç†è§£ã€‚

ç¬¬62ï½81è¡Œï¼ŒåŠ è½½å¤§æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆæ¨¡å‹å¯¹è±¡ã€‚æˆ‘ä»¬è¿™ä¸ªæ¨¡å‹æ˜¯GGUFæ ¼å¼çš„ï¼Œå› æ­¤éœ€è¦ç”¨gguf\_fileæ¨¡å—æ¥è¯»ã€‚Tensoræ˜¯LLMä¸­çš„é‡è¦æ¦‚å¿µï¼Œå®ƒæ˜¯ä¸€ä¸ªå¤šç»´æ•°ç»„ï¼Œå¯ä»¥åœ¨CPUå’ŒGPUä¸Šè®¡ç®—ï¼Œåœ¨GPUä¸Šè¿˜å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚ä¸€ä¸ªå¤§æ¨¡å‹ç”±å¾ˆå¤šçš„Tensorç»„æˆã€‚æˆ‘ä»¬è¿™ä¸ªæ¨¡å‹ä¸­ï¼ŒåŠ è½½è¿›æ¥äº† 291 ä¸ª Tensorã€‚

ç¬¬84è¡Œï¼ŒåŠ è½½tokenizer.jsonæ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆ tokenizer å®ä¾‹ã€‚tokenizer ç”¨äºå°†è¾“å…¥å’Œè¾“å‡ºçš„æ–‡æœ¬è½¬åŒ–ä¸ºTensorï¼Œå˜æˆå¤§æ¨¡å‹å¯ç†è§£çš„æ•°æ®ã€‚ç¬¬85è¡Œåˆ›å»ºtokenè¾“å‡ºæµå®ä¾‹ã€‚ç¬¬89ï½99è¡Œï¼Œå»ºç«‹é—®ç­”ç•Œé¢ï¼Œè¾“å…¥æç¤ºç¬¦ä¸ºä¸€ä¸ª &gt; å·ï¼Œè¾“å‡ºä¸º `bot:` å¼€å¤´ã€‚ç¬¬101ï½104è¡Œå°†è¾“å…¥çš„é—®ç­”è½¬åŒ–ä¸º tokensã€‚è¿™ä¸ªtokenså°±æ˜¯Tensorå®ä¾‹ã€‚

ç¬¬106ï½122è¡Œï¼Œæ˜¯ç”¨äºå¤„ç†è¾“å…¥ï¼Œå¤§æ¨¡å‹å¯¹è¾“å…¥çš„tokenåšä¸€å¤„ç†ï¼Œä½ å¯ä»¥ç†è§£æˆå¤§æ¨¡å‹å¯¹ä½ çš„è¾“å…¥é—®é¢˜å…ˆè¦è¿›è¡Œä¸€ä¸‹ç†è§£ï¼Œç„¶ååé¢æ‰èƒ½åšå‡ºå¯¹åº”çš„å›ç­”ã€‚  
LogitsProcessor æ˜¯ä¸€ä¸ªç”¨äºä¿®æ”¹æ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒçš„å·¥å…·ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­ä½¿ç”¨çš„è®¾å¤‡å†™æ­»äº†ï¼Œç”¨çš„CPUã€‚

ç¬¬124è¡Œï¼Œ`"<|end_of_turn|>"` æ˜¯ OpenChat æ¨¡å¼å®šä¹‰çš„ä¸€è½®å¯¹è¯ç»“æŸçš„æ ‡å¿—ã€‚ç¬¬125ï½156è¡Œï¼Œå°±æ˜¯å¯¹é—®é¢˜çš„å›ç­”ã€‚ç”¨çš„è®¾å¤‡ä»ç„¶ä¸ºCPUï¼Œæˆ‘ä»¬å¯ä»¥çŒœæµ‹åº”è¯¥ä¼šå¾ˆæ…¢ã€‚å®ƒè¿™é‡Œé¢è¿˜æœ‰å¯¹penaltyæœºåˆ¶çš„å¤„ç†ã€‚ç»†èŠ‚ä¹Ÿéœ€è¦å»æŸ¥é˜…å¤§æ¨¡å‹NLPç›¸å…³çš„çŸ¥è¯†ã€‚

ç¬¬158ï½167è¡Œï¼Œæ˜¯å¯¹æœ¬æ¬¡å¤„ç†æ€§èƒ½çš„ä¸€ä¸ªæ±‡æ€»ã€‚åœ¨æˆ‘çš„ç”µè„‘ä¸Šï¼Œçº¯ç”¨CPUè®¡ç®—çš„è¯ï¼Œåªèƒ½è¾¾åˆ°1ç§’ä¸€ä¸ªå¤štokençš„é€Ÿåº¦ï¼Œéå¸¸å¡ã€‚æœ‰GPUåŠ æŒçš„è¯ï¼Œä¼šå¿«å¾ˆå¤šã€‚

ä½ å¯èƒ½å‘ç°äº†ï¼Œç¬¬87è¡Œæœ‰ä¸ªå¾ªç¯ï¼Œå®ƒæ˜¯ç”¨æ¥å®ç° interactive äº¤äº’æ•ˆæœçš„ï¼Œé—®å®Œä¸€å¥ï¼Œå›ç­”å®Œï¼Œè¿˜å¯ä»¥é—®ä¸‹ä¸€å¥ã€‚

### æ·»åŠ å‘½ä»¤è¡Œå‚æ•°

å‰é¢çš„simpleç¤ºä¾‹ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„å‚æ•°éƒ½æ˜¯å†™æ­»åœ¨ä»£ç é‡Œé¢çš„ã€‚è¿™æ ·æ–¹ä¾¿ç†è§£ï¼Œä½†ä¸æ–¹ä¾¿ä½¿ç”¨ã€‚æˆ‘ä»¬å¯ä»¥å°è¯•ä¸ºå®ƒæ·»åŠ å‘½ä»¤è¡Œå‚æ•°åŠŸèƒ½ã€‚

åœ¨Rustä¸­ï¼Œå†™ä¸€ä¸ªå‘½ä»¤è¡Œéå¸¸ç®€å•ï¼Œç›´æ¥ç”¨clapï¼Œæ”¹å‡ è¡Œä»£ç å°±å¯ä»¥äº†ã€‚å°†ä¸Šé¢ç¤ºä¾‹ä¸­çš„Argsç»“æ„ä½“çš„å®šä¹‰å˜æˆä¸‹é¢è¿™æ ·å°±å¯ä»¥äº†ï¼Œç„¶ååœ¨è°ƒç”¨çš„æ—¶å€™ä½¿ç”¨ `Args::parse()` ç”Ÿæˆ Args å®ä¾‹ã€‚

```plain
#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
Â  Â  #[arg(long, default_value = "../hf_hub/openchat_3.5_tokenizer.json")]
Â  Â  tokenizer: String,
Â  Â  #[arg(long, default_value = "../hf_hub/openchat_3.5.Q8_0.gguf")]
Â  Â  model: String,
Â  Â  #[arg(short = 'n', long, default_value_t = 1000)]
Â  Â  sample_len: usize,
Â  Â  #[arg(long, default_value_t = 0.8)]
Â  Â  temperature: f64,
Â  Â  #[arg(long, default_value_t = 299792458)]
Â  Â  seed: u64,
Â  Â  #[arg(long, default_value_t = 1.1)]
Â  Â  repeat_penalty: f32,
Â  Â  #[arg(long, default_value_t = 64)]
Â  Â  repeat_last_n: usize,
Â  Â  #[arg(long, default_value_t = 8)]
Â  Â  gqa: usize,
}

fn main() {
    // ...
    let args = Args::parse();  
    // ...
}
```

ç»è¿‡å‡çº§çš„å‘½ä»¤æœ‰äº†ä¸‹é¢è¿™äº›å‚æ•°ï¼š

```plain
$ cargo run --release --bin cli -- --help
    Finished release [optimized] target(s) in 0.04s
     Running `target/release/cli --help`
avx: false, neon: false, simd128: false, f16c: false
Usage: cli [OPTIONS]
Options:
      --tokenizer <TOKENIZER>            [default: ../hf_hub/openchat_3.5_tokenizer.json]
      --model <MODEL>                    [default: ../hf_hub/openchat_3.5.Q8_0.gguf]
  -n, --sample-len <SAMPLE_LEN>          [default: 1000]
      --temperature <TEMPERATURE>        [default: 0.8]
      --seed <SEED>                      [default: 299792458]
      --repeat-penalty <REPEAT_PENALTY>  [default: 1.1]
      --repeat-last-n <REPEAT_LAST_N>    [default: 64]
      --gqa <GQA>                        [default: 8]
  -h, --help                             Print help
  -V, --version                          Print version
```

æ˜¯ä¸æ˜¯å¾ˆæ–¹ä¾¿ï¼Ÿè°è¯´Rustç”Ÿäº§åŠ›ä¸è¡Œçš„ï¼

ç‚¹å‡»[è¿™é‡Œ](https://github.com/miketang84/jikeshijian/tree/master/23-candle_chat)å¯ä»¥æ‰¾åˆ°æºæ–‡ä»¶ï¼Œä½ å¯ä»¥æœ¬åœ°è¯•è¯•è·‘èµ·ä¸€ä¸ªå¤§æ¨¡å‹å¯¹è¯æœºå™¨äººã€‚å¦å¤–ï¼ŒCandleå®˜æ–¹ä»“åº“ä¸­çš„[ç¤ºä¾‹](https://github.com/huggingface/candle/tree/main/candle-examples/examples/quantized)åŠŸèƒ½æ›´å¼ºå¤§ï¼Œä½†ä¹Ÿæ›´å¤æ‚ï¼Œä½ å¯ä»¥ç»§ç»­æ·±å…¥ç ”ç©¶ã€‚

## å°ç»“

è¿™èŠ‚è¯¾æˆ‘ä»¬ä¸€èµ·æ¢ç´¢äº†ä½¿ç”¨Ruståˆ©ç”¨Candleæœºå™¨å­¦ä¹ æ¡†æ¶å¼€å‘ä¸€ä¸ªå¤§æ¨¡å‹èŠå¤©æœºå™¨äººçš„åº”ç”¨ã€‚Rustç›®å‰åœ¨AIç•Œè™½ç„¶è¿˜ä¸å¤Ÿæœ‰å½±å“åŠ›ï¼Œä½†æ˜¯æœªæ¥æ˜¯ç›¸å½“æœ‰æ½œåŠ›çš„ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆHuggingFaceå¸¦å¤´å‡ºä¸€ä¸ªRustæœºå™¨å­¦ä¹ æ¡†æ¶çš„åŸå› ã€‚

ä¸è¿‡è¿™èŠ‚è¯¾æˆ‘ä»¬åªæ˜¯è®²äº†æ€ä¹ˆç”¨èµ·æ¥ï¼Œè€Œå¦‚æœè¦æ·±å…¥ä¸‹å»çš„è¯ï¼Œæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†å°±å¿…ä¸å¯å°‘äº†ã€‚å¦‚æœä½ æœ‰æ—¶é—´ç²¾åŠ›çš„è¯ï¼Œä½ å¯ä»¥æ·±å…¥ä¸‹å»å¥½å¥½è¡¥å……ä¸€ä¸‹è¿™æ–¹é¢çš„å­¦æœ¯çŸ¥è¯†ï¼Œæ¯•ç«Ÿæœªæ¥å‡ åå¹´ï¼ŒAIæ˜¯ä¸€ä¸ªä¸»è¦é—®é¢˜ï¼Œä¹Ÿä¼šæ˜¯ä¸€ä¸ªä¸»è¦æœºä¼šã€‚ç›®å‰AIå‘å±•é€Ÿåº¦å¤ªå¿«äº†ï¼Œæœ‰ç§å­¦ä¹ è·Ÿä¸ä¸Šä¸šç•Œå‘å±•é€Ÿåº¦çš„æ„Ÿè§‰ã€‚ä½†æ˜¯ä¸ç®¡æ€æ ·ï¼Œå­¦å¥½åŸºç¡€ï¼Œæ°¸è¿œä¸ä¼šè¿‡æ—¶ã€‚

å¦å¤–ï¼ŒRust AIè¿™å—å„¿ï¼Œè™½ç„¶å·²ç»å°æœ‰èµ·è‰²ï¼Œä½†æ˜¯ä½œä¸ºç”Ÿæ€æ¥è®²ï¼Œç©ºç™½å¤„è¿˜å¾ˆå¤šã€‚æ‰€ä»¥è¿™ä¹Ÿæ­£æ˜¯å­¦å¥½Rustçš„æœºä¼šï¼ŒRustå¯ä»¥åœ¨AIåŸºå»ºè¿™å—åšå¤§é‡çš„å·¥ä½œï¼Œè¿™äº›å·¥ä½œå¯ä»¥æœåŠ¡äºRustç¤¾åŒºï¼Œä¹Ÿå¯ä»¥æœåŠ¡äºPythonä¹ƒè‡³æ•´ä¸ªAIç¤¾åŒºã€‚

## æ€è€ƒé¢˜

ä½ å¯ä»¥åœ¨æˆ‘çš„ç¤ºä¾‹ä¸Šç»§ç»­æ£é¼“ï¼Œæ·»åŠ GPUçš„æ”¯æŒï¼Œåœ¨Linuxã€Windowsã€macOSå¤šç§å¹³å°ä¸Šæµ‹è¯•ä¸€ä¸‹ã€‚æ¬¢è¿ä½ åœ¨è¯„è®ºåŒºè´´å‡ºä½ è‡ªå·±çš„ä»£ç ï¼Œä¹Ÿæ¬¢è¿ä½ æŠŠè¿™èŠ‚è¯¾åˆ†äº«ç»™å…¶ä»–æœ‹å‹ï¼Œæˆ‘ä»¬ä¸‹èŠ‚è¯¾å†è§ï¼
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>eriklee</span> ğŸ‘ï¼ˆ6ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>&quot;é¦–å…ˆ Rust çš„æ€§èƒ½ä¸ C&#47;C++ ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨è°ƒç”¨ GPU èƒ½åŠ›æ–¹é¢ä¹ŸåŒæ ·æ–¹ä¾¿&quot;
rustç›®å‰åº”è¯¥è¿˜ä¸èƒ½ç›´æ¥è®¿é—®cudaå§ï¼Ÿæ¯•ç«Ÿcudaæ˜¯cæ¥å£</p>2024-01-13</li><br/><li><span>å¬é›¨</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œæˆ‘ä¸‹è½½åˆ°äº†windowä¸‹çš„å­Linuxä¸­è¿è¡Œï¼ŒæŠ¥OS errorã€‚è¯·é—®æ˜¯ä¸æ˜¯ä¸æ”¯æŒåœ¨è¿™é‡Œé¢è¿è¡Œå•Š</p>2023-12-23</li><br/><li><span>tan</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>wsl: Error: No such file or directory (os error 2) . å¤„ç†ï¼šsimple.rsä¸­ tikenizerå’Œmodelçš„å‚æ•°å†™å…¨[&#47;mnt&#47;xx&#47;xx&#47;openchat_3.5_tokenizer.json]</p>2024-01-15</li><br/><li><span>zhuxiufenghust</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>avx: false, neon: false, simd128: false, f16c: false
Error: unknown magic 0x6f64213c è¿™ä¸ªé”™è¯¯è¦æ€ä¹ˆè§£å†³</p>2024-03-25</li><br/><li><span>ä¸å¿˜åˆå¿ƒ</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆ, AI ç”µé”€, éœ€è¦ä»€ä¹ˆæ ·çš„æŠ€æœ¯æ ˆ?</p>2024-03-21</li><br/><li><span>xl000</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>candle-core 0.4ç‰ˆæœ¬, åªéœ€è¦æ”¹ä¸¤å¤„
tensor.ggml_dtype.blck_size() æ”¹ä¸º tensor.ggml_dtype.block_size()
from_gguf(model, &amp;mut file)é‚£è¡Œæ”¹ä¸º 
let device = Device::cuda_if_available(0)?;
let mut model = quantized_model::ModelWeights::from_gguf(model, &amp;mut file, &amp;device)?;</p>2024-03-20</li><br/><li><span>Geek_3b58b9</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>Candle è¦æ˜¯æ”¯æŒ ROCm å°±æ›´å¥½äº†</p>2024-02-04</li><br/><li><span>tan</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>WSL: linker `cc` not found å¤„ç†æ–¹å¼ï¼š sudo apt update &amp;&amp; sudo apt install build-essential</p>2024-01-15</li><br/><li><span>eriklee</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆèƒ½å¯¹æ¯”ä¸‹candleå’Œburnå—ï¼Ÿ
å¦å¤–ï¼Œæ„Ÿè§‰rustä¼˜åŠ¿æ˜¯è¾¹ç¼˜ç«¯æ¨ç†ï¼Œæ¯•ç«Ÿè¾¹ç¼˜ä¾§èµ„æºç´§å¼ . æœåŠ¡å™¨ç«¯æ¨ç†ï¼Œæ¯•ç«Ÿè¿˜æ˜¯æ¯”ä¸è¿‡pythonç”Ÿæ€</p>2024-01-13</li><br/><li><span>é¸ æ‘©æ™º</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œè¯·é—®ä¸€ä¸‹ï¼Œå¯ä»¥ä¸‹è½½åˆ«äººè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œé€šè¿‡candle æ¥å®ç°æ ¹æ®éœ€æ±‚æè¿°è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„è¿™ç§åŠŸèƒ½å—ï¼Ÿ</p>2024-01-06</li><br/><li><span>superggn</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>mac 2021 æ¬¾ intel èŠ¯ç‰‡ï¼Œ ç³»ç»Ÿç‰ˆæœ¬ Montery 12.5.1 ä¼šå¡ä½ï¼Œ model built ä¹‹åè¾“å…¥ä¸€ä¸ª hello å°±ä¸åŠ¨äº†ï¼Œ è¦è¿‡ 5 åˆ†é’Ÿä»¥ä¸Šæ‰ä¼šæœ‰å›å¤</p>2024-01-02</li><br/><li><span>Geek_54dac1</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¯¥æ¨¡å‹åœ¨ Candle ä¸­è¿˜æš‚æ—¶ä¸æ”¯æŒåœ¨ GPU ä¸Šè¿è¡Œï¼Œå› ä¸º Quantized models on Cuda è¿˜ä¸æ”¯æŒï¼Œå‚è€ƒï¼šhttps:&#47;&#47;github.com&#47;huggingface&#47;candle&#47;issues&#47;1250ï¼›é¿å…å¤§å®¶æŒ–å‘</p>2023-12-15</li><br/><li><span>æ— é™å¯èƒ½</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>è€å¸ˆå¥½ï¼Œå¤§æ¨¡å‹å°ç™½æœ‰å‡ ä¸ªé—®é¢˜ã€‚é€šè¿‡ huggingface-cli scan-cacheï¼Œæ‰«æäº†ä¸€ä¸‹ huggingface ä¸‹è½½è¿‡çš„æ–‡ä»¶ï¼Œéƒ½æ˜¯ 7G æˆ–è€…æ›´å¤§ï¼š
1. è¿™äº›æ¨¡å‹æ–‡ä»¶é‡Œä¸»è¦æ˜¯ä»€ä¹ˆå†…å®¹ï¼Œæ˜¯åŒ…æ‹¬äº†æ•°æ®ä¹ˆï¼Ÿä¸ºå•¥ä¼šè¿™ä¹ˆå¤§ï¼Ÿ
2. ç±»ä¼¼è¿™ä¹ˆå¤§çš„æ–‡ä»¶ï¼Œä¸€å®šè¦ä¸‹è½½åˆ°æœ¬åœ°ä¹ˆï¼Œæ˜¯å¦å¯ä»¥äº‘éƒ¨ç½²ä¹‹ç±»ï¼Œå¯ä»¥ç”¨å®Œé”€æ¯ã€‚
3. æˆ‘ç†è§£ï¼Œæˆ‘æƒ³è¦è¿è¡Œæœ¬èŠ‚è¯¾çš„demoï¼Œè¿™ 7G çš„ç©ºé—´å°±æ˜¯è¦é•¿æœŸè§„åˆ’å‡ºå»äº†å§</p>2023-12-15</li><br/><li><span>å­¦æ°´</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¿™å¼ æ„Ÿè§‰ä¸»è¦æ˜¯ç»™å¯¹rustæœ‰å…´è¶£çš„mleï¼Œä¸­é—´ä¸€äº›ä»£ç å¦‚æœä¸æ˜¯å› ä¸ºç”¨è¿‡pytorchï¼Œå®Œå…¨ä¸çŸ¥é“åœ¨å¹²å•¥</p>2023-12-15</li><br/><li><span>åˆ˜ä¸¹</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œä½ å¥½ï¼Œè¯·é—®èƒ½å¦æ¨èå‡ ä¸ªå’Œ ChatGPT äº¤äº’çš„ Rust SDK ? æœ€å¥½ä¹Ÿèƒ½æ”¯æŒå…¶å®ƒ LLM ã€‚</p>2023-12-15</li><br/>
</ul>