ä½ å¥½ï¼Œæˆ‘æ˜¯å¾æ–‡æµ©ã€‚

ä¸çŸ¥é“è¯¾ç¨‹ä¸Šåˆ°è¿™é‡Œï¼Œä½ è´¦æˆ·é‡Œå…è´¹çš„5ç¾å…ƒçš„é¢åº¦è¿˜å‰©ä¸‹å¤šå°‘äº†ï¼Ÿå¦‚æœä½ å°è¯•ç€å®Œæˆæˆ‘ç»™çš„å‡ ä¸ªæ•°æ®é›†é‡Œçš„æ€è€ƒé¢˜ï¼Œç›¸ä¿¡è¿™ä¸ªé¢åº¦åº”è¯¥æ˜¯ä¸å¤ªå¤Ÿç”¨çš„ã€‚è€ŒChatCompletionçš„æ¥å£ï¼Œåˆéœ€è¦ä¼ å…¥å¤§é‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®é™…æ¶ˆè€—çš„Tokenæ•°é‡å…¶å®æ¯”æˆ‘ä»¬æ„Ÿè§‰çš„è¦å¤šã€‚

è€Œä¸”ï¼Œé™¤äº†è´¹ç”¨ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯æ•°æ®å®‰å…¨ã€‚å› ä¸ºæ¯ä¸ªå›½å®¶çš„æ•°æ®ç›‘ç®¡è¦æ±‚ä¸åŒï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„æ•°æ®ï¼Œéƒ½é€‚åˆé€šè¿‡OpenAIçš„APIæ¥å¤„ç†çš„ã€‚æ‰€ä»¥ï¼Œä»è¿™ä¸¤ä¸ªè§’åº¦å‡ºå‘ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªOpenAIä»¥å¤–çš„è§£å†³æ–¹æ¡ˆã€‚é‚£å¯¹äºæ²¡æœ‰è¶³å¤ŸæŠ€æœ¯å‚¨å¤‡çš„ä¸­å°å‹å…¬å¸æ¥è¯´ï¼Œæœ€å¯è¡Œçš„ä¸€ä¸ªæ€è·¯å°±æ˜¯åˆ©ç”¨å¥½å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ã€‚

## åœ¨Colabé‡Œä½¿ç”¨GPU

å› ä¸ºè¿™ä¸€è®²æˆ‘ä»¬è¦ä½¿ç”¨ä¸€äº›å¼€æºæ¨¡å‹ï¼Œä½†ä¸æ˜¯æ‰€æœ‰äººçš„ç”µè„‘é‡Œéƒ½æœ‰ä¸€ä¸ªå¼ºåŠ²çš„NVidia GPUçš„ã€‚æ‰€ä»¥ï¼Œæˆ‘å»ºè®®ä½ é€šè¿‡Colabæ¥è¿è¡Œå¯¹åº”çš„Notebookï¼Œå¹¶ä¸”æ³¨æ„ï¼Œè¦æŠŠå¯¹åº”çš„è¿è¡Œç¯å¢ƒè®¾ç½®æˆGPUã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/1c/21/1c0791bd5c1e088eeb527f2acb81a021.png?wh=1255x584)

1. ä½ å…ˆé€‰æ‹©èœå•æ é‡Œçš„Runtimeï¼Œç„¶åç‚¹å‡»Change runtime typeã€‚

<!--THE END-->

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/50/23/502a4baceab267e949957c6477bc5823.png?wh=1257x489)

2. ç„¶ååœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†é‡Œï¼ŒæŠŠHardware acceleratoræ¢æˆGPUï¼Œç„¶åç‚¹å‡»Saveå°±å¯ä»¥äº†ã€‚

åªè¦ç”¨å¾—ä¸æ˜¯å¤ªå¤šï¼ŒColabçš„GPUæ˜¯å¯ä»¥å…è´¹ä½¿ç”¨çš„ã€‚

## HuggingfaceEmbeddingï¼Œä½ çš„å¼€æºä¼™ä¼´

å…¶å®æˆ‘ä»¬ä¹‹å‰åœ¨ [ç¬¬ 4 è®²](https://time.geekbang.org/column/article/642224)å¯¹æ¯”é›¶æ ·æœ¬åˆ†ç±»æ•ˆæœçš„æ—¶å€™ï¼Œå°±å·²ç»ä½¿ç”¨è¿‡Googleå¼€æºçš„æ¨¡å‹T5äº†ã€‚é‚£ä¸ªæ¨¡å‹çš„æ•ˆæœï¼Œè™½ç„¶æ¯”OpenAIçš„APIè¿˜æ˜¯è¦å·®ä¸€äº›ï¼Œä½†æ˜¯å…¶å®90%çš„å‡†ç¡®ç‡ä¹Ÿè¿˜ç®—ä¸é”™äº†ã€‚é‚£ä¹ˆè”æƒ³ä¸€ä¸‹ï¼Œä¸Šä¸€è®²æˆ‘ä»¬ä½¿ç”¨çš„llama-indexå‘é‡æœç´¢éƒ¨åˆ†ï¼Œæ˜¯ä¸æ˜¯å¯ä»¥ç”¨å¼€æºæ¨¡å‹çš„Embeddingç»™æ›¿æ¢æ‰å‘¢ï¼Ÿ

å½“ç„¶æ˜¯å¯ä»¥çš„ï¼Œllama-indexæ”¯æŒä½ è‡ªå·±ç›´æ¥å®šä¹‰ä¸€ä¸ªå®šåˆ¶åŒ–çš„Embeddingï¼Œå¯¹åº”çš„ä»£ç æˆ‘æ”¾åœ¨äº†ä¸‹é¢ã€‚

```python
conda install -c conda-forge sentence-transformers
```

æ³¨ï¼šæˆ‘ä»¬éœ€è¦å…ˆå®‰è£…ä¸€ä¸‹sentence-transformersè¿™ä¸ªåº“ã€‚

```python
import openai, os
import faiss
from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTFaissIndex, ServiceContext
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from llama_index.node_parser import SimpleNodeParser

openai.api_key = ""

text_splitter = CharacterTextSplitter(separator="\n\n", chunk_size=100, chunk_overlap=20)
parser = SimpleNodeParser(text_splitter=text_splitter)
documents = SimpleDirectoryReader('./data/faq/').load_data()
nodes = parser.get_nodes_from_documents(documents)

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
))
service_context = ServiceContext.from_defaults(embed_model=embed_model)

dimension = 768
faiss_index = faiss.IndexFlatIP(dimension)
index = GPTFaissIndex(nodes=nodes,faiss_index=faiss_index, service_context=service_context)
```

è¾“å‡ºç»“æœï¼š

```python
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu
WARNING:root:Created a chunk of size 130, which is longer than the specified 100
â€¦â€¦
INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens
INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 3198 tokens
```

åœ¨è¿™ä¸ªä¾‹å­é‡Œé¢ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªé¢å‘ç”µå•†çš„FAQçš„çº¯æ–‡æœ¬æ–‡ä»¶ä½œä¸ºè¾“å…¥ã€‚é‡Œé¢æ˜¯ä¸€ç³»åˆ—é¢„è®¾å¥½çš„FAQé—®ç­”å¯¹ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨OpenAIçš„APIï¼Œæˆ‘ä»¬å…ˆæŠŠopenai.api\_keyç»™è®¾æˆäº†ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ã€‚ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªembeded\_modelï¼Œè¿™ä¸ªembeded\_modelé‡Œé¢ï¼Œæˆ‘ä»¬åŒ…è£…çš„æ˜¯ä¸€ä¸ªHuggingFaceEmbeddingsçš„ç±»ã€‚

å› ä¸ºHuggingFaceä¸ºåŸºäºtransformersçš„æ¨¡å‹å®šä¹‰äº†ä¸€ä¸ªæ ‡å‡†ï¼Œæ‰€ä»¥å¤§éƒ¨åˆ†æ¨¡å‹ä½ åªéœ€è¦ä¼ å…¥ä¸€ä¸ªæ¨¡å‹åç§°ï¼ŒHuggingFacebEmbeddingè¿™ä¸ªç±»å°±ä¼šä¸‹è½½æ¨¡å‹ã€åŠ è½½æ¨¡å‹ï¼Œå¹¶é€šè¿‡æ¨¡å‹æ¥è®¡ç®—ä½ è¾“å…¥çš„æ–‡æœ¬çš„Embeddingã€‚ä½¿ç”¨HuggingFaceçš„å¥½å¤„æ˜¯ï¼Œä½ å¯ä»¥é€šè¿‡ä¸€å¥—ä»£ç ä½¿ç”¨æ‰€æœ‰çš„transfomersç±»å‹çš„æ¨¡å‹ã€‚

[sentence-transformers](https://sbert.net/) æ˜¯ç›®å‰æ•ˆæœæœ€å¥½çš„è¯­ä¹‰æœç´¢ç±»çš„æ¨¡å‹ï¼Œå®ƒåœ¨BERTçš„åŸºç¡€ä¸Šé‡‡ç”¨äº†å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œæ¥åŒºåˆ†æ–‡æœ¬è¯­ä¹‰çš„ç›¸ä¼¼åº¦ï¼Œå®ƒåŒ…æ‹¬äº†ä¸€ç³»åˆ—çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œï¼Œé€‰ç”¨çš„æ˜¯ sentence-transformersä¸‹é¢çš„ paraphrase-multilingual-mpnet-base-v2Â æ¨¡å‹ã€‚é¡¾åæ€ä¹‰ï¼Œè¿™ä¸ªæ˜¯ä¸€ä¸ªæ”¯æŒå¤šè¯­è¨€ï¼ˆmultilingualï¼‰å¹¶ä¸”èƒ½æŠŠè¯­å¥å’Œæ®µè½ï¼ˆparaphraseï¼‰å˜æˆå‘é‡çš„ä¸€ä¸ªæ¨¡å‹ã€‚å› ä¸ºæˆ‘ä»¬ç»™çš„ç¤ºä¾‹éƒ½æ˜¯ä¸­æ–‡ï¼Œæ‰€ä»¥é€‰å–äº†è¿™ä¸ªæ¨¡å‹ã€‚ä½ å¯ä»¥æ ¹æ®ä½ è¦è§£å†³çš„å®é™…é—®é¢˜ï¼Œæ¥é€‰å–ä¸€ä¸ªé€‚åˆè‡ªå·±çš„æ¨¡å‹ã€‚

æˆ‘ä»¬è¿˜æ˜¯ä½¿ç”¨Faissè¿™ä¸ªåº“æ¥ä½œä¸ºæˆ‘ä»¬çš„å‘é‡ç´¢å¼•åº“ï¼Œæ‰€ä»¥éœ€è¦æŒ‡å®šä¸€ä¸‹å‘é‡çš„ç»´åº¦ï¼Œparaphrase-multilingual-mpnet-base-v2Â è¿™ä¸ªæ¨¡å‹çš„ç»´åº¦æ˜¯768ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±æŠŠç»´åº¦å®šä¹‰æˆ768ç»´ã€‚

ç›¸åº”çš„å¯¹æ–‡æ¡£çš„åˆ‡åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯CharacterTextSplitterï¼Œå¹¶ä¸”åœ¨å‚æ•°ä¸Šæˆ‘ä»¬åšäº†ä¸€äº›è°ƒæ•´ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬æŠŠâ€œ\\n\\nâ€è¿™æ ·ä¸¤ä¸ªè¿ç»­çš„æ¢è¡Œç¬¦ä½œä¸ºä¸€æ®µæ®µæ–‡æœ¬çš„åˆ†éš”ç¬¦ï¼Œå› ä¸ºæˆ‘ä»¬çš„FAQæ•°æ®é‡Œï¼Œæ¯ä¸€ä¸ªé—®ç­”å¯¹éƒ½æœ‰ä¸€ä¸ªç©ºè¡Œéš”å¼€ï¼Œæ­£å¥½æ˜¯è¿ç»­ä¸¤ä¸ªæ¢è¡Œã€‚

ç„¶åï¼Œæˆ‘ä»¬æŠŠchunk\_sizeè®¾ç½®å¾—æ¯”è¾ƒå°ï¼Œåªæœ‰100ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ‰€ä½¿ç”¨çš„å¼€æºæ¨¡å‹æ˜¯ä¸ªå°æ¨¡å‹ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½åœ¨å•æœºåŠ è½½èµ·æ¥ã€‚å®ƒèƒ½å¤Ÿæ”¯æŒçš„è¾“å…¥é•¿åº¦æœ‰é™ï¼Œåªæœ‰128ä¸ªTokenï¼Œè¶…å‡ºçš„éƒ¨åˆ†ä¼šè¿›è¡Œæˆªæ–­å¤„ç†ã€‚å¦‚æœæˆ‘ä»¬ä¸è®¾ç½®chunk\_sizeï¼Œllama-indexä¼šè‡ªåŠ¨åˆå¹¶å¤šä¸ªchunkå˜æˆä¸€ä¸ªæ®µè½ã€‚

å…¶æ¬¡ï¼Œæˆ‘ä»¬è¿˜å¢åŠ äº†ä¸€ä¸ªå°å°çš„å‚æ•°ï¼Œå«åšchunk\_overlapã€‚è¿™ä¸ªå‚æ•°ä»£è¡¨æˆ‘ä»¬è‡ªåŠ¨åˆå¹¶å°çš„æ–‡æœ¬ç‰‡æ®µçš„æ—¶å€™ï¼Œå¯ä»¥æ¥å—å¤šå¤§ç¨‹åº¦çš„é‡å ã€‚å®ƒçš„é»˜è®¤å€¼æ˜¯200ï¼Œè¶…è¿‡äº†å•æ®µæ–‡æ¡£çš„chunk\_sizeï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œè¦æŠŠå®ƒè®¾å°ä¸€ç‚¹ï¼Œä¸ç„¶ç¨‹åºä¼šæŠ¥é”™ã€‚

æˆ‘ä»¬å¯ä»¥åœ¨å¯¹åº”çš„verboseæ—¥å¿—é‡Œçœ‹åˆ°ï¼Œè¿™é‡Œçš„Embeddingä½¿ç”¨äº†3198ä¸ªTokenï¼Œä¸è¿‡è¿™äº›Tokenéƒ½æ˜¯æˆ‘ä»¬é€šè¿‡sentence\_transformersç±»å‹çš„å¼€æºæ¨¡å‹è®¡ç®—çš„ï¼Œä¸éœ€è¦èŠ±é’±ã€‚ä½ çš„æˆæœ¬å°±èŠ‚çº¦ä¸‹æ¥äº†ã€‚

åœ¨åˆ›å»ºå®Œæ•´ä¸ªç´¢å¼•ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥æ‹¿ä¸€äº›å¸¸è§çš„ç”µå•†ç±»å‹çš„FAQé—®é¢˜è¯•ä¸€è¯•ã€‚

é—®é¢˜1ï¼š

```plain
from llama_index import QueryMode

openai.api_key = os.environ.get("OPENAI_API_KEY")

response = index.query(
    "è¯·é—®ä½ ä»¬æµ·å—èƒ½å‘è´§å—ï¼Ÿ", 
    mode=QueryMode.EMBEDDING,
    verbose=True, 
)
print(response)
```

è¾“å‡ºç»“æœï¼š

```plain
> Got node text: Q: æ”¯æŒå“ªäº›çœä»½é…é€ï¼Ÿ
A: æˆ‘ä»¬æ”¯æŒå…¨å›½å¤§éƒ¨åˆ†çœä»½çš„é…é€ï¼ŒåŒ…æ‹¬åŒ—äº¬ã€ä¸Šæµ·ã€å¤©æ´¥ã€é‡åº†ã€æ²³åŒ—ã€å±±è¥¿ã€è¾½å®ã€å‰æ—ã€é»‘é¾™æ±Ÿã€æ±Ÿè‹ã€æµ™æ±Ÿã€å®‰å¾½ã€ç¦å»ºã€æ±Ÿè¥¿ã€å±±ä¸œã€æ²³å—ã€æ¹–åŒ—ã€æ¹–å—ã€å¹¿ä¸œã€æµ·å—ã€å››å·ã€è´µå·ã€äº‘å—ã€é™•è¥¿ã€ç”˜è‚ƒã€é’æµ·ã€å°æ¹¾ã€å†…è’™å¤ã€å¹¿è¥¿ã€è¥¿è—ã€å®å¤å’Œæ–°ç–†...

INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 341 tokens
INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 24 tokens

æ˜¯çš„ï¼Œæˆ‘ä»¬æ”¯æŒæµ·å—çœçš„é…é€ã€‚
```

é—®é¢˜2ï¼š

```plain
response = index.query(
    "ä½ ä»¬ç”¨å“ªäº›å¿«é€’å…¬å¸é€è´§ï¼Ÿ", 
    mode=QueryMode.EMBEDDING,
    verbose=True, 
)
print(response)
```

è¾“å‡ºç»“æœï¼š

```plain
> Got node text: Q: æä¾›å“ªäº›å¿«é€’å…¬å¸çš„æœåŠ¡ï¼Ÿ
A: æˆ‘ä»¬ä¸é¡ºä¸°é€Ÿè¿ã€åœ†é€šé€Ÿé€’ã€ç”³é€šå¿«é€’ã€éŸµè¾¾å¿«é€’ã€ä¸­é€šå¿«é€’ã€ç™¾ä¸–å¿«é€’ç­‰å¤šå®¶çŸ¥åå¿«é€’å…¬å¸åˆä½œã€‚...
INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 281 tokens
INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 27 tokens

æˆ‘ä»¬ä¸é¡ºä¸°é€Ÿè¿ã€åœ†é€šé€Ÿé€’ã€ç”³é€šå¿«é€’ã€éŸµè¾¾å¿«é€’ã€ä¸­é€šå¿«é€’ã€ç™¾ä¸–å¿«é€’ç­‰å¤šå®¶çŸ¥åå¿«é€’å…¬å¸åˆä½œï¼Œç”¨ä»–ä»¬çš„æœåŠ¡é€è´§ã€‚
```

é—®é¢˜3ï¼š

```plain
response = index.query(
    "ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿ", 
    mode=QueryMode.EMBEDDING,
    verbose=True, 
)
print(response)
```

è¾“å‡ºç»“æœï¼š

```plain
> Got node text: Q: é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ
A: è‡ªæ”¶åˆ°å•†å“ä¹‹æ—¥èµ·7å¤©å†…ï¼Œå¦‚äº§å“æœªä½¿ç”¨ã€åŒ…è£…å®Œå¥½ï¼Œæ‚¨å¯ä»¥ç”³è¯·é€€è´§ã€‚æŸäº›ç‰¹æ®Šå•†å“å¯èƒ½ä¸æ”¯æŒé€€è´§ï¼Œè¯·åœ¨è´­ä¹°å‰æŸ¥çœ‹å•†å“è¯¦æƒ…é¡µé¢çš„é€€è´§æ”¿ç­–ã€‚...
INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 393 tokens
INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 27 tokens

æˆ‘ä»¬çš„é€€è´§æ”¿ç­–æ˜¯è‡ªæ”¶åˆ°å•†å“ä¹‹æ—¥èµ·7å¤©å†…ï¼Œå¦‚äº§å“æœªä½¿ç”¨ã€åŒ…è£…å®Œå¥½ï¼Œæ‚¨å¯ä»¥ç”³è¯·é€€è´§ã€‚æŸäº›ç‰¹æ®Šå•†å“å¯èƒ½ä¸æ”¯æŒé€€è´§ï¼Œè¯·åœ¨è´­ä¹°å‰æŸ¥çœ‹å•†å“è¯¦æƒ…é¡µé¢çš„é€€è´§æ”¿ç­–ã€‚
```

æˆ‘ä»¬åœ¨é—®é—®é¢˜çš„æ—¶å€™ï¼ŒæŒ‡å®šäº†queryçš„modeæ˜¯Embeddingã€‚é€šè¿‡ä¸‰ä¸ªå¸¸ç”¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒAIéƒ½ç»™å‡ºäº†æ­£ç¡®çš„å›ç­”ï¼Œæ•ˆæœè¿˜æ˜¯ä¸é”™çš„ã€‚

## ä½¿ç”¨ChatGLMæä¾›å¯¹è¯æ•ˆæœ

é€šè¿‡ä¸Šé¢çš„ä»£ç ï¼Œæˆ‘ä»¬å·²ç»æŠŠç”ŸæˆEmbeddingä»¥åŠåˆ©ç”¨Embeddingçš„ç›¸ä¼¼åº¦è¿›è¡Œæœç´¢æå®šäº†ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨å®é™…é—®ç­”çš„è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨çš„è¿˜æ˜¯OpenAIçš„Completion APIã€‚é‚£ä¹ˆè¿™ä¸€éƒ¨åˆ†æˆ‘ä»¬æœ‰æ²¡æœ‰åŠæ³•ä¹Ÿæ›¿æ¢æ‰å‘¢ï¼Ÿ

åŒæ ·çš„ï¼Œæˆ‘ä»¬å¯»æ±‚å¼€æºæ¨¡å‹çš„å¸®åŠ©ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°±ä¸å¦¨æ¥è¯•ä¸€ä¸‹æ¥è‡ªæ¸…åå¤§å­¦çš„ChatGLMè¯­è¨€æ¨¡å‹ï¼Œçœ‹çœ‹ä¸­æ–‡çš„å¼€æºè¯­è¨€æ¨¡å‹ï¼Œæ˜¯ä¸æ˜¯ä¹Ÿæœ‰åŸºæœ¬çš„çŸ¥è¯†ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

é¦–å…ˆæˆ‘ä»¬è¿˜æ˜¯è¦å®‰è£…ä¸€äº›ä¾èµ–åŒ…ï¼Œå› ä¸ºicetkæˆ‘æ²¡æœ‰æ‰¾åˆ°Condaçš„æºï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œé€šè¿‡pipæ¥å®‰è£…ï¼Œä½†æ˜¯åœ¨Condaçš„åŒ…ç®¡ç†å™¨é‡Œä¸€æ ·èƒ½å¤Ÿçœ‹åˆ°ã€‚

```python
pip install icetk
pip install cpm_kernels
```

ç„¶åï¼Œæˆ‘ä»¬è¿˜æ˜¯å…ˆé€šè¿‡transformersæ¥åŠ è½½æ¨¡å‹ã€‚[ChatGLM](https://github.com/THUDM/GLM-130B) æœ€å¤§çš„ä¸€ä¸ªæ¨¡å‹æœ‰1300äº¿ä¸ªå‚æ•°ã€‚

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).half().cuda()
model = model.eval()
```

è¾“å‡ºç»“æœï¼š

```python
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
No compiled kernel found.
Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels.c
Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels.so
Kernels compiled : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels.so
Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels.so
Using quantization cache
Applying quantization to glm layers
```

ä½†æ˜¯è¿™ä¹ˆå¤§çš„æ¨¡å‹ï¼Œæ— è®ºæ˜¯ä½ è‡ªå·±çš„ç”µè„‘ï¼Œè¿˜æ˜¯Colabæä¾›çš„GPUå’ŒTPUæ˜¾ç„¶éƒ½æ”¾ä¸äº†ã€‚æ‰€ä»¥æˆ‘ä»¬åªèƒ½é€‰ç”¨ä¸€ä¸ªè£å‰ªåçš„60äº¿ä¸ªå‚æ•°çš„ç‰ˆæœ¬ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜å¿…é¡»ç”¨int-4é‡åŒ–çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯ç”¨float16çš„æµ®ç‚¹æ•°ã€‚æ‰€ä»¥ï¼Œè¿™é‡Œæˆ‘ä»¬çš„æ¨¡å‹åå­—å°±å«åš chatglm-6b-int4ï¼Œä¹Ÿå°±æ˜¯ 6Bçš„å‚æ•°é‡ï¼Œé€šè¿‡int-4é‡åŒ–ã€‚ç„¶åï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡GPUè¿›è¡Œæ¨¡å‹çš„è®¡ç®—ï¼Œæ‰€ä»¥åŠ è½½æ¨¡å‹çš„æ—¶å€™è°ƒç”¨äº†.cuda()ã€‚

è¿™é‡ŒåŠ è½½æ¨¡å‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¿˜è®¾ç½®äº†ä¸€ä¸ª trust\_remote\_code = true çš„å‚æ•°ï¼Œè¿™æ˜¯å› ä¸ºChatGLMçš„æ¨¡å‹ä¸æ˜¯ä¸€ä¸ªHuggingfaceå®˜æ–¹å‘å¸ƒçš„æ¨¡å‹ï¼Œè€Œæ˜¯ç”±ç”¨æˆ·è´¡çŒ®çš„ï¼Œæ‰€ä»¥éœ€è¦ä½ æ˜¾å¼ç¡®è®¤ä½ ä¿¡ä»»è¿™ä¸ªæ¨¡å‹çš„ä»£ç ï¼Œå®ƒä¸ä¼šé€ æˆæ¶æ„çš„ç ´åã€‚æˆ‘ä»¬åæ­£æ˜¯åœ¨Colabé‡Œé¢è¿è¡Œè¿™ä¸ªä»£ç ï¼Œæ‰€ä»¥å€’æ˜¯ä¸ç”¨å¤ªæ‹…å¿ƒã€‚

å¦‚æœä½ æƒ³è¦ç”¨CPUè¿è¡Œï¼Œå¯ä»¥æŠŠæ¨¡å‹åŠ è½½çš„ä»£ç æ¢æˆä¸‹é¢è¿™æ ·ã€‚

```python
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4",trust_remote_code=True).float()
```

ä¸è¿‡ï¼Œæˆ‘ä¸å»ºè®®ä½ è¿™ä¹ˆåšã€‚ä½ æ²¡æœ‰GPUçš„è¯ï¼Œè¿˜æ˜¯ç›´æ¥ä½¿ç”¨Colabçš„GPUå°±å¥½äº†ã€‚å› ä¸ºCPUåœ¨è¿è¡Œå¯¹è¯çš„æ—¶å€™éå¸¸æ…¢ã€‚

åœ¨æ‹¿åˆ°æ¨¡å‹ä¹‹åæˆ‘ä»¬å°±å¯ä»¥å°è¯•ç€é€šè¿‡è¿™ä¸ªæ¨¡å‹æ¥è¿›è¡Œé—®ç­”äº†ã€‚

é—®é¢˜1ï¼š

```python
question = """
è‡ªæ”¶åˆ°å•†å“ä¹‹æ—¥èµ·7å¤©å†…ï¼Œå¦‚äº§å“æœªä½¿ç”¨ã€åŒ…è£…å®Œå¥½ï¼Œæ‚¨å¯ä»¥ç”³è¯·é€€è´§ã€‚æŸäº›ç‰¹æ®Šå•†å“å¯èƒ½ä¸æ”¯æŒé€€è´§ï¼Œè¯·åœ¨è´­ä¹°å‰æŸ¥çœ‹å•†å“è¯¦æƒ…é¡µé¢çš„é€€è´§æ”¿ç­–ã€‚

æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œè¯·å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š

Q: ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿ
"""
response, history = model.chat(tokenizer, question, history=[])
print(response)
```

è¾“å‡ºç»“æœï¼š

```python
æˆ‘ä»¬çš„é€€è´§æ”¿ç­–æ˜¯åœ¨äº§å“æœªä½¿ç”¨ã€åŒ…è£…å®Œå¥½çš„æƒ…å†µä¸‹ï¼Œè‡ªæ”¶åˆ°å•†å“ä¹‹æ—¥èµ·7å¤©å†…å¯ä»¥é€€è´§ã€‚è¯·æ³¨æ„ï¼ŒæŸäº›ç‰¹æ®Šå•†å“å¯èƒ½ä¸æ”¯æŒé€€è´§ï¼Œåœ¨è´­ä¹°å‰è¯·æŸ¥çœ‹å•†å“è¯¦æƒ…é¡µé¢çš„é€€è´§æ”¿ç­–ï¼Œä»¥äº†è§£å…·ä½“æƒ…å†µã€‚
```

å¯ä»¥çœ‹åˆ°ï¼ŒChatGLMçš„å›ç­”ï¼Œçš„ç¡®æ˜¯æŒ‰ç…§æˆ‘ä»¬æç¤ºè¯­é‡Œé¢æœ‰çš„ä¸Šä¸‹æ–‡ç»™å‡ºçš„ã€‚æˆ‘ä»¬å¯ä»¥è¯•ç€å»æ‰æä¾›çš„FAQçš„å†…å®¹ï¼Œçœ‹çœ‹å®ƒçš„å›ç­”æ˜¯æ€æ ·çš„ã€‚

é—®é¢˜2ï¼š

```python
question = """
Q: ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿ
A: 
"""
response, history = model.chat(tokenizer, question, history=[])
print(response)
```

è¾“å‡ºç»“æœï¼š

```python
æˆ‘ä»¬å…¬å¸çš„æ”¿ç­–æ˜¯ï¼Œå¦‚æœæ‚¨åœ¨è´­ä¹°åçš„7å¤©å†…å‘ç°æˆ‘ä»¬çš„äº§å“ä¸ç¬¦åˆæ‚¨çš„æœŸæœ›ï¼Œå¯ä»¥ç”³è¯·é€€è´§ã€‚é€€è´§æ¡ä»¶å¦‚ä¸‹ï¼š
1. ç¬¦åˆé€€è´§æ”¿ç­–è¦æ±‚çš„äº§å“ï¼›
2. æœªä½¿ç”¨æˆ–æŸåçš„äº§å“ï¼›
3. ç¬¦åˆé€€è´§åœ°ç‚¹çš„è¦æ±‚ï¼Œå³é€€è´§åœ°å€ä¸è´­ä¹°åœ°å€ç›¸åŒï¼›
4. ç¬¦åˆé€€è´§æµç¨‹çš„è¦æ±‚ï¼Œå³æ‚¨éœ€è¦æä¾›æœ‰æ•ˆçš„é€€è´§å•å¹¶å®Œæˆé€€è´§æµç¨‹ã€‚
å¦‚æœç¬¦åˆä¸Šè¿°é€€è´§æ”¿ç­–è¦æ±‚ï¼Œæˆ‘ä»¬å°†å°½å¿«å¤„ç†æ‚¨çš„é€€è´§ç”³è¯·ï¼Œå¹¶ä¸ºæ‚¨é€€æ¬¾ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨å·²ç»ä½¿ç”¨äº†æˆ‘ä»¬çš„äº§å“ï¼Œæˆ–è€…äº§å“æœ‰æŸåæˆ–å˜è´¨ç­‰æƒ…å†µï¼Œæˆ‘ä»¬å°†æ— æ³•ä¸ºæ‚¨æä¾›é€€è´§æœåŠ¡ã€‚
```

å¯ä»¥çœ‹åˆ°ï¼Œåœ¨æ²¡æœ‰åˆé€‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ—¶å€™ï¼Œå®ƒç¼–é€ å‡ºäº†ä¸€äº›åŸæœ¬æ²¡æœ‰çš„è¦æ±‚ï¼Œæ¯”å¦‚â€œé€€è´§åœ°å€ä¸è´­ä¹°åœ°å€ç›¸åŒâ€ã€‚

æˆ‘ä»¬å†æ¥çœ‹ä¸€ä¸ªä¾‹å­ï¼Œçœ‹çœ‹å®ƒèƒ½ä¸èƒ½æ‹¥æœ‰ç®€å•çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡é‡Œåªè¯´äº†å¯ä»¥é€åˆ°æµ·å—ï¼Œæ²¡æœ‰è¯´æ˜¯å¦æ”¯æŒä¸‰äºšè¿™ä¸ªåŸå¸‚ï¼Œçœ‹çœ‹è¿™ä¸ªæ—¶å€™AIèƒ½ä¸èƒ½å›ç­”å¯¹è¿™ä¸ªé—®é¢˜ã€‚

é—®é¢˜3ï¼š

```python
question = """
æˆ‘ä»¬æ”¯æŒå…¨å›½å¤§éƒ¨åˆ†çœä»½çš„é…é€ï¼ŒåŒ…æ‹¬åŒ—äº¬ã€ä¸Šæµ·ã€å¤©æ´¥ã€é‡åº†ã€æ²³åŒ—ã€å±±è¥¿ã€è¾½å®ã€å‰æ—ã€é»‘é¾™æ±Ÿã€æ±Ÿè‹ã€æµ™æ±Ÿã€å®‰å¾½ã€ç¦å»ºã€æ±Ÿè¥¿ã€å±±ä¸œã€æ²³å—ã€æ¹–åŒ—ã€æ¹–å—ã€å¹¿ä¸œã€æµ·å—ã€å››å·ã€è´µå·ã€äº‘å—ã€é™•è¥¿ã€ç”˜è‚ƒã€é’æµ·ã€å°æ¹¾ã€å†…è’™å¤ã€å¹¿è¥¿ã€è¥¿è—ã€å®å¤å’Œæ–°ç–†.

æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œè¯·å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š

Q: ä½ ä»¬èƒ½é…é€åˆ°ä¸‰äºšå—ï¼Ÿ
"""
response, history = model.chat(tokenizer, question, history=[])
print(response)
```

è¾“å‡ºç»“æœï¼š

```python
æ˜¯çš„ï¼Œæˆ‘ä»¬æ”¯æŒå…¨å›½å¤§éƒ¨åˆ†çœä»½çš„é…é€ï¼ŒåŒ…æ‹¬ä¸‰äºšå¸‚ã€‚
```

å¯ä»¥çœ‹åˆ°ï¼ŒChatGLMçŸ¥é“æ˜¯å¯ä»¥é…é€åˆ°ä¸‰äºšçš„ã€‚ä¸è¿‡ä¸‡ä¸€æ˜¯å·§åˆå‘¢ï¼Ÿæˆ‘ä»¬å†çœ‹çœ‹åœ¨ä¸Šä¸‹æ–‡é‡Œé¢ï¼Œå»æ‰äº†ä¸œä¸‰çœï¼Œç„¶åé—®é—®å®ƒèƒ½ä¸èƒ½é€åˆ°å“ˆå°”æ»¨ã€‚

é—®é¢˜4ï¼š

```python
question = """
æˆ‘ä»¬æ”¯æŒå…¨å›½å¤§éƒ¨åˆ†çœä»½çš„é…é€ï¼ŒåŒ…æ‹¬åŒ—äº¬ã€ä¸Šæµ·ã€å¤©æ´¥ã€é‡åº†ã€æ²³åŒ—ã€å±±è¥¿ã€æ±Ÿè‹ã€æµ™æ±Ÿã€å®‰å¾½ã€ç¦å»ºã€æ±Ÿè¥¿ã€å±±ä¸œã€æ²³å—ã€æ¹–åŒ—ã€æ¹–å—ã€å¹¿ä¸œã€æµ·å—ã€å››å·ã€è´µå·ã€äº‘å—ã€é™•è¥¿ã€ç”˜è‚ƒã€é’æµ·ã€å°æ¹¾ã€å†…è’™å¤ã€å¹¿è¥¿ã€è¥¿è—ã€å®å¤å’Œæ–°ç–†.ä½†æ˜¯ä¸èƒ½é…é€åˆ°ä¸œä¸‰çœ

æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œè¯·å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š

Q: ä½ ä»¬èƒ½é…é€åˆ°å“ˆå°”æ»¨å—ï¼Ÿ
"""
response, history = model.chat(tokenizer, question, history=[])
print(response)
```

å›ç­”ï¼š

```python
å¾ˆæŠ±æ­‰ï¼Œæˆ‘ä»¬ç›®å‰ä¸èƒ½é…é€åˆ°å“ˆå°”æ»¨ã€‚
```

ç»“æœä¹Ÿæ˜¯æ­£ç¡®çš„ï¼Œè¿™ä¸ªæ—¶å€™ï¼ŒChatGLMä¼šå›ç­”æˆ‘ä»¬æ˜¯é€ä¸åˆ°å“ˆå°”æ»¨çš„ã€‚æ—¢ç„¶ChatGLMèƒ½å¤Ÿæ­£ç¡®å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œé‚£æˆ‘ä»¬çš„FAQé—®ç­”å°±å¯ä»¥ç”¨ChatGLMæ¥æå®šäº†ã€‚

## å°†ChatGLMå°è£…æˆLLM

ä¸è¿‡ä¸Šé¢çš„ä»£ç é‡Œé¢ï¼Œæˆ‘ä»¬ç”¨çš„è¿˜æ˜¯åŸå§‹çš„ChatGLMçš„æ¨¡å‹ä»£ç ï¼Œè¿˜ä¸èƒ½ç›´æ¥é€šè¿‡queryæ¥è®¿é—®llama-indexç›´æ¥å¾—åˆ°ç­”æ¡ˆã€‚è¦åšåˆ°è¿™ä¸€ç‚¹å€’ä¹Ÿä¸éš¾ï¼Œæˆ‘ä»¬æŠŠå®ƒå°è£…æˆä¸€ä¸ªLLMç±»ï¼Œè®©æˆ‘ä»¬çš„indexä½¿ç”¨è¿™ä¸ªæŒ‡å®šçš„å¤§è¯­è¨€æ¨¡å‹å°±å¥½äº†ã€‚å¯¹åº”çš„ [llama-index çš„æ–‡æ¡£](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html)ï¼Œä½ ä¹Ÿå¯ä»¥è‡ªå·±å»çœ‹ä¸€ä¸‹ã€‚

```python
import openai, os
import faiss
from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTFaissIndex, ServiceContext
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from llama_index.node_parser import SimpleNodeParser

from langchain.llms.base import LLM
from llama_index import LLMPredictor
from typing import Optional, List, Mapping, Any

class CustomLLM(LLM):
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response, history = model.chat(tokenizer, prompt, history=[])
        return response

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"name_of_model": "chatglm-6b-int4"}

    @property
    def _llm_type(self) -> str:
        return "custom"
```

æˆ‘ä»¬æŠŠè¿™ä¸ªCustomLLMå¯¹è±¡ï¼Œä¼ å…¥indexçš„æ„é€ å‡½æ•°é‡Œï¼Œé‡æ–°è¿è¡Œä¸€ä¸‹æˆ‘ä»¬çš„é—®é¢˜ï¼Œçœ‹çœ‹æ•ˆæœæ˜¯æ€æ ·çš„ã€‚

```python
from langchain.text_splitter import SpacyTextSplitter

llm_predictor = LLMPredictor(llm=CustomLLM())

text_splitter = CharacterTextSplitter(separator="\n\n", chunk_size=100, chunk_overlap=20)
parser = SimpleNodeParser(text_splitter=text_splitter)
documents = SimpleDirectoryReader('./drive/MyDrive/colab_data/faq/').load_data()
nodes = parser.get_nodes_from_documents(documents)

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
))
service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)

dimension = 768
faiss_index = faiss.IndexFlatIP(dimension)
index = GPTFaissIndex(nodes=nodes, faiss_index=faiss_index, service_context=service_context)
```

```python
from llama_index import QuestionAnswerPrompt
from llama_index import QueryMode

QA_PROMPT_TMPL = (
    "{context_str}"
    "\n\n"
    "æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œè¯·å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š\n"
    "Q: {query_str}\n"
    )
QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)

response = index.query(
    "è¯·é—®ä½ ä»¬æµ·å—èƒ½å‘è´§å—ï¼Ÿ", 
    mode=QueryMode.EMBEDDING,
    text_qa_template=QA_PROMPT,
    verbose=True, 
)
print(response)
```

è¾“å‡ºç»“æœï¼š

```python
> Got node text: Q: æ”¯æŒå“ªäº›çœä»½é…é€ï¼Ÿ
A: æˆ‘ä»¬æ”¯æŒå…¨å›½å¤§éƒ¨åˆ†çœä»½çš„é…é€ï¼ŒåŒ…æ‹¬åŒ—äº¬ã€ä¸Šæµ·ã€å¤©æ´¥ã€é‡åº†ã€æ²³åŒ—ã€å±±è¥¿ã€è¾½å®ã€å‰æ—ã€é»‘é¾™æ±Ÿã€æ±Ÿè‹ã€æµ™æ±Ÿã€å®‰å¾½ã€ç¦å»ºã€æ±Ÿè¥¿ã€å±±ä¸œã€æ²³å—ã€æ¹–åŒ—ã€æ¹–å—ã€å¹¿ä¸œã€æµ·å—ã€å››å·ã€è´µå·ã€äº‘å—ã€é™•è¥¿ã€ç”˜è‚ƒã€é’æµ·ã€å°æ¹¾ã€å†…è’™å¤ã€å¹¿è¥¿ã€è¥¿è—ã€å®å¤å’Œæ–°ç–†...

æµ·å—èƒ½å‘è´§ã€‚
```

å¯ä»¥çœ‹åˆ°ï¼Œè¿™æ ·å¤„ç†ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥ä½¿ç”¨ChatGLMçš„æ¨¡å‹ï¼Œæ¥è¿›è¡Œæˆ‘ä»¬çš„FAQçš„é—®ç­”äº†ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªé€šè¿‡paraphrase-multilingual-mpnet-base-v2æ¨¡å‹æ¥è®¡ç®—Embedddingå¹¶è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œç„¶åé€šè¿‡chatglm-6b-int4çš„æ¨¡å‹æ¥è¿›è¡Œé—®ç­”çš„è§£å†³æ–¹æ¡ˆäº†ã€‚è€Œä¸”è¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œå¯ä»¥è·‘åœ¨ä¸€å—å®¶ç”¨çº§åˆ«çš„æ˜¾å¡ä¸Šã€‚æ˜¯ä¸æ˜¯å¾ˆå‰å®³ï¼Ÿ

## å¼€æºæ¨¡å‹çš„ä¸è¶³ä¹‹å¤„

çœ‹èµ·æ¥ï¼Œæˆ‘ä»¬è¿™ä¸ªæœ¬æœºå°±èƒ½è¿è¡Œçš„å°æ¨¡å‹ä¼¼ä¹å·²ç»å®Œæˆäº†ã€‚æ•°æ®å®‰å…¨ï¼Œåˆä¸ç”¨æ‹…å¿ƒèŠ±è´¹ã€‚ä½†æ˜¾ç„¶ï¼Œäº‹æƒ…æ²¡æœ‰é‚£ä¹ˆç®€å•ã€‚å› ä¸ºåˆšæ‰æˆ‘ä»¬å¤„ç†çš„ç”µå•†FAQé—®é¢˜æ¯”è¾ƒç®€å•ï¼Œæˆ‘ä»¬å†æ‹¿ä¸€ä¸ªç¨å¾®å¤æ‚ä¸€ç‚¹çš„é—®é¢˜æ¥çœ‹çœ‹æ•ˆæœã€‚

```python
text_splitter = SpacyTextSplitter(pipeline="zh_core_web_sm", chunk_size = 128, chunk_overlap=32)
parser = SimpleNodeParser(text_splitter=text_splitter)
documents = SimpleDirectoryReader('./drive/MyDrive/colab_data/zhaohuaxishi/').load_data()
nodes = parser.get_nodes_from_documents(documents)

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
))
service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)

dimension = 768
faiss_index = faiss.IndexFlatIP(dimension)
index = GPTFaissIndex(nodes=nodes, faiss_index=faiss_index, service_context=service_context)
```

è¾“å‡ºç»“æœï¼š

```python
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu
â€¦â€¦
INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens
INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 91882 tokens
```

è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬è¾“å…¥ç´¢å¼•èµ·æ¥çš„æ•°æ®ï¼Œæ˜¯é²è¿…å…ˆç”Ÿæ•´å¥—ã€ŠæœèŠ±å¤•æ‹¾ã€‹çš„æ•£æ–‡é›†ã€‚é€‰ç”¨è¿™ä¸ªæ˜¯å› ä¸ºå¯¹åº”ä½œå“çš„ç‰ˆæƒå·²ç»è¿‡äº†ä¿æŠ¤æœŸã€‚æˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œåœ¨è¿™å¥—æ–‡é›†çš„å†…å®¹é‡Œé¢ï¼Œä½¿ç”¨æˆ‘ä»¬ä¸Šé¢çš„çº¯å¼€æºæ–¹æ¡ˆï¼Œæ•ˆæœä¼šæ˜¯æ€æ ·çš„ã€‚

å¯¹åº”çš„æ¨¡å‹å’Œç´¢å¼•åŠ è½½çš„ä»£ç åŸºæœ¬ä¸€è‡´ï¼Œåªæœ‰ä¸€ä¸ªå°å°çš„åŒºåˆ«ï¼Œå°±æ˜¯åœ¨æ–‡æœ¬åˆ†å‰²çš„æ—¶å€™ï¼Œæˆ‘ä»¬ç”¨äº†ä¸Šä¸€è®²ä»‹ç»è¿‡çš„SpacyTextSplitterï¼Œå› ä¸ºè¿™é‡Œéƒ½æ˜¯æ•£æ–‡çš„å†…å®¹ï¼Œè€Œä¸æ˜¯ç¡®å®šå¥½æ ¼å¼çš„QAå¯¹ã€‚æ‰€ä»¥é€šè¿‡SpacyTextSplitteræ¥åˆ†å¥ï¼Œå¹¶åœ¨å…è®¸çš„æ—¶å€™åˆå¹¶å°çš„ç‰‡æ®µæ˜¯æœ‰æ„ä¹‰çš„ã€‚

ç„¶åï¼Œæˆ‘ä»¬è¯•ç€é—®ä¸€ä¸‹ä¸Šä¸€è®²æˆ‘ä»¬é—®è¿‡çš„é—®é¢˜ï¼Œçœ‹çœ‹æ•ˆæœæ€ä¹ˆæ ·ã€‚

é—®é¢˜1ï¼š

```python
# query will use the same embed_model
from llama_index import QueryMode
from llama_index import QuestionAnswerPrompt

openai.api_key = os.environ.get("OPENAI_API_KEY")

QA_PROMPT_TMPL = (
    "ä¸‹é¢çš„å†…å®¹æ¥è‡ªé²è¿…å…ˆç”Ÿçš„æ•£æ–‡é›†ã€ŠæœèŠ±å¤•æ‹¾ã€‹ï¼Œå¾ˆå¤šå†…å®¹æ˜¯ä»¥ç¬¬ä¸€äººç§°å†™çš„ \n"
    "---------------------\n"
    "{context_str}"
    "\n---------------------\n"
    "æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œè¯·å›ç­”é—®é¢˜: {query_str}\n"
    "å¦‚æœæ‚¨ä¸çŸ¥é“çš„è¯ï¼Œè¯·å›ç­”ä¸çŸ¥é“\n"
)
QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)

response = index.query(
    "é²è¿…å…ˆç”Ÿåœ¨æ—¥æœ¬å­¦ä¹ åŒ»å­¦çš„è€å¸ˆæ˜¯è°ï¼Ÿ", 
    mode=QueryMode.EMBEDDING,
    similarity_top_k = 1,
    text_qa_template=QA_PROMPT,
    verbose=True, 
)
print(response)
```

è¾“å‡ºç»“æœï¼š

```python
> Got node text: ä¸€å°†ä¹¦æ”¾åœ¨è®²å°ä¸Šï¼Œä¾¿ç”¨äº†ç¼“æ…¢è€Œå¾ˆæœ‰é¡¿æŒ«çš„å£°è°ƒï¼Œå‘å­¦ç”Ÿä»‹ç»è‡ªå·±é“ï¼šâ€”â€” 
    â€œæˆ‘å°±æ˜¯å«ä½œè—¤é‡ä¸¥ä¹éƒçš„â€¦â€¦ã€‚â€

    
åé¢æœ‰å‡ ä¸ªäººç¬‘èµ·æ¥äº†ã€‚
ä»–æ¥ç€ä¾¿è®²è¿°è§£å‰–å­¦åœ¨æ—¥æœ¬å‘è¾¾çš„å†å²ï¼Œé‚£äº›å¤§å¤§å°å°çš„ä¹¦ï¼Œä¾¿æ˜¯ä»æœ€åˆåˆ°ç°ä»Šå…³äºè¿™ä¸€é—¨å­¦é—®çš„è‘—ä½œã€‚...

é²è¿…å…ˆç”Ÿåœ¨æ—¥æœ¬å­¦ä¹ åŒ»å­¦çš„è€å¸ˆæ˜¯è—¤é‡ä¸¥ä¹éƒã€‚
```

é—®é¢˜2ï¼š

```python
response = index.query(
    "é²è¿…å…ˆç”Ÿæ˜¯åœ¨æ—¥æœ¬çš„å“ªä¸ªåŸå¸‚å­¦ä¹ åŒ»å­¦çš„ï¼Ÿ", 
    mode=QueryMode.EMBEDDING, 
    similarity_top_k = 1,   
    text_qa_template=QA_PROMPT,
    verbose=True, 
)
print(response)
```

è¾“å‡ºç»“æœï¼š

```python
> Got node text: æœ‰æ—¶æˆ‘å¸¸å¸¸æƒ³ï¼šä»–çš„å¯¹äºæˆ‘çš„çƒ­å¿ƒçš„å¸Œæœ›ï¼Œä¸å€¦çš„æ•™è¯²ï¼Œå°è€Œè¨€ä¹‹ï¼Œæ˜¯ä¸ºä¸­å›½ï¼Œå°±æ˜¯å¸Œæœ›ä¸­å›½æœ‰æ–°çš„åŒ»å­¦ï¼›å¤§è€Œè¨€ä¹‹ï¼Œæ˜¯ä¸ºå­¦æœ¯ï¼Œå°±æ˜¯å¸Œæœ›æ–°çš„åŒ»å­¦ä¼ åˆ°ä¸­å›½å»ã€‚...

æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œæ— æ³•å¾—å‡ºé²è¿…å…ˆç”Ÿæ˜¯åœ¨æ—¥æœ¬çš„å“ªä¸ªåŸå¸‚å­¦ä¹ åŒ»å­¦çš„ç­”æ¡ˆã€‚
```

å¯ä»¥çœ‹åˆ°ï¼Œæœ‰äº›é—®é¢˜åœ¨è¿™ä¸ªæ¨¡å¼ä¸‹ï¼Œå®šä½åˆ°çš„æ–‡æœ¬ç‰‡æ®µæ˜¯æ­£ç¡®çš„ã€‚ä½†æ˜¯æœ‰äº›é—®é¢˜ï¼Œè™½ç„¶å®šä½çš„è¿˜ç®—æ˜¯ä¸€ä¸ªç›¸å…³çš„ç‰‡æ®µï¼Œä½†æ˜¯çš„ç¡®æ— æ³•å¾—å‡ºç­”æ¡ˆã€‚

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼š é‚£å°±æ˜¯å•æœºçš„å¼€æºå°æ¨¡å‹èƒ½å¤Ÿæ‰¿è½½çš„æ–‡æœ¬è¾“å…¥çš„é•¿åº¦é—®é¢˜ã€‚åœ¨æˆ‘ä»¬ä½¿ç”¨OpenAIçš„gpt-3.5-turboæ¨¡å‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ€é•¿æ”¯æŒ4096ä¸ªTokenï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µå¯ä»¥æ”¾ä¸Šä¸Šåƒå­—åœ¨é‡Œé¢ã€‚ä½†æ˜¯æˆ‘ä»¬è¿™é‡Œå•æœºç”¨çš„paraphrase-multilingual-mpnet-base-v2æ¨¡å‹ï¼Œåªèƒ½æ”¯æŒ128ä¸ªTokençš„è¾“å…¥ï¼Œè™½ç„¶å¯¹åº”çš„Tokenizerä¸ä¸€æ ·ï¼Œä½†æ˜¯å°±ç®—ä¸€ä¸ªå­—ä¸€ä¸ªTokenï¼Œä¹Ÿå°±100ä¸ªå­—è€Œå·²ã€‚è¿™ä½¿å¾—æˆ‘ä»¬æ£€ç´¢å‡ºæ¥çš„å†…å®¹çš„ä¸Šä¸‹æ–‡å¤ªå°‘äº†ï¼Œå¾ˆå¤šæ—¶å€™æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œè®©è¯­è¨€æ¨¡å‹å»å›ç­”ã€‚

å½“ç„¶ï¼Œè¿™ä¸ªé—®é¢˜å¹¶ä¸æ˜¯æ— æ³•å¼¥è¡¥çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æŠŠæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œéƒ¨ç½²åˆ°äº‘ç«¯æ¥è§£å†³ã€‚è¿™ä¸ªå†…å®¹ï¼Œæˆ‘ä»¬è¯¾ç¨‹çš„ç¬¬ä¸‰éƒ¨åˆ†ä¸“é—¨æœ‰ä¸€è®²ä¼šè®²è§£ã€‚

ä¸è¿‡ï¼Œæœ‰ä¸€ä¸ªæ›´éš¾è§£å†³çš„é—®é¢˜ï¼Œå°±æ˜¯æ¨¡å‹çš„æ¨ç†èƒ½åŠ›é—®é¢˜ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å†è¯•è¯•[ç¬¬ 1 è®²](https://time.geekbang.org/column/article/641742)é‡Œç»™å•†å“æ€»ç»“è‹±æ–‡åç§°å’Œå–ç‚¹çš„ä¾‹å­ã€‚

```python
question = """Consideration proudct : å·¥å‚ç°è´§PVCå……æ°”é’è›™å¤œå¸‚åœ°æ‘Šçƒ­å–å……æ°”ç©å…·å‘å…‰è›™å„¿ç«¥æ°´ä¸Šç©å…·

1. Compose human readale product title used on Amazon in english within 20 words.
2. Write 5 selling points for the products in Amazon.
3. Evaluate a price range for this product in U.S.

Output the result in json format with three properties called title, selling_points and price_range"""
response, history = model.chat(tokenizer, question, history=[])
print(response)
```

è¾“å‡ºç»“æœï¼š

```python
1. title: å……æ°”ç©å…·é’è›™å¤œå¸‚åœ°æ‘Šå–
2. selling_points:
    - å·¥å‚ç°è´§ï¼šä¿è¯äº§å“è´¨é‡
    - PVCå……æ°”ï¼šç¯ä¿è€ç”¨
    - å¤œå¸‚åœ°æ‘Šï¼šæ–¹ä¾¿é”€å”®
    - çƒ­å–ï¼šæœ€å—æ¬¢è¿äº§å“
    - å„¿ç«¥æ°´ä¸Šç©å…·ï¼šé€‚åˆå„ç§å¹´é¾„æ®µå„¿ç«¥
3. price_range: (in USD)
    - low:   $1.99
    - high:   $5.99
```

å¯ä»¥çœ‹åˆ°ï¼Œè™½ç„¶è¿™ä¸ªç»“æœä¸ç®—å¤ªç¦»è°±ï¼Œå¤šå°‘å’Œé—®é¢˜è¿˜æ˜¯æœ‰äº›å…³ç³»çš„ã€‚ä½†æ˜¯æ— è®ºæ˜¯ç¿»è¯‘æˆè‹±æ–‡ï¼Œè¿˜æ˜¯ä½¿ç”¨JSONè¿”å›ï¼Œæ¨¡å‹éƒ½æ²¡æœ‰åšåˆ°ã€‚ç»™åˆ°çš„å–ç‚¹ä¹Ÿæ²¡æœ‰ä»»ä½•â€œæ¨ç†å‡ºæ¥â€çš„æ€§è´¨ï¼Œéƒ½æ˜¯ç®€å•åœ°å¯¹æ ‡é¢˜çš„é‡å¤æè¿°ã€‚å³ä½¿ä½ éƒ¨ç½²ä¸€ä¸ªæ›´å¤§ç‰ˆæœ¬çš„æ¨¡å‹åˆ°äº‘ç«¯ï¼Œä¹Ÿå¥½ä¸åˆ°å“ªé‡Œå»ã€‚

è¿™ä¹Ÿæ˜¯ChatGPTè®©äººéœ‡æ’¼çš„åŸå› ï¼Œçš„ç¡®ç›®å‰å®ƒçš„æ•ˆæœè¿˜æ˜¯è¦è¿œè¿œè¶…å‡ºä»»ä½•ä¸€ä¸ªç«äº‰å¯¹æ‰‹å’Œå¼€æºé¡¹ç›®çš„ã€‚

## å°ç»“

å¥½äº†ï¼Œæœ€åæˆ‘ä»¬æ¥å›é¡¾ä¸€ä¸‹ã€‚è¿™ä¸€è®²é‡Œï¼Œæˆ‘ä»¬ä¸€èµ·å°è¯•ç”¨å¼€æºæ¨¡å‹æ¥ä»£æ›¿ChatGPTã€‚æˆ‘ä»¬é€šè¿‡sentence\_transfomersç±»å‹çš„æ¨¡å‹ï¼Œç”Ÿæˆäº†æ–‡æœ¬åˆ†ç‰‡çš„Embeddingï¼Œå¹¶ä¸”åŸºäºè¿™ä¸ªEmbeddingæ¥è¿›è¡Œè¯­ä¹‰æ£€ç´¢ã€‚æˆ‘ä»¬é€šè¿‡ ChatGLM è¿™ä¸ªå¼€æºæ¨¡å‹ï¼Œå®ç°äº†åŸºäºä¸Šä¸‹æ–‡æç¤ºè¯­çš„é—®ç­”ã€‚åœ¨ç®€å•çš„ç”µå•†QAè¿™æ ·çš„åœºæ™¯é‡Œï¼Œæ•ˆæœä¹Ÿè¿˜æ˜¯ä¸é”™çš„ã€‚å³ä½¿æˆ‘ä»¬ä½¿ç”¨çš„éƒ½æ˜¯å•æœºå°æ¨¡å‹ï¼Œå®ƒä¹Ÿèƒ½æ­£ç¡®å›ç­”å‡ºæ¥ã€‚è¿™äº›æ–¹æ³•ï¼Œä¹Ÿèƒ½èŠ‚çº¦æˆ‘ä»¬çš„æˆæœ¬ã€‚ä¸ç”¨æŠŠé’±éƒ½äº¤ç»™OpenAIï¼Œå¯ä»¥æ”’ç€ä¹°æ˜¾å¡æ¥è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚

ä½†æ˜¯ï¼Œå½“æˆ‘ä»¬éœ€è¦è§£å†³æ›´åŠ å¤æ‚çš„é—®é¢˜æ—¶ï¼Œæ¯”å¦‚éœ€è¦æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæˆ–è€…éœ€è¦æ¨¡å‹æœ¬èº«æ›´å¼ºçš„æ¨ç†èƒ½åŠ›çš„æ—¶å€™ï¼Œè¿™æ ·çš„å°æ¨¡å‹å°±è¿œè¿œä¸å¤Ÿç”¨äº†ã€‚æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ£€ç´¢ï¼Œæˆ‘ä»¬è¿˜èƒ½å¤Ÿé€šè¿‡åœ¨äº‘ç«¯éƒ¨ç½²æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œè§£å†³éƒ¨åˆ†é—®é¢˜ã€‚ä½†æ˜¯æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç›®å‰çš„ç¡®æ²¡æœ‰å¥½çš„è§£å†³æ–¹æ¡ˆã€‚

æ‰€ä»¥ä¸å¾—ä¸ä½©æœï¼ŒOpenAIçš„åœ¨AGIè¿™ä¸ªç›®æ ‡ä¸Šè€•è€˜å¤šå¹´åéœ‡æƒŠä¸–äººçš„æ•ˆæœã€‚

## æ€è€ƒé¢˜

æœ€åï¼Œç»™ä½ ç•™ä¸€ä¸ªæ€è€ƒé¢˜ã€‚ChatGLMå¹¶ä¸æ˜¯å”¯ä¸€çš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼Œå¼€æºç¤¾åŒºç›®å‰åœ¨å¿«é€Ÿæ¨è¿›ï¼Œå°è¯•ç”¨å„ç§æ–¹å¼æä¾›æ›´å¥½çš„å¼€æºå¤§æ¨¡å‹ã€‚æ¯”å¦‚åŸºäºæ–¯å¦ç¦çš„Alpacaæ•°æ®é›†è¿›è¡Œå¾®è°ƒçš„ [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)ï¼Œé“¾å®¶ç§‘æŠ€å¼€æºçš„ [BELLE](https://github.com/LianjiaTech/BELLE)ã€‚ä½ å¯ä»¥æŒ‘é€‰ä¸€ä¸ªæ¨¡å‹è¯•ä¸€è¯•ï¼Œçœ‹çœ‹å®ƒä»¬çš„æ•ˆæœå’ŒChatGLMæ¯”èµ·æ¥æ€ä¹ˆæ ·ã€‚æ¬¢è¿ä½ æŠŠä½ çš„è¯„æµ‹ç»“æœåˆ†äº«å‡ºæ¥ï¼Œä¹Ÿæ¬¢è¿ä½ æŠŠè¿™èŠ‚è¯¾åˆ†äº«ç»™éœ€è¦çš„æœ‹å‹ï¼Œå…±åŒå‚è°‹ï¼Œä¸€èµ·è¿›æ­¥ã€‚æˆ‘ä»¬ä¸‹èŠ‚è¯¾å†è§ã€‚

## æ¨èé˜…è¯»

åŸºäºå¼€æºæ¨¡å‹æ¥è§£å†³é—®é¢˜çš„æ€è·¯å¹¶éæˆ‘çš„åŸåˆ›ï¼Œç½‘ä¸Šä¹Ÿæœ‰ä¸å°‘å…¶ä»–æœ‹å‹ç”¨ç±»ä¼¼çš„æ–¹å¼è§£å†³äº†è‡ªå·±çš„é—®é¢˜ã€‚æ¯”å¦‚[ã€Šè®© LLM å›ç­”é—®é¢˜æ›´é è°±ã€‹è¿™ç¯‡æ–‡ç« ](https://mp.weixin.qq.com/s/iplUoK_JYeL_9EC7Ttt3tw)å°±ç»„åˆäº†ä¸‰ä¸ªæ¨¡å‹æ¥å®Œæˆäº†åŒ»å­¦é¢†åŸŸçš„è¯­ä¹‰æœç´¢ã€è¯­ä¹‰åŒ¹é…æ’åºï¼Œä»¥åŠæœ€ç»ˆçš„é—®ç­”è¯­å¥ç”Ÿæˆã€‚ä½ å¯ä»¥è¯»ä¸€ä¸‹ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>Toni</span> ğŸ‘ï¼ˆ6ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>åœ¨ç¬¬äºŒè®²ä¸­ï¼Œç”¨äºšé©¬é€Šæä¾›çš„ç”¨æˆ·å¯¹ä¸€äº›é£Ÿç‰©è¯„ä»·çš„çœŸå®æ•°æ®é›†è¿›è¡Œäº†æƒ…æ„Ÿåˆ†æã€‚å½“æ—¶ä¸ºäº†é¿å…é‡æ–°è°ƒç”¨ OpenAI çš„ API æµªè´¹é’±ï¼Œè€å¸ˆæ¨èä½¿ç”¨å·²ç»è®¡ç®—å¥½çš„å«æœ‰ Embedding çš„æ•°æ®ã€‚ç”¨openai.embeddings_utils ä¸­çš„ get_embedding (EMBEDDING_MODEL = &quot;text-embedding-ada-002&quot;)ä¸ä»…è´¹é’±è¿˜è€—æ—¶ã€‚æˆ‘è¯•ç€è·‘è¿‡100ä¸ªæ•°æ®ï¼Œå¥½åƒç”¨äº†20åˆ†é’Ÿï¼ŒèŠ±è´¹ä¹Ÿä¸å°‘ã€‚

æœ¬èŠ‚è¯¾ä¸­è€å¸ˆä»‹ç»äº†å…è´¹çš„ sentence_transformers æ­£å¥½å¯ä»¥æ‹¿æ¥ç”¨åœ¨æƒ…æ„Ÿæ•°æ®åˆ†æä¸Šï¼Œ
é€‰ç”¨ model = SentenceTransformer(&#39;paraphrase-multilingual-mpnet-base-v2&#39;)ã€‚åŒæ ·è®¡ç®—1000ä¸ªæ•°æ®çš„ embeddingï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä¸”æ— è´¹ç”¨ï¼Œé€‚åˆç»ƒæ‰‹ã€‚

æµ‹è¯•ç»“æœå¦‚ä¸‹:
                     precision    recall  f1-score   support
        negative      0.77      0.78      0.77       148
         positive      0.96      0.95      0.96       773
       accuracy                               0.93       921
    macro avg       0.86      0.87      0.86       921
weighted avg       0.93      0.93      0.93       921

ä¸ºäº†å¯¹æ¯”ï¼Œå°†ç¬¬äºŒè®²ä¸­è€å¸ˆç”¨ OpenAI çš„æ–¹æ³•å¾—åˆ°çš„ç»“æœæ”¾åœ¨äº†è¿™é‡Œ:

                    precision    recall  f1-score   support
        negative      0.98      0.73      0.84       136
         positive      0.96      1.00      0.98       789
       accuracy                               0.96       925
    macro avg       0.97      0.86      0.91       925
weighted avg       0.96      0.96      0.96       925

ä½¿ç”¨çš„æ–¹æ³•ä¸ç”¨ï¼Œç»“æœä¹Ÿä¸åŒã€‚OpenAI çš„å‡†ç¡®ç‡æ›´é«˜ã€‚</p>2023-04-11</li><br/><li><span>zhihai.tu</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œæƒ³è¯·æ•™ä¸€ä¸‹ï¼Œæˆ‘å‘ç°æˆ‘ç”¨colabè·‘äº†ç¨‹åºï¼Œå¦‚æœç¬¬äºŒå¤©å†æ‰“å¼€è¿™ä¸ªç¨‹åºï¼Œç›¸åº”çš„å·²ç»è·‘è¿‡å¾—è„šæœ¬ï¼Œæ•ˆæœéƒ½å¤±æ•ˆäº†ï¼Œæ¯”å¦‚pipå®‰è£…è¿‡çš„éƒ½è¿˜åŸäº†ï¼Œå¦å¤–ç£ç›˜ç›®å½•ä¸Šç”Ÿæˆçš„æ–‡ä»¶å¤¹å’Œæ–‡ä»¶ä¹Ÿæ²¡äº†ã€‚æƒ³è¯·é—®ä¸‹å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿä¸ç„¶ç›¸å½“çš„éº»çƒ¦ã€‚è°¢è°¢ã€‚</p>2023-04-26</li><br/><li><span>ä¸œæ–¹å¥‡éª¥</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>è€å¸ˆï¼Œæ˜¯ä¸æ˜¯è¦4080æ˜¾å¡æ‰è·‘å¾—åŠ¨ï¼Ÿå•æœºèƒ½è·‘å¾—åŠ¨å—</p>2023-04-07</li><br/><li><span>èƒ–å­</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ5ï¼‰<p>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-54-086bfff09511&gt; in &lt;cell line: 8&gt;()
      6 Q: ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿ
      7 &quot;&quot;&quot;
----&gt; 8 response, history = model.chat(tokenizer, question, history=[])
      9 print(response)

21 frames
&#47;usr&#47;local&#47;lib&#47;python3.9&#47;dist-packages&#47;google&#47;protobuf&#47;unknown_fields.py in &lt;module&gt;
     42 from google.protobuf.internal import api_implementation
     43 
---&gt; 44 if api_implementation._c_module is not None:  # pylint: disable=protected-access
     45   UnknownFieldSet = api_implementation._c_module.UnknownFieldSet  # pylint: disable=protected-access
     46 else:

AttributeError: module &#39;google.protobuf.internal.api_implementation&#39; has no attribute &#39;_c_module&#39;


æä¸å®š</p>2023-04-22</li><br/><li><span>å·æœˆ</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>ä½¿ç”¨è¿™ä¸ªå¼€æºæ¨¡å‹è·å–çš„indexæ€ä¹ˆä¿å­˜å•Šï¼Œä½¿ç”¨ä¹‹å‰çš„æ–¹æ³•å¹¶ä¸è¡Œï¼Œè¿˜æœ‰ç”Ÿæˆçš„indexå¯ä»¥è¿½åŠ å—ï¼Œä¸ç„¶æ¯æ¬¡æœ‰æ–°æ•°æ®çš„æ—¶å€™éƒ½è¦é‡æ–°è·‘ä¸€è¾¹ï¼Œæˆ–è€…å¯ä»¥ä½¿ç”¨æ•°æ®åº“å­˜å‚¨å—ï¼Œå¸Œæœ›è€å¸ˆè®²è§£ä¸€ä¸‹</p>2023-04-07</li><br/><li><span>à¼ºáƒ¦å¤©å£Â²ÂºÂ²Â²áƒ¦à¼»</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼ŒFAQæ•°æ®åœ¨å“ªé‡Œï¼Ÿ</p>2023-05-06</li><br/><li><span>å­Ÿå¥</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>Metaæœ€è¿‘ä¹Ÿå¼€æºäº†å¤§è¯­è¨€æ¨¡å‹ï¼Œå¥½åƒæ›´å¥½ä¸€äº›ï¼Ÿ</p>2023-04-06</li><br/><li><span>åœ°å¹³çº¿</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>ç”±äºllama-index å‡çº§ï¼Œæˆ‘ä½¿ç”¨çš„ç‰ˆæœ¬æ˜¯0.6.8ï¼Œä¿®æ”¹äº†llama-indexä»£ç ï¼Œç¨‹åºè¿è¡Œæ²¡æœ‰æŠ¥é”™ï¼Œä½†æ˜¯æ²¡æœ‰è¾“å‡ºå†…å®¹

from langchain.text_splitter import SpacyTextSplitter

llm_predictor = LLMPredictor(llm=CustomLLM())

text_splitter = CharacterTextSplitter(separator=&quot;\n\n&quot;, chunk_size=100, chunk_overlap=20)
parser = SimpleNodeParser(text_splitter=text_splitter)
documents = SimpleDirectoryReader(&#39;.&#47;drive&#47;MyDrive&#47;colab_data&#47;faq&#47;&#39;).load_data()
nodes = parser.get_nodes_from_documents(documents)

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(
    model_name=&quot;sentence-transformers&#47;paraphrase-multilingual-mpnet-base-v2&quot;
))
service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)

dimension = 768
faiss_index = faiss.IndexFlatIP(dimension)

# index = GPTListIndex(nodes=nodes, faiss_index=faiss_index, service_context=service_context)
index = GPTListIndex(nodes=nodes, service_context=service_context)

from llama_index import QuestionAnswerPrompt

QA_PROMPT_TMPL = (
    &quot;{context_str}&quot;
    &quot;\n\n&quot;
    &quot;æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œè¯·å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š\n&quot;
    &quot;Q: {query_str}\n&quot;
    )
QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)

query_engine = index.as_query_engine(
    retriever_mode=&quot;embedding&quot;, 
    verbose=True, 
    text_qa_template=QA_PROMPT,
)
response = query_engine.query(&quot;è¯·é—®ä½ ä»¬æµ·å—èƒ½å‘è´§å—ï¼Ÿ&quot;)
print(response)</p>2023-05-18</li><br/><li><span>èŒ¶æ¡</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œâ€œGPTFaissIndexâ€è¿™ä¸ªæ–¹æ³•ä¼¼ä¹æ²¡æœ‰äº†ï¼Œæ›¿æ¢çš„å…¶ä»–æ–¹æ³•ä¸çŸ¥é“æ˜¯ä»€ä¹ˆï¼Œæ€ä¹ˆç”¨ã€‚</p>2023-05-18</li><br/><li><span>horn</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ColabæŠ¥é”™æ€ä¹ˆå›äº‹å‘¢
ImportError: cannot import name &#39;GPTFaissIndex&#39; from &#39;llama_index&#39; (&#47;usr&#47;local&#47;lib&#47;python3.10&#47;dist-packages&#47;llama_index&#47;__init__.py)</p>2023-05-15</li><br/><li><span>haha</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>PaLM2å‘¢ï¼Œä»Šå¤©è¢«å…¬ä¼—å·åˆ·å±äº†</p>2023-05-11</li><br/><li><span>Geek_9znsx3</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>åœ¨colabè¿è¡Œ11_colab_chatglm_opensource.ipynbï¼Œå®‰è£…pythonåŒ…æ—¶æŠ¥é”™ï¼ŒprotobufåŒ…ç‰ˆæœ¬å†²çªï¼Œè¯·é—®è¿™ä¸ªæ€ä¹ˆè§£å†³ï¼Ÿä»æŠ¥é”™ä¿¡æ¯çœ‹æ˜¯ç”±äºtensorflow-2.12.0ä¾èµ–protobuf3.20.3ï¼Œä½†æ˜¯icetk-0.0.7ä¾èµ– protobuf-3.18.3
Installing collected packages: protobuf, icetk
  Attempting uninstall: protobuf
    Found existing installation: protobuf 3.20.3
    Uninstalling protobuf-3.20.3:
      Successfully uninstalled protobuf-3.20.3
ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
........
tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3, but you have protobuf 3.18.3 which is incompatible.
tensorflow-datasets 4.9.2 requires protobuf&gt;=3.20, but you have protobuf 3.18.3 which is incompatible.
tensorflow-hub 0.13.0 requires protobuf&gt;=3.19.6, but you have protobuf 3.18.3 which is incompatible.
tensorflow-metadata 1.13.1 requires protobuf&lt;5,&gt;=3.20.3, but you have protobuf 3.18.3 which is incompatible.
Successfully installed icetk-0.0.7 protobuf-3.18.3</p>2023-05-10</li><br/><li><span>ææ–‡é¾™</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>è¯·é—®ä½¿ç”¨ llama_index 0.6.1 ç‰ˆæœ¬ï¼Œä½¿ç”¨ Faissï¼Œä»£ç å¦‚ä¸‹ï¼Œæœ‰æŠ¥é”™ã€‚
import faiss
from llama_index.vector_stores import FaissVectorStore
from llama_index import GPTVectorStoreIndex, StorageContext

dimension = 768
faiss_index = faiss.IndexFlatIP(dimension)
storage_context = StorageContext.from_defaults(
    vector_store=FaissVectorStore(faiss_index)
)
print(len(nodes))
index = GPTVectorStoreIndex(nodes, storage_context=storage_context)

query_engine = index.as_query_engine(
    retriever_mode=&quot;embedding&quot;,
    verbose=True,
    service_context = service_context,
)

æŠ¥é”™ï¼š
&#47;usr&#47;local&#47;lib&#47;python3.10&#47;dist-packages&#47;faiss&#47;__init__.py in replacement_add(self, x)
    212 
    213         n, d = x.shape
--&gt; 214         assert d == self.d
    215         self.add_c(n, swig_ptr(x))
    216 </p>2023-05-05</li><br/><li><span>Santiago</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ4ï¼‰<p>è€å¸ˆæˆ‘æƒ³é—®ä¸€ä¸‹ æˆ‘ç”¨colabæŠ¥é”™ 
KeyError: &#39;-1&#39;
è¿™æ˜¯å’‹å›äº‹å‘€ï¼Œè¿˜æœ‰å°±æ˜¯æˆ‘ç”¨3060çš„æ˜¾å¡6Gï¼Œç›´æ¥çˆ†æ˜¾å­˜äº†</p>2023-04-20</li><br/><li><span>Santiago</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>RuntimeError: Failed to import transformers.models.t5.configuration_t5 because of the following error (look up to see its traceback):
Failed to import transformers.onnx.config because of the following error (look up to see its traceback):
DLL load failed while importing _imaging: æ‰¾ä¸åˆ°æŒ‡å®šçš„æ¨¡å—ã€‚</p>2023-04-19</li><br/>
</ul>