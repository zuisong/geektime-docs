ä½ å¥½ï¼Œæˆ‘æ˜¯å¾æ–‡æµ©ã€‚

OpenAIçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œåªæ˜¯æä¾›äº†ç®€ç®€å•å•çš„Completionå’ŒEmbeddingè¿™æ ·ä¸¤ä¸ªæ ¸å¿ƒæ¥å£ã€‚ä½†æ˜¯ä½ ä¹Ÿçœ‹åˆ°äº†ï¼Œåœ¨è¿‡å»çš„13è®²é‡Œï¼Œé€šè¿‡åˆç†ä½¿ç”¨è¿™ä¸¤ä¸ªæ¥å£ï¼Œæˆ‘ä»¬å®Œæˆäº†å„ç§å„æ ·å¤æ‚çš„ä»»åŠ¡ã€‚

- é€šè¿‡æç¤ºè¯­ï¼ˆPromptï¼‰é‡ŒåŒ…å«å†å²çš„èŠå¤©è®°å½•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè®©AIæ ¹æ®ä¸Šä¸‹æ–‡æ­£ç¡®åœ°å›ç­”é—®é¢˜ã€‚
- é€šè¿‡å°†Embeddingæå‰ç´¢å¼•å¥½å­˜èµ·æ¥ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè®©AIæ ¹æ®å¤–éƒ¨çŸ¥è¯†å›ç­”é—®é¢˜ã€‚
- è€Œé€šè¿‡å¤šè½®å¯¹è¯ï¼Œå°†AIè¿”å›çš„ç­”æ¡ˆæ”¾åœ¨æ–°çš„é—®é¢˜é‡Œï¼Œæˆ‘ä»¬èƒ½å¤Ÿè®©AIå¸®æˆ‘ä»¬ç»™è‡ªå·±çš„ä»£ç æ’°å†™å•å…ƒæµ‹è¯•ã€‚

è¿™äº›æ–¹æ³•ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå®ç”¨çš„è‡ªç„¶è¯­è¨€ç±»åº”ç”¨é‡Œå¸¸è§çš„æ¨¡å¼ã€‚æˆ‘ä¹‹å‰ä¹Ÿéƒ½é€šè¿‡ä»£ç ä¸ºä½ æ¼”ç¤ºè¿‡å…·ä½“çš„åšæ³•ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æ¯æ¬¡å†™åº”ç”¨çš„æ—¶å€™ï¼Œéƒ½éœ€è¦è‡ªå·±å†å»OpenAIæä¾›çš„åŸå§‹APIé‡Œåšä¸€éï¼Œé‚£å°±å¤ªéº»çƒ¦äº†ã€‚äºæ˜¯ï¼Œå¼€æºç¤¾åŒºå°±æœ‰äººå°†è¿™äº›å¸¸è§çš„éœ€æ±‚å’Œæ¨¡å¼æŠ½è±¡äº†å‡ºæ¥ï¼Œå¼€å‘äº†ä¸€ä¸ªå«åšLangchainçš„å¼€æºåº“ã€‚é‚£ä¹ˆæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨LangChainæ¥å¿«é€Ÿå®ç°ä¹‹å‰æˆ‘ä»¬åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°è¿‡çš„åŠŸèƒ½ã€‚ä»¥åŠæˆ‘ä»¬å¦‚ä½•è¿›ä¸€æ­¥åœ°ï¼Œå°†Langchainå’Œæˆ‘ä»¬çš„ä¸šåŠ¡ç³»ç»Ÿæ•´åˆï¼Œå®Œæˆæ›´å¤æ‚ã€æ›´æœ‰å®ç”¨ä»·å€¼çš„åŠŸèƒ½ã€‚

## ä½¿ç”¨Langchainçš„é“¾å¼è°ƒç”¨

å¦‚æœä½ è§‚å¯Ÿå¾—æ¯”è¾ƒä»”ç»†çš„è¯ï¼Œä½ ä¼šå‘ç°åœ¨[ç¬¬ 11 è®²](https://time.geekbang.org/column/article/646363)æˆ‘ä»¬ä½¿ç”¨llama-indexçš„æ—¶å€™ï¼Œå°±å·²ç»è£…å¥½LangChainäº†ã€‚llama-indexä¸“æ³¨äºä¸ºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨æ„å»ºç´¢å¼•ï¼Œè™½ç„¶Langchainä¹Ÿæœ‰ç±»ä¼¼çš„åŠŸèƒ½ï¼Œä½†è¿™ä¸€ç‚¹å¹¶ä¸æ˜¯Langchainçš„ä¸»è¦å–ç‚¹ã€‚Langchainçš„ç¬¬ä¸€ä¸ªå–ç‚¹å…¶å®å°±åœ¨å®ƒçš„åå­—é‡Œï¼Œä¹Ÿå°±æ˜¯**é“¾å¼è°ƒç”¨**ã€‚

æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸ªä½¿ç”¨ChatGPTçš„ä¾‹å­ï¼Œä½ å°±èƒ½ç†è§£ä¸ºä»€ä¹ˆä¼šæœ‰é“¾å¼è°ƒç”¨çš„éœ€æ±‚äº†ã€‚æˆ‘ä»¬çŸ¥é“ï¼ŒGPT-3çš„åŸºç¡€æ¨¡å‹é‡Œé¢ï¼Œä¸­æ–‡çš„è¯­æ–™å¾ˆå°‘ã€‚ç”¨ä¸­æ–‡é—®å®ƒé—®é¢˜ï¼Œå¾ˆå¤šæ—¶å€™å®ƒå›ç­”å¾—ä¸å¥½ã€‚æ‰€ä»¥æœ‰æ—¶å€™ï¼Œæˆ‘ä¼šè¿‚å›å¤„ç†ä¸€ä¸‹ï¼Œå…ˆæŠŠä¸­æ–‡é—®é¢˜ç»™AIï¼Œè¯·å®ƒç¿»è¯‘æˆè‹±æ–‡ï¼Œç„¶åå†æŠŠè‹±æ–‡é—®é¢˜è´´è¿›å»æé—®ï¼Œå¾—åˆ°ä¸€ä¸ªè‹±æ–‡ç­”æ¡ˆã€‚æœ€åï¼Œå†è¯·AIæŠŠè‹±æ–‡ç­”æ¡ˆç¿»è¯‘å›ä¸­æ–‡ã€‚å¾ˆå¤šæ—¶å€™ï¼Œé—®é¢˜çš„ç­”æ¡ˆä¼šæ›´å‡†ç¡®ä¸€ç‚¹ã€‚æ¯”å¦‚ï¼Œä¸‹é¢çš„æˆªå›¾é‡Œï¼Œæˆ‘å°±è¯·å®ƒç®€å•ä»‹ç»ä¸€ä¸‹Stable Diffusionçš„åŸç†æ˜¯ä»€ä¹ˆã€‚

æ³¨ï¼šStable Diffusionæ˜¯ä¸€ä¸ªçƒ­é—¨çš„å¼€æºAIç”»å›¾å·¥å…·ï¼Œåé¢æˆ‘ä»¬åœ¨ä»‹ç»ç”¨AIç”Ÿæˆå›¾ç‰‡çš„æ—¶å€™ä¼šç”¨åˆ°ã€‚

### äººå·¥é“¾å¼è°ƒç”¨

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/b9/ba/b904189cc5e23c5015aae7f6736f5dba.png?wh=697x796 "å…ˆè®©AIæŠŠä¸­æ–‡é—®é¢˜ç¿»è¯‘æˆè‹±æ–‡ï¼Œå†ç›´æ¥æŠŠè‹±æ–‡é—®é¢˜è´´è¿›å»å¾—åˆ°è‹±æ–‡ç­”æ¡ˆ")

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/27/44/272869ac3fb95c57674843264eb62c44.png?wh=714x770 "æˆ‘ä»¬å†è¯·å®ƒç¿»è¯‘ä¸€ä¸‹è‹±æ–‡ç­”æ¡ˆ")

å¦‚æœç”¨APIæ¥å®ç°è¿™ä¸ªè¿‡ç¨‹ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ªé“¾å¼è°ƒç”¨çš„è¿‡ç¨‹ã€‚

1. æˆ‘ä»¬å…ˆè°ƒç”¨OpenAIï¼ŒæŠŠç¿»è¯‘è¯·æ±‚å’ŒåŸå§‹é—®é¢˜ç»„åˆåœ¨ä¸€èµ·å‘é€ç»™AIï¼Œå®Œæˆé—®é¢˜çš„ä¸­è¯‘è‹±ã€‚
2. ç„¶åå†æŠŠæ‹¿åˆ°çš„ç¿»è¯‘å¥½çš„è‹±æ–‡é—®é¢˜å‘é€ç»™OpenAIï¼Œå¾—åˆ°è‹±æ–‡ç­”æ¡ˆã€‚
3. æœ€åå†æŠŠè‹±æ–‡ç­”æ¡ˆï¼Œå’Œå¯¹åº”è¦æ±‚AIç¿»è¯‘ç­”æ¡ˆçš„è¯·æ±‚ç»„åˆåœ¨ä¸€èµ·ï¼Œå®Œæˆç­”æ¡ˆçš„è‹±è¯‘ä¸­ã€‚

### ä½¿ç”¨LLMChainè¿›è¡Œé“¾å¼è°ƒç”¨

å¦‚æœæˆ‘ä»¬ç”¨ä»£ç ï¼Œå¯ä»¥åƒä¸‹é¢è¿™æ ·ï¼Œä¸€æ­¥æ­¥è¿›è¡Œã€‚

```python
import openai, os
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
from langchain.chains import LLMChain

openai.api_key = os.environ.get("OPENAI_API_KEY")

llm = OpenAI(model_name="text-davinci-003", max_tokens=2048, temperature=0.5)

en_to_zh_prompt = PromptTemplate(
    template="è¯·æŠŠä¸‹é¢è¿™å¥è¯ç¿»è¯‘æˆè‹±æ–‡ï¼š \n\n {question}?", input_variables=["question"]
)

question_prompt = PromptTemplate(
    template = "{english_question}", input_variables=["english_question"]
)

zh_to_cn_prompt = PromptTemplate(
    input_variables=["english_answer"],
    template="è¯·æŠŠä¸‹é¢è¿™ä¸€æ®µç¿»è¯‘æˆä¸­æ–‡ï¼š \n\n{english_answer}?",
)

question_translate_chain = LLMChain(llm=llm, prompt=en_to_zh_prompt, output_key="english_question")
english = question_translate_chain.run(question="è¯·ä½ ä½œä¸ºä¸€ä¸ªæœºå™¨å­¦ä¹ çš„ä¸“å®¶ï¼Œä»‹ç»ä¸€ä¸‹CNNçš„åŸç†ã€‚")
print(english)

qa_chain = LLMChain(llm=llm, prompt=question_prompt, output_key="english_answer")
english_answer = qa_chain.run(english_question=english)
print(english_answer)

answer_translate_chain = LLMChain(llm=llm, prompt=zh_to_cn_prompt)
answer = answer_translate_chain.run(english_answer=english_answer)
print(answer)
```

è¾“å‡ºç»“æœï¼š

```plain
Please explain the principles of CNN as an expert in Machine Learning.

A Convolutional Neural Network (CNN) is a type of deep learning algorithm that is used to analyze visual imagery. It is modeled after the structure of the human visual cortex and is composed of multiple layers of neurons that process and extract features from an image. The main principle behind a CNN is that it uses convolutional layers to detect patterns in an image. Each convolutional layer is comprised of a set of filters that detect specific features in an image. These filters are then used to extract features from the image and create a feature map. The feature map is then passed through a pooling layer which reduces the size of the feature map and helps to identify the most important features in the image. Finally, the feature map is passed through a fully-connected layer which classifies the image and outputs the result.

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œç”¨äºåˆ†æè§†è§‰å›¾åƒã€‚å®ƒæ¨¡ä»¿äººç±»è§†è§‰çš®å±‚çš„ç»“æ„ï¼Œç”±å¤šå±‚ç¥ç»å…ƒç»„æˆï¼Œå¯ä»¥å¤„ç†å’Œæå–å›¾åƒä¸­çš„ç‰¹å¾ã€‚CNNçš„ä¸»è¦åŸç†æ˜¯ä½¿ç”¨å·ç§¯å±‚æ¥æ£€æµ‹å›¾åƒä¸­çš„æ¨¡å¼ã€‚æ¯ä¸ªå·ç§¯å±‚ç”±ä¸€ç»„æ»¤æ³¢å™¨ç»„æˆï¼Œå¯ä»¥æ£€æµ‹å›¾åƒä¸­çš„ç‰¹å®šç‰¹å¾ã€‚ç„¶åä½¿ç”¨è¿™äº›æ»¤æ³¢å™¨ä»å›¾åƒä¸­æå–ç‰¹å¾ï¼Œå¹¶åˆ›å»ºç‰¹å¾å›¾ã€‚ç„¶åï¼Œå°†ç‰¹å¾å›¾é€šè¿‡æ± åŒ–å±‚ä¼ é€’ï¼Œè¯¥å±‚å¯ä»¥å‡å°ç‰¹å¾å›¾çš„å¤§å°ï¼Œå¹¶æœ‰åŠ©äºè¯†åˆ«å›¾åƒä¸­æœ€é‡è¦çš„ç‰¹å¾ã€‚æœ€åï¼Œå°†ç‰¹å¾å›¾ä¼ é€’ç»™å®Œå…¨è¿æ¥çš„å±‚ï¼Œè¯¥å±‚å°†å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¾“å‡ºç»“æœã€‚
```

è¿™é‡Œçš„ä»£ç ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Langchainè¿™ä¸ªåº“ï¼Œä¸è¿‡è¿˜æ²¡æœ‰åŠ¨ç”¨å®ƒçš„é“¾å¼è°ƒç”¨è¿‡ç¨‹ã€‚æˆ‘ä»¬ä¸»è¦ç”¨äº†Langchainçš„ä¸‰ä¸ªåŒ…ã€‚

1. LLMï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬ä½¿ç”¨å“ªä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼Œæ¥å›ç­”æˆ‘ä»¬æå‡ºçš„é—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜æ˜¯ä½¿ç”¨OpenAIChatï¼Œä¹Ÿå°±æ˜¯æœ€æ–°æ”¾å‡ºæ¥çš„ gpt-3.5-turbo æ¨¡å‹ã€‚
2. PromptTemplateï¼Œå’Œæˆ‘ä»¬åœ¨[ç¬¬ 11 è®²](https://time.geekbang.org/column/article/646363)é‡Œçœ‹åˆ°çš„llama-indexçš„PromptTemplateæ˜¯ä¸€ä¸ªä¸œè¥¿ã€‚å®ƒå¯ä»¥å®šä¹‰ä¸€ä¸ªæç¤ºè¯­æ¨¡ç‰ˆï¼Œé‡Œé¢èƒ½å¤Ÿå®šä¹‰ä¸€äº›å¯ä»¥åŠ¨æ€æ›¿æ¢çš„å˜é‡ã€‚æ¯”å¦‚ï¼Œä»£ç é‡Œçš„question\_promptè¿™ä¸ªæ¨¡ç‰ˆé‡Œï¼Œæˆ‘ä»¬å°±å®šä¹‰äº†ä¸€ä¸ªå«åšquestionçš„å˜é‡ï¼Œå› ä¸ºæˆ‘ä»¬æ¯æ¬¡é—®çš„é—®é¢˜éƒ½ä¼šä¸ä¸€æ ·ã€‚äº‹å®ä¸Šï¼Œllamd-indexé‡Œé¢çš„PromptTemplateå°±æ˜¯å¯¹Langchainçš„PromptTemplateåšäº†ä¸€å±‚ç®€å•çš„å°è£…ã€‚
3. ä¸»è§’ LLMChainï¼Œå®ƒçš„æ„é€ å‡½æ•°æ¥æ”¶ä¸€ä¸ªLLMå’Œä¸€ä¸ªPromptTemplateä½œä¸ºå‚æ•°ã€‚æ„é€ å®Œæˆä¹‹åï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨é‡Œé¢çš„runæ–¹æ³•ï¼Œå°†PromptTemplateéœ€è¦çš„å˜é‡ï¼Œç”¨K=&gt;Vå¯¹çš„å½¢å¼ä¼ å…¥è¿›å»ã€‚è¿”å›çš„ç»“æœï¼Œå°±æ˜¯LLMç»™æˆ‘ä»¬çš„ç­”æ¡ˆã€‚

ä¸è¿‡å¦‚æœçœ‹ä¸Šé¢è¿™æ®µä»£ç ï¼Œæˆ‘ä»¬ä¼¼ä¹åªæ˜¯å¯¹OpenAIçš„APIåšäº†ä¸€å±‚å°è£…è€Œå·²ã€‚æˆ‘ä»¬æ„å»ºäº†3ä¸ªLLMChainï¼Œç„¶åæŒ‰ç…§é¡ºåºè°ƒç”¨ï¼Œæ¯æ¬¡æ‹¿åˆ°ç­”æ¡ˆä¹‹åï¼Œå†ä½œä¸ºè¾“å…¥ï¼Œäº¤ç»™ä¸‹ä¸€ä¸ªLLMè°ƒç”¨ã€‚æ„Ÿè§‰å¥½åƒæ›´éº»çƒ¦äº†ï¼Œæ²¡æœ‰å‡å°‘ä»€ä¹ˆå·¥ä½œé‡å‘€ï¼Ÿ

åˆ«ç€æ€¥ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰çœŸæ­£ç”¨ä¸ŠLLMChainçš„â€œé“¾å¼è°ƒç”¨â€åŠŸèƒ½ï¼Œè€Œç”¨è¿™ä¸ªåŠŸèƒ½ï¼Œåªéœ€è¦åŠ ä¸Šä¸€è¡Œå°å°çš„ä»£ç ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªå«åšSimpleSequentialChainçš„LLMChainç±»ï¼ŒæŠŠæˆ‘ä»¬è¦æŒ‰ç…§é¡ºåºä¾æ¬¡è°ƒç”¨çš„ä¸‰ä¸ªLLMChainæ”¾åœ¨ä¸€ä¸ªæ•°ç»„é‡Œï¼Œä¼ ç»™è¿™ä¸ªç±»çš„æ„é€ å‡½æ•°ã€‚

ç„¶åå¯¹äºè¿™ä¸ªå¯¹è±¡ï¼Œæˆ‘ä»¬è°ƒç”¨runæ–¹æ³•ï¼ŒæŠŠæˆ‘ä»¬ç”¨ä¸­æ–‡é—®çš„é—®é¢˜äº¤ç»™å®ƒã€‚è¿™ä¸ªæ—¶å€™ï¼Œè¿™ä¸ªSimpleSequentialChainï¼Œå°±ä¼šæŒ‰ç…§é¡ºåºå¼€å§‹è°ƒç”¨chainsè¿™ä¸ªæ•°ç»„å‚æ•°é‡Œé¢åŒ…å«çš„å…¶ä»–LLMChainã€‚å¹¶ä¸”ï¼Œæ¯ä¸€æ¬¡è°ƒç”¨çš„ç»“æœï¼Œä¼šå­˜å‚¨åœ¨è¿™ä¸ªChainæ„é€ æ—¶å®šä¹‰çš„output\_keyå‚æ•°é‡Œã€‚è€Œä¸‹ä¸€ä¸ªè°ƒç”¨çš„LLMChainï¼Œé‡Œé¢æ¨¡ç‰ˆå†…çš„å˜é‡å¦‚æœæœ‰å’Œä¹‹å‰çš„output\_keyåå­—ç›¸åŒçš„ï¼Œå°±ä¼šç”¨output\_keyé‡Œå­˜å…¥çš„å†…å®¹æ›¿æ¢æ‰æ¨¡ç‰ˆå†…å˜é‡æ‰€åœ¨çš„å ä½ç¬¦ã€‚

è¿™æ¬¡ï¼Œæˆ‘ä»¬åªå‘è¿™ä¸ªSimpleSequentialChainè°ƒç”¨ä¸€æ¬¡runæ–¹æ³•ï¼ŒæŠŠä¸€å¼€å§‹çš„é—®é¢˜äº¤ç»™å®ƒå°±å¥½äº†ã€‚åé¢æ ¹æ®ç­”æ¡ˆå»é—®æ–°çš„é—®é¢˜ï¼Œè¿™ä¸ªLLMChainä¼šè‡ªåŠ¨åœ°é“¾å¼æå®šã€‚æˆ‘åœ¨è¿™é‡ŒæŠŠæ—¥å¿—çš„Verboseæ¨¡å¼æ‰“å¼€äº†ï¼Œä½ åœ¨è¾“å‡ºçš„è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥çœ‹åˆ°å…¶å®è¿™ä¸ªLLMChainæ˜¯è°ƒç”¨äº†ä¸‰æ¬¡ï¼Œå¹¶ä¸”ä¸­é—´ä¸¤æ¬¡çš„è¿”å›ç»“æœä½ ä¹Ÿå¯ä»¥ä¸€å¹¶çœ‹åˆ°ã€‚

```python
from langchain.chains import SimpleSequentialChain

chinese_qa_chain = SimpleSequentialChain(
    chains=[question_translate_chain, qa_chain, answer_translate_chain], input_key="question",
    verbose=True)
answer = chinese_qa_chain.run(question="è¯·ä½ ä½œä¸ºä¸€ä¸ªæœºå™¨å­¦ä¹ çš„ä¸“å®¶ï¼Œä»‹ç»ä¸€ä¸‹CNNçš„åŸç†ã€‚")
print(answer)
```

Verboseæ—¥å¿—ä¿¡æ¯ï¼š

```plain
> Entering new SimpleSequentialChain chain...

Please introduce the principle of CNN as a machine learning expert.

Convolutional Neural Networks (CNNs) are a type of artificial neural network that are commonly used in image recognition and classification tasks. They are inspired by the structure of the human brain and are composed of multiple layers of neurons connected in a specific pattern. The neurons in the first layer of a CNN are connected to the input image, and the neurons in the last layer are connected to the output. The neurons in between the input and output layers are called feature maps and are responsible for extracting features from the input image. CNNs use convolutional layers to detect patterns in the input image and pooling layers to reduce the size of the feature maps. This allows the CNN to learn the most important features in the image and use them to make predictions.

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç§å¸¸ç”¨äºå›¾åƒè¯†åˆ«å’Œåˆ†ç±»ä»»åŠ¡çš„äººå·¥ç¥ç»ç½‘ç»œã€‚å®ƒä»¬å—åˆ°äººè„‘ç»“æ„çš„å¯å‘ï¼Œç”±å¤šå±‚ç¥ç»å…ƒä»¥ç‰¹å®šæ¨¡å¼è¿æ¥è€Œæˆã€‚CNNçš„ç¬¬ä¸€å±‚ç¥ç»å…ƒä¸è¾“å…¥å›¾åƒè¿æ¥ï¼Œæœ€åä¸€å±‚ç¥ç»å…ƒä¸è¾“å‡ºè¿æ¥ã€‚è¾“å…¥å’Œè¾“å‡ºå±‚ä¹‹é—´çš„ç¥ç»å…ƒç§°ä¸ºç‰¹å¾æ˜ å°„ï¼Œè´Ÿè´£ä»è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾ã€‚CNNä½¿ç”¨å·ç§¯å±‚æ£€æµ‹è¾“å…¥å›¾åƒä¸­çš„æ¨¡å¼ï¼Œä½¿ç”¨æ± åŒ–å±‚å‡å°ç‰¹å¾æ˜ å°„çš„å¤§å°ã€‚è¿™ä½¿å¾—CNNèƒ½å¤Ÿå­¦ä¹ å›¾åƒä¸­æœ€é‡è¦çš„ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬è¿›è¡Œé¢„æµ‹ã€‚
> Finished chain.
```

è¾“å‡ºç»“æœï¼š

```plain
å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç§å¸¸ç”¨äºå›¾åƒè¯†åˆ«å’Œåˆ†ç±»ä»»åŠ¡çš„äººå·¥ç¥ç»ç½‘ç»œã€‚å®ƒä»¬å—åˆ°äººè„‘ç»“æ„çš„å¯å‘ï¼Œç”±å¤šå±‚ç¥ç»å…ƒä»¥ç‰¹å®šæ¨¡å¼è¿æ¥è€Œæˆã€‚CNNçš„ç¬¬ä¸€å±‚ç¥ç»å…ƒä¸è¾“å…¥å›¾åƒè¿æ¥ï¼Œæœ€åä¸€å±‚ç¥ç»å…ƒä¸è¾“å‡ºè¿æ¥ã€‚è¾“å…¥å’Œè¾“å‡ºå±‚ä¹‹é—´çš„ç¥ç»å…ƒç§°ä¸ºç‰¹å¾æ˜ å°„ï¼Œè´Ÿè´£ä»è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾ã€‚CNNä½¿ç”¨å·ç§¯å±‚æ£€æµ‹è¾“å…¥å›¾åƒä¸­çš„æ¨¡å¼ï¼Œä½¿ç”¨æ± åŒ–å±‚å‡å°ç‰¹å¾æ˜ å°„çš„å¤§å°ã€‚è¿™ä½¿å¾—CNNèƒ½å¤Ÿå­¦ä¹ å›¾åƒä¸­æœ€é‡è¦çš„ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬è¿›è¡Œé¢„æµ‹ã€‚
```

åœ¨ä½¿ç”¨è¿™æ ·çš„é“¾å¼è°ƒç”¨çš„æ—¶å€™ï¼Œæœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œå°±æ˜¯ä¸€ä¸ªLLMChainé‡Œï¼Œæ‰€ä½¿ç”¨çš„PromptTemplateé‡Œçš„è¾“å…¥å‚æ•°ï¼Œ**ä¹‹å‰å¿…é¡»åœ¨LLMChainé‡Œï¼Œé€šè¿‡ output\_key å®šä¹‰è¿‡ã€‚**ä¸ç„¶ï¼Œè¿™ä¸ªå˜é‡æ²¡æœ‰å€¼ï¼Œç¨‹åºå°±ä¼šæŠ¥é”™ã€‚

### æ”¯æŒå¤šä¸ªå˜é‡è¾“å…¥çš„é“¾å¼è°ƒç”¨

äº‹å®ä¸Šï¼Œå› ä¸ºä½¿ç”¨å˜é‡çš„è¾“å…¥è¾“å‡ºï¼Œæ˜¯ç”¨è¿™äº›å‚æ•°å®šä¹‰çš„ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸æ˜¯åªèƒ½ç”¨å‰ä¸€ä¸ªLLMChainçš„è¾“å‡ºä½œä¸ºåä¸€ä¸ªLLMChainçš„è¾“å…¥ã€‚æˆ‘ä»¬å®Œå…¨å¯ä»¥è¿ç»­é—®å¤šä¸ªé—®é¢˜ï¼Œç„¶åæŠŠè¿™äº›é—®é¢˜çš„ç­”æ¡ˆï¼Œä½œä¸ºåç»­é—®é¢˜çš„è¾“å…¥æ¥ç»§ç»­å¤„ç†ã€‚ä¸‹é¢æˆ‘å°±ç»™ä½ çœ‹ä¸€ä¸ªä¾‹å­ã€‚

```python
from langchain.chains import SequentialChain

q1_prompt = PromptTemplate(
    input_variables=["year1"],
    template="{year1}å¹´çš„æ¬§å† è”èµ›çš„å† å†›æ˜¯å“ªæ”¯çƒé˜Ÿï¼Œåªè¯´çƒé˜Ÿåç§°ã€‚"
)
q2_prompt = PromptTemplate(
    input_variables=["year2"],
    template="{year2}å¹´çš„æ¬§å† è”èµ›çš„å† å†›æ˜¯å“ªæ”¯çƒé˜Ÿï¼Œåªè¯´çƒé˜Ÿåç§°ã€‚"
)
q3_prompt = PromptTemplate(
    input_variables=["team1", "team2"],
    template="{team1}å’Œ{team2}å“ªåªçƒé˜Ÿè·å¾—æ¬§å† çš„æ¬¡æ•°å¤šä¸€äº›ï¼Ÿ"
)
chain1 = LLMChain(llm=llm, prompt=q1_prompt, output_key="team1")
chain2 = LLMChain(llm=llm, prompt=q2_prompt, output_key="team2")
chain3 = LLMChain(llm=llm, prompt=q3_prompt)

sequential_chain = SequentialChain(chains=[chain1, chain2, chain3], input_variables=["year1", "year2"], verbose=True)
answer = sequential_chain.run(year1=2000, year2=2010)
print(answer)
```

è¾“å‡ºç»“æœï¼š

```plain
> Entering new SequentialChain chain...
> Finished chain.

è¥¿ç­ç‰™çš‡å®¶é©¬å¾·é‡Œé˜Ÿè·å¾—æ¬§å† çš„æ¬¡æ•°æ›´å¤šï¼Œå…±13æ¬¡ï¼Œè€Œæ‹œä»æ…•å°¼é»‘åªæœ‰5æ¬¡ã€‚
```

åœ¨è¿™ä¸ªä¾‹å­é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ªPromptTemplateå’Œå¯¹åº”çš„LLMChainï¼Œå„è‡ªæ¥æ”¶ä¸€ä¸ªå¹´ä»½ä½œä¸ºè¾“å…¥ï¼Œå›ç­”è¿™ä¸¤ä¸ªå¹´ä»½çš„æ¬§å† å† å†›ã€‚ç„¶åå°†ä¸¤ä¸ªé˜Ÿåä½œä¸ºè¾“å…¥ï¼Œæ”¾åˆ°ç¬¬ä¸‰ä¸ªé—®é¢˜é‡Œï¼Œè®©AIå‘Šè¯‰æˆ‘ä»¬è¿™ä¸¤æ”¯çƒé˜Ÿå“ªä¸€æ”¯è·å¾—æ¬§å† çš„æ¬¡æ•°å¤šä¸€äº›ã€‚åªéœ€è¦åœ¨æˆ‘ä»¬çš„SequentialChainé‡Œè¾“å…¥ä¸¤ä¸ªå¹´ä»½ï¼Œå°±èƒ½é€šè¿‡ä¸‰æ¬¡å›ç­”å¾—åˆ°ç­”æ¡ˆã€‚

## é€šè¿‡Langchainå®ç°è‡ªåŠ¨åŒ–æ’°å†™å•å…ƒæµ‹è¯•

çœ‹åˆ°è¿™é‡Œï¼Œä¸çŸ¥é“ä½ æœ‰æ²¡æœ‰æƒ³èµ·æˆ‘ä»¬ä¸Šä¸€è®²åˆšåˆšè®²è¿‡çš„é€šè¿‡å¤šæ­¥æç¤ºè¯­è‡ªåŠ¨ç»™ä»£ç å†™å•å…ƒæµ‹è¯•ã€‚æ²¡é”™ï¼ŒLangchainå¯ä»¥é¡ºåºåœ°é€šè¿‡å¤šä¸ªPromptè°ƒç”¨OpenAIçš„GPTæ¨¡å‹ã€‚è¿™ä¸ªèƒ½åŠ›æ‹¿æ¥å®ç°ä¸Šä¸€è®²çš„è‡ªåŠ¨åŒ–æµ‹è¯•çš„åŠŸèƒ½æ˜¯å†åˆé€‚ä¸è¿‡çš„äº†ã€‚ä¸‹é¢ï¼Œæˆ‘å°±æ‹¿Langchainé‡æ–°å®ç°äº†ä¸€éä¸Šä¸€è®²çš„è¿™ä¸ªåŠŸèƒ½ï¼Œå¹¶ä¸”ç»™å®ƒè¡¥ä¸Šäº†ASTè¯­æ³•è§£æå¤±è´¥ä¹‹åè‡ªåŠ¨é‡è¯•çš„èƒ½åŠ›ã€‚

````python
from langchain.chains import SequentialChain

def write_unit_test(function_to_test, unit_test_package = "pytest"):
    # è§£é‡Šæºä»£ç çš„æ­¥éª¤
    explain_code = """"# How to write great unit tests with {unit_test_package}

    In this advanced tutorial for experts, we'll use Python 3.10 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.
    ```python
    {function_to_test}
    ```

    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.
    - First,"""

    explain_code_template = PromptTemplate(
        input_variables=["unit_test_package", "function_to_test"],
        template=explain_code
    )
    explain_code_llm = OpenAI(model_name="text-davinci-002", temperature=0.4, max_tokens=1000, 
            top_p=1, stop=["\n\n", "\n\t\n", "\n    \n"])
    explain_code_step = LLMChain(llm=explain_code_llm, prompt=explain_code_template, output_key="code_explaination")

    # åˆ›å»ºæµ‹è¯•è®¡åˆ’ç¤ºä¾‹çš„æ­¥éª¤
    test_plan = """
        
    A good unit test suite should aim to:
    - Test the function's behavior for a wide range of possible inputs
    - Test edge cases that the author may not have foreseen
    - Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain
    - Be easy to read and understand, with clean code and descriptive names
    - Be deterministic, so that the tests always pass or fail in the same way

    `{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.

    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):
    -"""
    test_plan_template = PromptTemplate(
        input_variables=["unit_test_package", "function_to_test", "code_explaination"],
        template= explain_code + "{code_explaination}" + test_plan
    )
    test_plan_llm = OpenAI(model_name="text-davinci-002", temperature=0.4, max_tokens=1000, 
            top_p=1, stop=["\n\n", "\n\t\n", "\n    \n"])
    test_plan_step = LLMChain(llm=test_plan_llm, prompt=test_plan_template, output_key="test_plan")

    # æ’°å†™æµ‹è¯•ä»£ç çš„æ­¥éª¤
    starter_comment = "Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator"
    prompt_to_generate_the_unit_test = """

Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.
```python
import {unit_test_package}  # used for our unit tests

{function_to_test}

#{starter_comment}"""

    unit_test_template = PromptTemplate(
        input_variables=["unit_test_package", "function_to_test", "code_explaination", "test_plan", "starter_comment"],
        template= explain_code + "{code_explaination}" + test_plan + "{test_plan}" + prompt_to_generate_the_unit_test
    )
    unit_test_llm = OpenAI(model_name="text-davinci-002", temperature=0.4, max_tokens=1000, stop="```")
    unit_test_step = LLMChain(llm=unit_test_llm, prompt=unit_test_template, output_key="unit_test")

    sequential_chain = SequentialChain(chains=[explain_code_step, test_plan_step, unit_test_step], 
                                    input_variables=["unit_test_package", "function_to_test", "starter_comment"], verbose=True)
    answer = sequential_chain.run(unit_test_package=unit_test_package, function_to_test=function_to_test, starter_comment=starter_comment)
    return f"""#{starter_comment}""" + answer

code = """
def format_time(seconds):
    minutes, seconds = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    if hours > 0:
        return f"{hours}h{minutes}min{seconds}s"
    elif minutes > 0:
        return f"{minutes}min{seconds}s"
    else:
        return f"{seconds}s"
"""

import ast

def write_unit_test_automatically(code, retry=3):
    unit_test_code = write_unit_test(code)
    all_code = code + unit_test_code
    tried = 0
    while tried < retry:
        try:
            ast.parse(all_code)
            return all_code
        except SyntaxError as e:
            print(f"Syntax error in generated code: {e}")
            all_code = code + write_unit_test(code)
            tried += 1
            
print(write_unit_test_automatically(code))
````

è¾“å‡ºç»“æœï¼š

```python

def format_time(seconds):
Â  Â  minutes, seconds = divmod(seconds, 60)
Â  Â  hours, minutes = divmod(minutes, 60)
Â  Â  if hours > 0:
Â  Â  Â  Â  return f"{hours}h{minutes}min{seconds}s"
Â  Â  elif minutes > 0:
Â  Â  Â  Â  return f"{minutes}min{seconds}s"
Â  Â  else:
Â  Â  Â  Â  return f"{seconds}s"
#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.
#The first element of the tuple is the name of the test case, and the second element is a list of tuples,
#where each tuple contains the input values for the format_time() function and the expected output.
@pytest.mark.parametrize("test_case, input_values, expected_output", [
Â  Â  # Test cases for when the seconds parameter is an integer
Â  Â  ("seconds is positive", (42,), "42s"),
Â  Â  ("seconds is negative", (-42,), "-42s"),
Â  Â  ("seconds is 0", (0,), "0s"),
Â  Â  # Test cases for when the seconds parameter is not an integer
Â  Â  ("seconds is a float", (42.0,), "42.0s"),
Â  Â  ("seconds is a string", ("42",), "42s"),
Â  Â  ("seconds is None", (None,), "None"),
Â  Â  # Test cases for when the seconds parameter is an integer, but it is not in the range 0-3600
Â  Â  ("seconds is too small", (-1,), "-1s"),
Â  Â  ("seconds is too large", (3601,), "1h0min1s"),
])
def test_format_time(test_case, input_values, expected_output):
Â  Â  # We use the pytest.raises context manager to assert that the function raises a TypeError
Â  Â  # if the input is not an integer.
Â  Â  with pytest.raises(TypeError):
Â  Â  Â  Â  format_time(input_values)
Â  Â  # We use the pytest.approx context manager to assert that the output is approximately equal
Â  Â  # to the expected output, within a certain tolerance.
Â  Â  assert format_time(input_values) == pytest.approx(expected_output)

```

è¿™ä¸ªä»£ç çš„å…·ä½“åŠŸèƒ½ï¼Œå…¶å®å’Œä¸Šä¸€è®²æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ï¼Œåªæ˜¯é€šè¿‡Langchainåšäº†å°è£…ï¼Œä½¿å®ƒæ›´åŠ å®¹æ˜“ç»´æŠ¤äº†ã€‚æˆ‘ä»¬æŠŠè§£é‡Šä»£ç ã€ç”Ÿæˆæµ‹è¯•è®¡åˆ’ï¼Œä»¥åŠæœ€ç»ˆç”Ÿæˆæµ‹è¯•ä»£ç ï¼Œå˜æˆäº†ä¸‰ä¸ªLLMChainã€‚æ¯ä¸€æ­¥çš„è¾“å…¥ï¼Œéƒ½æ¥è‡ªä¸Šä¸€æ­¥çš„è¾“å‡ºã€‚è¿™ä¸ªè¾“å…¥æ—¢åŒ…æ‹¬ä¸Šä¸€æ­¥çš„Prompt Templateå’Œè¿™ä¸€æ­¥çš„Prompt Templateçš„ç»„åˆï¼Œä¹ŸåŒ…æ‹¬è¿‡ç¨‹ä¸­çš„ä¸€äº›å˜é‡ï¼Œè¿™äº›å˜é‡æ˜¯ä¸Šä¸€æ­¥æ‰§è¡Œçš„ç»“æœä½œä¸ºè¾“å…¥å˜é‡ä¼ é€’è¿›æ¥çš„ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨SequentialChainæ¥è‡ªåŠ¨åœ°æŒ‰ç…§è¿™ä¸‰ä¸ªæ­¥éª¤ï¼Œæ‰§è¡ŒOpenAIçš„APIè°ƒç”¨ã€‚

è¿™æ•´ä¸ªè¿‡ç¨‹é€šè¿‡write\_unit\_testè¿™ä¸ªå‡½æ•°ç»™å°è£…èµ·æ¥äº†ã€‚å¯¹äºé‡è¯•ï¼Œæˆ‘ä»¬åˆ™æ˜¯é€šè¿‡ä¸€ä¸ªwhileå¾ªç¯æ¥è°ƒç”¨ write\_unit\_testã€‚æ‹¿åˆ°çš„ç»“æœå’Œè¾“å…¥çš„ä»£ç æ‹¼è£…åœ¨ä¸€èµ·ï¼Œäº¤ç»™ASTåº“åšè§£æã€‚å¦‚æœè§£æé€šä¸è¿‡ï¼Œåˆ™é‡è¯•æ•´ä¸ªå•å…ƒæµ‹è¯•ç”Ÿæˆçš„è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æœ€å¤§çš„é‡è¯•æ¬¡æ•°ä¸ºæ­¢ã€‚

LangChainçš„è¿™ä¸ªåˆ†å¤šä¸ªæ­¥éª¤è°ƒç”¨OpenAIæ¨¡å‹çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬é€šè¿‡AIå®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶ä¸”å°†æ•´ä¸ªä»»åŠ¡çš„å®Œæˆè¿‡ç¨‹å®šä¹‰æˆäº†ä¸€ä¸ªå›ºå®šçš„æµç¨‹æ¨¡ç‰ˆã€‚åœ¨ä¸‹ä¸€è®²é‡Œï¼Œæˆ‘ä»¬è¿˜ä¼šè¿›ä¸€æ­¥çœ‹åˆ°ï¼Œé€šè¿‡è¿™æ ·ä¸€ä¸ªé“¾å¼ç»„åˆå¤šä¸ªLLMChainçš„æ–¹æ³•ï¼Œå¦‚ä½•å®Œæˆæ›´å¤æ‚å¹¶ä¸”æ›´å…·æœ‰ç°å®æ„ä¹‰çš„å·¥ä½œã€‚

## å°ç»“

å¥½äº†ï¼Œç›¸ä¿¡åˆ°è¿™é‡Œï¼Œä½ è„‘å­é‡Œåº”è¯¥æœ‰äº†æ›´å¤šå¯ä»¥åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„å¥½ç‚¹å­ã€‚è¿™ä¸€è®²ï¼Œæˆ‘å¸¦ä½ å­¦ä¼šäº†å¦‚ä½•é€šè¿‡Langchainè¿™ä¸ªå¼€æºåº“ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé“¾å¼è°ƒç”¨ã€‚æƒ³è¦é€šè¿‡å¤§è¯­è¨€æ¨¡å‹ï¼Œå®Œæˆä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ï¼Œå¾€å¾€éœ€è¦æˆ‘ä»¬å¤šæ¬¡å‘AIæé—®ï¼Œå¹¶ä¸”å‰é¢æé—®çš„ç­”æ¡ˆï¼Œå¯èƒ½æ˜¯åé¢é—®é¢˜è¾“å…¥çš„ä¸€éƒ¨åˆ†ã€‚LangChainé€šè¿‡å°†å¤šä¸ªLLMChainç»„åˆæˆä¸€ä¸ªSequantialChainå¹¶é¡ºåºæ‰§è¡Œï¼Œå¤§å¤§ç®€åŒ–äº†è¿™ç±»ä»»åŠ¡çš„å¼€å‘å·¥ä½œã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/36/da/36916e57fc0618e5cb8ce49991e423da.png?wh=1920x1962 "LLMChainå°±æ˜¯ä¸€ä¸ªå¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé“¾å¼è°ƒç”¨çš„æ¨¡å¼ï¼Œå‰é¢çš„å˜é‡å’Œè¾“å‡ºéƒ½å¯ä»¥ä½œä¸ºä¸‹ä¸€è½®è°ƒç”¨çš„å˜é‡è¾“å…¥")

Langchainè¿˜æœ‰å¾ˆå¤šæ›´å¼ºå¤§çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬ä¸ä»…èƒ½è°ƒç”¨è¯­è¨€æ¨¡å‹ï¼Œè¿˜èƒ½è°ƒç”¨å¤–éƒ¨ç³»ç»Ÿï¼Œç”šè‡³æˆ‘ä»¬è¿˜èƒ½ç›´æ¥è®©AIåšå†³ç­–ï¼Œå†³å®šè¯¥è®©æˆ‘ä»¬çš„ç³»ç»Ÿåšä»€ä¹ˆã€‚åœ¨åé¢çš„å‡ è®²é‡Œï¼Œæˆ‘ä»¬ä¼šè¦†ç›–è¿™äº›å†…å®¹ï¼Œå¹¶æœ€ç»ˆç»™ä½ ä¸€ä¸ªå®Œæ•´çš„ç”µå•†èŠå¤©æœºå™¨äººã€‚

## æ€è€ƒé¢˜

æœ€åï¼Œç»™ä½ ç•™ä¸€é“æ€è€ƒé¢˜ã€‚ä½ èƒ½è¯•ç€é€šè¿‡Langchainç»„åˆå¤šä¸ªé—®é¢˜ï¼Œå¹¶ä¸”åˆ©ç”¨å‰é¢é—®é¢˜çš„å›ç­”ç»“æœï¼Œè§¦å‘æ–°çš„é—®é¢˜æ‰¾åˆ°ä½ æƒ³è¦çš„ç­”æ¡ˆå—ï¼Ÿæ¬¢è¿ä½ æŠŠä½ çš„ä¾‹å­æ‹¿å‡ºæ¥åˆ†äº«åœ¨è¯„è®ºåŒºï¼Œä¹Ÿæ¬¢è¿ä½ æŠŠè¿™ä¸€è®²åˆ†äº«ç»™éœ€è¦çš„æœ‹å‹ï¼Œæˆ‘ä»¬ä¸‹ä¸€è®²å†è§ã€‚

## æ¨èé˜…è¯»

å’Œä¹‹å‰ä»‹ç»è¿‡çš„llama-indexè¿™ä¸ªé¡¹ç›®ä¸€æ ·ï¼ŒLangchainè¿™ä¸ªé¡¹ç›®ä¹Ÿåœ¨å¿«é€Ÿåœ°å‘å±•å’Œè¿­ä»£è¿‡ç¨‹ä¸­ã€‚æˆ‘æ¨èä½ å»çœ‹ä¸€çœ‹ä»–ä»¬çš„[å®˜æ–¹æ–‡æ¡£](https://langchain.readthedocs.io/en/latest/)ï¼Œå¥½çŸ¥é“ä»–ä»¬æä¾›çš„æœ€æ–°åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªæˆ‘ä»¬ä¹‹å‰æåˆ°è¿‡çš„å‘é‡æ•°æ®åº“å…¬å¸Pineconeï¼Œä¹Ÿåˆ¶ä½œäº†ä¸€ä»½ [Langchain AI Handbook](https://www.pinecone.io/learn/langchain/)ï¼Œä½ ä¹Ÿå¯ä»¥å»çœ‹ä¸€çœ‹ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>æ™ºèƒ½</span> ğŸ‘ï¼ˆ7ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>è¿™ç§é“¾å¼è°ƒç”¨æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“è®©é—®é¢˜è¶…è¿‡tokené™åˆ¶ï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆåŠæ³•æ¥è‡ªåŠ¨è§£å†³è¿™ä¸ªé—®é¢˜</p>2023-04-11</li><br/><li><span>æ„</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆå¥½ï¼Œçœ‹åˆ°LangChainè·å¾—1000ä¸‡ç¾å…ƒç§å­è½®èèµ„çš„æ–°é—»ã€‚
æƒ³é—®ä¸‹ï¼šåƒLangChainè¿™ç§å¼€æºçš„äº§å“ï¼Œå•†ä¸šæ¨¡å¼æ˜¯æ€ä¹ˆæ ·çš„ï¼ŒæŠ•èµ„æœºæ„æ˜¯çœ‹ä¸­äº†å“ªç‚¹è¿›è¡ŒæŠ•èµ„çš„ã€‚</p>2023-04-11</li><br/><li><span>ä¸€å¶</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆæˆ‘çš„æƒ³é—®ä¸‹,å›½å†…ä½¿ç”¨Pineconeçš„æ•ˆç‡å¦‚ä½•? ä¼šä¸ä¼šå—åˆ°ç½‘ç»œçš„å½±å“?</p>2023-04-11</li><br/><li><span>åšç§¯è–„å‘</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>æ”¯æŒå¤šä¸ªå˜é‡è¾“å…¥çš„é“¾å¼è°ƒç”¨  è¿™ä¸ªæ¡ˆä¾‹ å¤šè¿è¡Œå‡ æ¬¡ï¼Œæœ€åçš„æ•°æ®ç»“æœï¼Œæ¯æ¬¡éƒ½ä¸ä¸€æ ·
å…¶ä¸­ä¸€æ¬¡æ˜¯è¿™ä¸ªâ€˜æ³¢å°”å›¾æ›´å¤šï¼Œä»–ä»¬è·å¾—è¿‡4æ¬¡æ¬§å† å† å†›ï¼Œè€Œè¥¿ç­ç‰™çš‡å®¶é©¬å¾·é‡Œåªè·å¾—è¿‡3æ¬¡æ¬§å† å† å†›ã€‚  â€™  è€å¸ˆï¼ŒçŸ¥é“è¿™ä¸ªæ˜¯ä»€ä¹ˆåŸå› å—ï¼Ÿ</p>2023-05-11</li><br/><li><span>æå®¢é›·</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>autowgptï¼Ÿ</p>2023-04-29</li><br/><li><span>è¶…è¶…è¶…è¶…äºº</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è€å¸ˆä½ å¥½ï¼ŒAutoGPT æœ¬è´¨ä¸Šæ˜¯ä¸æ˜¯ä¹Ÿä½¿ç”¨äº†é“¾å¼è°ƒç”¨å‘¢ï¼Ÿ</p>2023-04-19</li><br/><li><span>å¼ å¼›</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç”¨ChatGPTå®æµ‹æœ¬è®²ä¸­æåˆ°çš„é—®é¢˜ï¼Œå…ˆç¿»è¯‘è‹±æ–‡æé—®ï¼Œå†ç¿»è¯‘å›æ¥ï¼Œå¥½åƒå¹¶æœªäº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œè·Ÿç›´æ¥ä¸­æ–‡æé—®çš„ç»“æœå·®ä¸å¤šã€‚æˆ‘è¿˜ä¸“é—¨å¼€äº†æ–°çš„chatçª—å£æ¥é¿å…ä¸Šä¸‹æ–‡å½±å“ã€‚è€å¸ˆèƒ½å¦ä¸¾ä¸ªå…·ä½“çš„é€šè¿‡è¿™ç§æ–¹å¼å¾—åˆ°æ›´å¥½ç»“æœçš„æ¡ˆä¾‹å‘¢ï¼Ÿ</p>2023-04-14</li><br/><li><span>Evan</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>input_variables=[&quot;team1&quot;, &quot;team2&quot;],  æ˜¯æ€ä¹ˆä¼ å…¥å‚æ•°çš„ï¼Ÿ</p>2023-04-11</li><br/><li><span>Toni</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>part 1 (å—é™äº2000å­—ç¬¦ï¼Œå°†ç›¸åº”çš„è¿è¡Œä»£ç æ”¾åœ¨äº†part 2)

é¢˜ç›®:
é€šè¿‡ Langchain å®ç°è‡ªåŠ¨åŒ–æ’°å†™ Python çš„ä¸€ä¸ªå‡½æ•°(è¿›è¡Œæ—¶é—´æ ¼å¼åŒ–è¾“å‡º)ï¼Œå¹¶ç»™å‡ºå¯¹è¯¥å‡½æ•°çš„å•å…ƒæµ‹è¯•ï¼ŒåŒ…å«å¯¹å¼‚å¸¸è¾“å…¥çš„æµ‹è¯•ã€‚

ç›®çš„: 
1. é€šè¿‡è°ƒç”¨ SequentialChain ä½¿ä¸ ChatOpenAI çš„ç¬¬ä¸€æ¬¡å¯¹è¯çš„ç»“æœæˆä¸ºç¬¬äºŒæ¬¡å¯¹è¯çš„è¾“å…¥ï¼Œå¹¶å°†ä¸€ï¼ŒäºŒæ¬¡å¯¹è¯çš„ç»“æœæ˜¾ç¤ºå‡ºæ¥ï¼Œä»¥å¤‡åç»­è°ƒæ•´æ”¹è¿›ã€‚(æ³¨: åœ¨ ChatOpenAI çš„å¯¹è¯çª—ä¸‹ï¼ŒChatGPT çŸ¥é“ä¸Šä¸€æ¬¡å¯¹è¯çš„å†…å®¹ï¼Œæ— éœ€é‡å¤)ã€‚
2. ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¼–ç¨‹è¦æ±‚ã€‚
3. é€šè¿‡è°ƒåˆ¶ PromptTemplate ä¸­çš„å‚æ•° template æ¥å®ç°è¾“å‡ºç»“æœçš„æœ€ä¼˜åŒ–ã€‚è¿™å…¶å®å°±æ˜¯è®¾ç½®åˆé€‚çš„ Promptï¼Œä»¥æœŸæœ€æœ‰æ•ˆåœ°ä½¿ç”¨ ChatGPTã€‚

æ–¹æ³•:
å°è¯•ç€è°ƒç”¨äº† SequentialChainï¼Œä½¿ç”¨ ChatOpenAI çš„ &quot;gpt-3.5-turbo&quot;ï¼Œå‚æ•°è®¾ç½® temperature=1

ç»“æœ:

&gt; Entering new SequentialChain chain...

&gt; Finished chain.
def time_format(seconds):
    if seconds &lt; 60:
        return f&quot;{seconds}s&quot;
    elif seconds &lt; 3600:
        minutes = seconds &#47;&#47; 60
        seconds %= 60
        return f&quot;{minutes}min{seconds}s&quot;
    else:
        hours = seconds &#47;&#47; 3600
        seconds %= 3600
        minutes = seconds &#47;&#47; 60
        seconds %= 60
        return f&quot;{hours}h{minutes}min{seconds}s&quot;


&gt; Entering new SequentialChain chain...

&gt; Finished chain.
import pytest

def test_time_format():
    assert time_format(1) == &#39;1s&#39;
    assert time_format(61) == &#39;1min1s&#39;
    assert time_format(3678) == &#39;1h1min18s&#39;
    assert time_format(-1) == &#39;Invalid input&#39;
    assert time_format(&#39;abc&#39;) == &#39;Invalid input&#39;
    assert time_format(None) == &#39;Invalid input&#39;
    assert time_format(999999) == &#39;277h46min39s&#39; # Add a test case for a large input

As an AI language model, I cannot run this code, but I can assure you that the above code functions when used in a Python environment with the necessary dependencies and libraries installed.

ç»“è®º:
è¾“å‡ºåŸºæœ¬æ»¡è¶³äº†è®¾è®¡è¦æ±‚ã€‚ChatGPT3.5 åœ¨ç¼–ç¨‹æ–¹é¢æœ‰æ‰€è¡¨ç°ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°æœ¬ä¾‹ä¸­ä½¿ç”¨çš„æ¨¡å‹æ˜¯ gpt-3.5-turboã€‚</p>2023-04-11</li><br/><li><span>Toni</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>part 2

åœ¨ part 1 ä¸­ä½¿ç”¨çš„ä»£ç å¦‚ä¸‹:

import openai, os
from langchain.chat_models import ChatOpenAI  #from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chains import SequentialChain

openai.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)
llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=2048, temperature=1)  #text-davinci-003, 2048, max_tokens: 4096 for gpt-3.5-turbo

Q1_prompt = PromptTemplate(
    template=&quot;ç”¨Pythonå†™ä¸€ä¸ªå‡½æ•°ï¼Œè¿›è¡Œæ—¶é—´æ ¼å¼åŒ–è¾“å‡ºï¼Œè¦æ±‚ä»…éœ€è¦æ ¼å¼åŒ–åˆ°å°æ—¶(?h?min?s)ã€‚æ¯”å¦‚ï¼š{Q1}&quot;,
    input_variables=[&quot;Q1&quot;]
)
Q2_prompt = PromptTemplate(
#    template=&quot;è¯·ä¸ºç¨‹åº{A1}ç”¨&#39;pytest&#39; å†™ä¸€ä¸ªå•å…ƒæµ‹è¯•&quot;,
    template=&quot;&quot;&quot;è¯·ä¸ºç¨‹åº{A1}ç”¨&#39;pytest&#39; å†™ä¸€ä¸ªå•å…ƒæµ‹è¯•, 
    Besides the test that counts negative numbers, include test cases like the input string &quot;abc&quot;, 
    and any other test cases you can think of, 
    å°†æ‰€æœ‰çš„ Test Cases å†™å…¥åŒä¸€ä¸ªæµ‹è¯•ä¸­&quot;&quot;&quot;,
    input_variables=[&quot;A1&quot;]
)

chain1 = LLMChain(llm=llm, prompt=Q1_prompt, output_key=&quot;A1&quot;)
chain2 = LLMChain(llm=llm, prompt=Q2_prompt, output_key=&quot;A2&quot;)

q1=&quot;&quot;&quot;
è¾“å…¥  è¾“å‡º
1  1s
61  1min1s
&quot;&quot;&quot;

sequential_chain_p1 = SequentialChain(chains=[chain1], input_variables=[&quot;Q1&quot;], verbose=True)
answer1 = sequential_chain_p1.run(Q1=q1)
print(answer1)

sequential_chain_p2 = SequentialChain(chains=[chain1, chain2], input_variables=[&quot;Q1&quot;], verbose=True)
answer2 = sequential_chain_p2.run(Q1=q1)
print(answer2)

-----------
-----------

å¦‚ä½•å°†ä»£ç å°è£…åœ¨ä¸€ä¸ª App ä¸­å‘¢? 
æœªæ¥çš„ç¨‹åºè¾…åŠ©è®¾è®¡æ˜¯æ²¿è¿™ä¸ªæ€è·¯èµ°è¿˜æ˜¯å¦è¾Ÿè¹Šå¾„? 
å¦‚æœæœ‰äº†ç”¨æˆ·äº¤äº’ç•Œé¢ï¼Œå¦‚ä½•æ§åˆ¶ç”Ÿæˆçš„ç¨‹åºä¸è‡ªå·±ä¹±è·‘ï¼Œäº¦æˆ– &#39;åœ¨æ­£ç¡®ä½¿ç”¨çš„å¼•å¯¼ä¸‹&#39; è®©è‡ªåŠ¨åˆè‡ªåŠ¨ç”Ÿæˆçš„ç¨‹åºè·‘å‡ºäº† &#39;å¤©é™…&#39;ï¼Œä½¿å¾—è®©äººç™¾æ€ä¸å¾—å…¶è§£çš„äº‹ï¼Œè±ç„¶å¼€æœ—äº†èµ·æ¥ã€‚

è§£äº†ä¸€é¢˜ç•™ä¸‹äº†æ›´å¤šé—®é¢˜ã€‚</p>2023-04-11</li><br/><li><span>HXL</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>é‡åˆ°ç»™ä»£ç é—®é¢˜,ç¿»æ–‡æ¡£ä¹Ÿæ²¡æ‰¾åˆ°æ€ä¹ˆè§£å†³,ä¸çŸ¥é“è¯¥å¦‚ä½•ç»™plan_chain ä¼ é€’å‚æ•°. ç°åœ¨ä¸€ç›´æŠ¥ &quot;Error: Missing value for input unit_test_package&quot;

&quot;&quot;&quot;
const modal = new ChatOpenAI({
    maxTokens: 2048,
    temperature: 0.5,
    stop: [&#39;\n\n&#39;],
    topP: 1,
});
&#47;&#47;
const explain_prompt = new PromptTemplate({
    &#47;&#47;...
})
const explain_chain = explain_prompt.pipe(modal);
&#47;&#47;
const plan_prompt = new PromptTemplate({
      template: `
      &quot;&quot;&quot;
    A good unit test suite should aim to:
    - Test the function&#39;s behavior for a wide range of possible inputs
    - Test edge cases that the author may not have foreseen
    - Take advantage of the features of &#39;{unit_test_package}&#39; to make the tests easy to write and maintain
    - Be easy to read and understand, with clean code and descriptive names
    - Be deterministic, so that the tests always pass or fail in the same way

    &#39;{unit_test_package}&#39; has many convenient features that make it easy to write and maintain unit tests. We&#39;ll use them to write unit tests for the function above.

    For this particular function, we&#39;ll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):
    -&quot;&quot;&quot;
      `,
    inputVariables: [&#39;unit_test_package&#39;],
});
const plan_chain = plan_prompt.pipe(modal);
&#47;&#47;
const write_prompt = new PromptTemplate({
    &#47;&#47;...
});
const write_chain = write_prompt.pipe(modal);
&#47;&#47;
async function main(){
    &#47;&#47;
    const test_code = `
    &#47;&#47;...
    `;
    &#47;&#47; FIXME: å¦‚ä½•ç»™ plain_chain ä¼ é€’å‚æ•°ï¼Ÿ
    const sequence = RunnableSequence.from([explain_chain, plan_chain, write_chain]);
    const resp = await sequence.invoke({
        unit_test_package: &#39;jest&#39;,
        function_to_test: test_code,
    })
    console.log(resp);
};
main()
&quot;&quot;&quot;</p>2024-03-17</li><br/><li><span>Esquel-GET IT - gaofeng</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>ä¸Šä¸‹æ–‡è®°å¿†ç­”å¤ï¼Œä¸Šæ–‡æ€»ç»“ï¼Œå°†æ€»ç»“çš„ä¿¡æ¯é™„åŠ åˆ°ä¸‹ä¸€æ¬¡å¯¹è¯ä¸­ï¼Œå¦‚æœæ˜¯å°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼Œç­”æ¡ˆç”±è‹±æ–‡è½¬æ¢æˆä¸­æ–‡ï¼Œå¯¹äºè¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œè‹±æ–‡çš„æ€»ç»“å’Œä¸­æ–‡çš„æ€»ç»“æ˜¯å¦ä¸€è‡´ï¼Ÿåº”è¯¥æ˜¯æœ‰å·®å¼‚çš„å§ï¼Ÿå¯¹äºå›å¤çš„å†…å®¹å¦‚ä½•æé«˜ç²¾å‡†åº¦ï¼Ÿ</p>2024-02-19</li><br/><li><span>å°ç†æƒ³ã€‚</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è€å¸ˆæƒ³é—®ä¸€ä¸‹ï¼Œlangchainå¢åŠ äº†PromptTempleteæœ‰ä»€ä¹ˆæ€§èƒ½çš„ä¼˜åŠ¿å—ï¼Ÿ</p>2023-09-16</li><br/><li><span>èŠ±é›¨ç”°</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>ç†è§£ä¸‹æ–¹ä»£ç ï¼Œå¦‚æœå‡ºç°å¼‚å¸¸ï¼Œå†ç”Ÿæˆä¸€éwrite_unit_testï¼ˆcodeï¼‰ã€‚

æ˜¯å¦é‡ç”Ÿæˆæ—¶ï¼ŒæŠŠå¼‚å¸¸ä¿¡æ¯ä¹Ÿç»™åˆ°è¯­è¨€æ¨¡å‹ä¼šæœ‰å¸®åŠ©ï¼Ÿ

except SyntaxError as e:
            print(f&quot;Syntax error in generated code: {e}&quot;)
            all_code = code + write_unit_test(code)
            tried += 1</p>2023-06-10</li><br/><li><span>éª¨æ±¤é¸¡è›‹é¢</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è€å¸ˆå‰æ–‡è¯´è¿‡ï¼šå¤§è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªç¼ºç‚¹ï¼Œå°±æ˜¯å¯æ§æ€§å·®ã€‚é‚£æ‰€è°“çš„åŸºäºå¤§æ¨¡å‹å¼€å‘ï¼Œæ˜¯ä¸æ˜¯å°±æ˜¯å…ˆé’ˆå¯¹æ¯ä¸ª&#47;å¤šä¸ªé—®é¢˜æ‰¾åˆ°æ¯”è¾ƒå¥½çš„promptï¼Œä»¥ä¾¿äºåŸºäºè¿™ä¸ªprompt èƒ½å¤Ÿæ¯”è¾ƒå¥½çš„å¾—åˆ°æŸç±»é—®é¢˜çš„å›ç­”ï¼Œç„¶åå†ç”¨LangChainè¿™ç±»å·¥å…·å°†promptä¸²èµ·æ¥ï¼Œå³å¯è¿™å¯¹æŸä¸€ä¸ªåœºæ™¯å¾—åˆ°ç›¸å¯¹ç¡®å®šæ•ˆæœçš„ç»“æœã€‚</p>2023-05-27</li><br/>
</ul>