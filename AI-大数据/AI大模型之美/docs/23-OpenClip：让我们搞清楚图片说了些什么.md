ä½ å¥½ï¼Œæˆ‘æ˜¯å¾æ–‡æµ©ã€‚

å‰é¢æˆ‘ä»¬å·²ç»å­¦å®Œäº†æ–‡æœ¬å’ŒéŸ³é¢‘çš„éƒ¨åˆ†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±è¦è¿›å…¥è¯¾ç¨‹çš„æœ€åä¸€éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯å›¾åƒæ¨¡å—äº†ã€‚

ä¸è§†è§‰å’Œè¯­éŸ³ä¸€æ ·ï¼ŒTransformeræ¶æ„çš„æ¨¡å‹åœ¨è¿‡å»å‡ å¹´é‡Œä¹Ÿé€æ¸æˆä¸ºäº†å›¾åƒé¢†åŸŸçš„ä¸€ä¸ªä¸»æµç ”ç©¶æ–¹å‘ã€‚è‡ªç„¶ï¼Œå‘è¡¨äº†GPTå’ŒWhisperçš„OpenAIä¹Ÿä¸ä¼šè½åã€‚ä¸€è´¯ç›¸ä¿¡â€œå¤§åŠ›å‡ºå¥‡è¿¹â€çš„OpenAIï¼Œå°±æ‹¿4äº¿å¼ äº’è”ç½‘ä¸Šæ‰¾åˆ°çš„å›¾ç‰‡ï¼Œä»¥åŠå›¾ç‰‡å¯¹åº”çš„ALTæ–‡å­—è®­ç»ƒäº†ä¸€ä¸ªå«åšCLIPçš„å¤šæ¨¡æ€æ¨¡å‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°±çœ‹çœ‹åœ¨å®é™…çš„åº”ç”¨é‡Œæ€ä¹ˆä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ä½ ä¼šå‘ç°ï¼Œ**æˆ‘ä»¬ä¸ä»…å¯ä»¥æŠŠå®ƒæ‹¿æ¥åšå¸¸è§çš„å›¾ç‰‡åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ï¼Œä¹Ÿèƒ½å¤Ÿç”¨æ¥ä¼˜åŒ–ä¸šåŠ¡åœºæ™¯é‡Œé¢çš„å•†å“æœç´¢å’Œå†…å®¹æ¨èã€‚**

## å¤šæ¨¡æ€çš„CLIPæ¨¡å‹

ç›¸ä¿¡ä½ æœ€è¿‘å·²ç»å¬åˆ°è¿‡å¾ˆå¤šæ¬¡â€œå¤šæ¨¡æ€â€è¿™ä¸ªè¯å„¿äº†ï¼Œæ— è®ºæ˜¯åœ¨OpenAIå¯¹GPT-4çš„ä»‹ç»é‡Œï¼Œè¿˜æ˜¯æˆ‘ä»¬åœ¨ä¹‹å‰ä»‹ç»llama-indexçš„æ—¶å€™ï¼Œè¿™ä¸ªåè¯éƒ½å·²ç»å‡ºç°è¿‡äº†ã€‚

**æ‰€è°“â€œå¤šæ¨¡æ€â€ï¼Œå°±æ˜¯å¤šç§åª’ä½“å½¢å¼çš„å†…å®¹ã€‚**æˆ‘ä»¬çœ‹åˆ°å¾ˆå¤šè¯„æµ‹é‡Œé¢éƒ½æ‹¿GPTæ¨¡å‹æ¥åšæ•°å­¦è¯•é¢˜ï¼Œé‚£ä¹ˆå¦‚æœæˆ‘ä»¬é‡åˆ°ä¸€ä¸ªå¹³é¢å‡ ä½•é¢˜çš„è¯ï¼Œå…‰æœ‰é¢˜ç›®çš„æ–‡å­—ä¿¡æ¯æ˜¯ä¸å¤Ÿçš„ï¼Œè¿˜éœ€è¦æŠŠå¯¹åº”çš„å›¾å½¢ä¸€å¹¶æä¾›ç»™AIæ‰å¯ä»¥ã€‚è€Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬é€šå¾€é€šç”¨äººå·¥æ™ºèƒ½çš„å¿…ç»ä¹‹è·¯ï¼Œå› ä¸ºçœŸå®ä¸–ç•Œå°±æ˜¯å¤šæ¨¡æ€çš„ã€‚æˆ‘ä»¬æ¯å¤©é™¤äº†å¤„ç†æ–‡æœ¬ä¿¡æ¯ï¼Œè¿˜ä¼šçœ‹è§†é¢‘ã€å›¾ç‰‡ä»¥åŠå’Œäººè¯´è¯ã€‚

è€ŒCLIPè¿™ä¸ªæ¨¡å‹ï¼Œå°±æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ã€‚ä¸€å¦‚å³å¾€ï¼ŒOpenAIä»ç„¶æ˜¯é€šè¿‡æµ·é‡æ•°æ®æ¥è®­ç»ƒä¸€ä¸ªå¤§æ¨¡å‹ã€‚æ•´ä¸ªæ¨¡å‹ä½¿ç”¨äº†äº’è”ç½‘ä¸Šçš„4äº¿å¼ å›¾ç‰‡ï¼Œå®ƒä¸ä»…èƒ½å¤Ÿåˆ†åˆ«ç†è§£å›¾ç‰‡å’Œæ–‡æœ¬ï¼Œè¿˜é€šè¿‡å¯¹æ¯”å­¦ä¹ å»ºç«‹äº†å›¾ç‰‡å’Œæ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚è¿™ä¸ªä¹Ÿæ˜¯æœªæ¥æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡å†™å‡ ä¸ªæç¤ºè¯å°±èƒ½ç”¨AIç”»å›¾çš„ä¸€ä¸ªèµ·ç‚¹ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/26/43/263f5f9386b6787564bcdc6b6e8f1343.png?wh=1036x758 "å›¾ç‰‡æ¥è‡ªhttp://proceedings.mlr.press/v139/radford21a/radford21a.pdf")

CLIPçš„æ€è·¯å…¶å®ä¸å¤æ‚ï¼Œå°±æ˜¯äº’è”ç½‘ä¸Šå·²æœ‰çš„å¤§é‡å…¬å¼€çš„å›¾ç‰‡æ•°æ®ã€‚è€Œä¸”å…¶ä¸­æœ‰å¾ˆå¤šå·²ç»é€šè¿‡HTMLæ ‡ç­¾é‡Œé¢çš„titleæˆ–è€…altå­—æ®µï¼Œæä¾›äº†å¯¹å›¾ç‰‡çš„æ–‡æœ¬æè¿°ã€‚é‚£æˆ‘ä»¬åªè¦è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå°†æ–‡æœ¬è½¬æ¢æˆä¸€ä¸ªå‘é‡ï¼Œä¹Ÿå°†å›¾ç‰‡è½¬æ¢æˆä¸€ä¸ªå‘é‡ã€‚å›¾ç‰‡å‘é‡åº”è¯¥å’Œè‡ªå·±çš„æ–‡æœ¬æè¿°å‘é‡çš„è·ç¦»å°½é‡è¿‘ï¼Œå’Œå…¶ä»–çš„æ–‡æœ¬å‘é‡è¦å°½é‡è¿œã€‚é‚£ä¹ˆè¿™ä¸ªæ¨¡å‹ï¼Œå°±èƒ½å¤ŸæŠŠå›¾ç‰‡å’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ä¸ªç©ºé—´é‡Œã€‚æˆ‘ä»¬å°±èƒ½å¤Ÿé€šè¿‡å‘é‡åŒæ—¶ç†è§£å›¾ç‰‡å’Œæ–‡æœ¬äº†ã€‚

```python
<imgÂ src="img_girl.jpg"Â alt="Girl in a jacket"Â width="500"Â height="600">

<img src="/img/html/vangogh.jpg"
     title="Van Gogh, Self-portrait.">
```

æ³¨ï¼šimgæ ‡ç­¾é‡Œçš„altå’Œtitleå­—æ®µï¼Œæä¾›äº†å¯¹å›¾ç‰‡çš„æ–‡æœ¬æè¿°ã€‚

## å›¾ç‰‡çš„é›¶æ ·æœ¬åˆ†ç±»

ç†è§£äº†CLIPæ¨¡å‹çš„åŸºæœ¬æ€è·¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸å¦¨æ¥è¯•ä¸€è¯•è¿™ä¸ªæ¨¡å‹æ€ä¹ˆèƒ½å¤ŸæŠŠæ–‡æœ¬å’Œå›¾ç‰‡å…³è”èµ·æ¥ã€‚æˆ‘ä»¬åˆšåˆšä»‹ç»è¿‡çš„Transformerså¯ä»¥è¯´æ˜¯å½“ä»Šå¤§æ¨¡å‹é¢†åŸŸäº‹å®ä¸Šçš„æ ‡å‡†ï¼Œé‚£æˆ‘å°±è¿˜æ˜¯ç”¨Transformersåº“æ¥ç»™ä½ ä¸¾ä¸ªä¾‹å­å¥½äº†ï¼Œä½ å¯ä»¥çœ‹ä¸€ä¸‹å¯¹åº”çš„ä»£ç ã€‚

```python
import torch
from PIL import Image
from IPython.display import display
from IPython.display import Image as IPyImage
from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def get_image_feature(filename: str):
    image = Image.open(filename).convert("RGB")
    processed = processor(images=image, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        image_features = model.get_image_features(pixel_values=processed["pixel_values"])
    return image_features

def get_text_feature(text: str):
    processed = processor(text=text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        text_features = model.get_text_features(processed['input_ids'])
    return text_features

def cosine_similarity(tensor1, tensor2):
    tensor1_normalized = tensor1 / tensor1.norm(dim=-1, keepdim=True)
    tensor2_normalized = tensor2 / tensor2.norm(dim=-1, keepdim=True)
    return (tensor1_normalized * tensor2_normalized).sum(dim=-1)

image_tensor = get_image_feature("./data/cat.jpg")

cat_text = "This is a cat."
cat_text_tensor = get_text_feature(cat_text)

dog_text = "This is a dog."
dog_text_tensor = get_text_feature(dog_text)

display(IPyImage(filename='./data/cat.jpg'))

print("Similarity with cat : ", cosine_similarity(image_tensor, cat_text_tensor))
print("Similarity with dog : ", cosine_similarity(image_tensor, dog_text_tensor))
```

è¾“å‡ºç»“æœï¼š  
![å›¾ç‰‡](https://static001.geekbang.org/resource/image/41/db/4173ec2a86bcf5173d73b4beceaaacdb.jpg?wh=640x480)

```python
Similarity with cat :  tensor([0.2482])
Similarity with dog :  tensor([0.2080])
```

è¿™ä¸ªä»£ç å¹¶ä¸å¤æ‚ï¼Œåˆ†æˆäº†è¿™æ ·å‡ ä¸ªæ­¥éª¤ã€‚

1. æˆ‘ä»¬å…ˆæ˜¯é€šè¿‡Transformersåº“çš„CLIPModelå’ŒCLIPProcessorï¼ŒåŠ è½½äº†clip-vit-base-patch32è¿™ä¸ªæ¨¡å‹ï¼Œç”¨æ¥å¤„ç†æˆ‘ä»¬çš„å›¾ç‰‡å’Œæ–‡æœ¬ä¿¡æ¯ã€‚
2. åœ¨get\_image\_featuresæ–¹æ³•é‡Œï¼Œæˆ‘ä»¬åšäº†ä¸¤ä»¶äº‹æƒ…ã€‚

<!--THE END-->

- é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡åˆšæ‰æ‹¿åˆ°çš„CLIPProcessorå¯¹å›¾ç‰‡åšé¢„å¤„ç†ï¼Œå˜æˆä¸€ç³»åˆ—çš„æ•°å€¼ç‰¹å¾è¡¨ç¤ºçš„å‘é‡ã€‚è¿™ä¸ªé¢„å¤„ç†çš„è¿‡ç¨‹ï¼Œå…¶å®å°±æ˜¯æŠŠåŸå§‹çš„å›¾ç‰‡ï¼Œå˜æˆä¸€ä¸ªä¸ªåƒç´ çš„RGBå€¼ï¼›ç„¶åç»Ÿä¸€å›¾ç‰‡çš„å°ºå¯¸ï¼Œä»¥åŠå¯¹äºä¸è§„åˆ™çš„å›¾ç‰‡æˆªå–ä¸­é—´æ­£æ–¹å½¢çš„éƒ¨åˆ†ï¼Œæœ€ååšä¸€ä¸‹æ•°å€¼çš„å½’ä¸€åŒ–ã€‚å…·ä½“çš„æ“ä½œæ­¥éª¤ï¼Œå·²ç»å°è£…åœ¨CLIPProcessoré‡Œäº†ï¼Œä½ å¯ä»¥ä¸ç”¨å…³å¿ƒã€‚
- ç„¶åï¼Œæˆ‘ä»¬å†é€šè¿‡CLIPModelï¼ŒæŠŠä¸Šé¢çš„æ•°å€¼å‘é‡ï¼Œæ¨æ–­æˆä¸€ä¸ªè¡¨è¾¾äº†å›¾ç‰‡å«ä¹‰çš„å¼ é‡ï¼ˆTensorï¼‰ã€‚è¿™é‡Œï¼Œä½ å°±æŠŠå®ƒå½“æˆæ˜¯ä¸€ä¸ªå‘é‡å°±å¥½äº†ã€‚

<!--THE END-->

3. åŒæ ·çš„ï¼Œget\_text\_featuresä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œå…ˆæŠŠå¯¹åº”çš„æ–‡æœ¬é€šè¿‡CLIPProcessorè½¬æ¢æˆTokenï¼Œç„¶åå†é€šè¿‡æ¨¡å‹æ¨æ–­å‡ºè¡¨ç¤ºæ–‡æœ¬çš„å¼ é‡ã€‚
4. ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªcosine\_similarityå‡½æ•°ï¼Œç”¨æ¥è®¡ç®—ä¸¤ä¸ªå¼ é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
5. æœ€åï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨ä¸Šé¢çš„è¿™äº›å‡½æ•°ï¼Œæ¥è®¡ç®—å›¾ç‰‡å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦äº†ã€‚æˆ‘ä»¬æ‹¿äº†ä¸€å¼ ç¨‹åºå‘˜ä»¬æœ€å–œæ¬¢çš„çŒ«å’ªç…§ç‰‡ï¼Œå’Œâ€œThis is a cat.â€ ä»¥åŠ â€œThis is a dog.â€ çš„æ–‡æœ¬åšæ¯”è¾ƒã€‚å¯ä»¥çœ‹åˆ°ï¼Œç»“æœçš„ç¡®æ˜¯çŒ«å’ªç…§ç‰‡å’Œâ€œThis is a cat.â€ çš„ç›¸ä¼¼åº¦è¦æ›´é«˜ä¸€äº›ã€‚

æˆ‘ä»¬å¯ä»¥å†å¤šæ‹¿ä¸€äº›æ–‡æœ¬æ¥è¿›è¡Œæ¯”è¾ƒã€‚å›¾ç‰‡é‡Œé¢ï¼Œå®é™…æ˜¯2åªçŒ«å’ªåœ¨æ²™å‘ä¸Šï¼Œé‚£ä¹ˆæˆ‘ä»¬åˆ†åˆ«è¯•è¯•"There are two cats."ã€"This is a couch."ä»¥åŠä¸€ä¸ªå®Œå…¨ä¸ç›¸å…³çš„â€œThis is a truck.â€ï¼Œçœ‹çœ‹æ•ˆæœæ€ä¹ˆæ ·ã€‚

```python
two_cats_text = "There are two cats."
two_cats_text_tensor = get_text_feature(two_cats_text)

truck_text = "This is a truck."
truck_text_tensor = get_text_feature(truck_text)

couch_text = "This is a couch."
couch_text_tensor = get_text_feature(couch_text)

print("Similarity with cat : ", cosine_similarity(image_tensor, cat_text_tensor))
print("Similarity with dog : ", cosine_similarity(image_tensor, dog_text_tensor))
print("Similarity with two cats : ", cosine_similarity(image_tensor, two_cats_text_tensor))
print("Similarity with truck : ", cosine_similarity(image_tensor, truck_text_tensor))
print("Similarity with couch : ", cosine_similarity(image_tensor, couch_text_tensor))
```

è¾“å‡ºç»“æœï¼š

```python
Similarity with cat :  tensor([0.2482])
Similarity with dog :  tensor([0.2080])
Similarity with two cats :  tensor([0.2723])
Similarity with truck :  tensor([0.1814])
Similarity with couch :  tensor([0.2376])
```

å¯ä»¥çœ‹åˆ°ï¼Œâ€œThere are two cats.â€ çš„ç›¸ä¼¼åº¦æœ€é«˜ï¼Œå› ä¸ºå›¾é‡Œæœ‰æ²™å‘ï¼Œæ‰€ä»¥â€œThis is a couch.â€çš„ç›¸ä¼¼åº¦ä¹Ÿè¦é«˜äºâ€œThis is a dog.â€ã€‚è€ŒDogå¥½æ­¹å’ŒCatåŒå±äºå® ç‰©ï¼Œç›¸ä¼¼åº¦ä¹Ÿæ¯”å®Œå…¨ä¸ç›¸å…³çš„Truckè¦é«˜ä¸€äº›ã€‚å¯ä»¥çœ‹åˆ°ï¼ŒCLIPæ¨¡å‹å¯¹å›¾ç‰‡å’Œæ–‡æœ¬çš„è¯­ä¹‰ç†è§£æ˜¯éå¸¸åˆ°ä½çš„ã€‚

çœ‹åˆ°è¿™é‡Œï¼Œä½ æœ‰æ²¡æœ‰è§‰å¾—è¿™å’Œæˆ‘ä»¬è¯¾ç¨‹ä¸€å¼€å§‹çš„æ–‡æœ¬é›¶æ ·æœ¬åˆ†ç±»å¾ˆåƒï¼Ÿçš„ç¡®ï¼ŒCLIPæ¨¡å‹çš„ä¸€ä¸ªéå¸¸é‡è¦çš„ç”¨é€”å°±æ˜¯é›¶æ ·æœ¬åˆ†ç±»ã€‚åœ¨CLIPè¿™æ ·çš„æ¨¡å‹å‡ºç°ä¹‹å‰ï¼Œå›¾åƒè¯†åˆ«å·²ç»æ˜¯ä¸€ä¸ªå‡†ç¡®ç‡éå¸¸é«˜çš„é¢†åŸŸäº†ã€‚é€šè¿‡RESNETæ¶æ„çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œåœ¨ImageNetè¿™æ ·çš„å¤§æ•°æ®é›†ä¸Šï¼Œå·²ç»èƒ½å¤Ÿåšåˆ°90%ä»¥ä¸Šçš„å‡†ç¡®ç‡äº†ã€‚

ä½†æ˜¯è¿™äº›æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªç¼ºé™·ï¼Œå°±æ˜¯å®ƒä»¬éƒ½æ˜¯åŸºäºç›‘ç£å­¦ä¹ çš„æ–¹å¼æ¥è¿›è¡Œåˆ†ç±»çš„ã€‚è¿™æ„å‘³ç€ä¸¤ç‚¹ï¼Œä¸€ä¸ªæ˜¯**æ‰€æœ‰çš„åˆ†ç±»éœ€è¦é¢„å…ˆå®šä¹‰å¥½**ï¼Œæ¯”å¦‚ImageNetå°±æ˜¯é¢„å…ˆå®šä¹‰å¥½äº†1000ä¸ªåˆ†ç±»ã€‚å¦ä¸€ä¸ªæ˜¯**æ•°æ®å¿…é¡»æ ‡æ³¨**ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œè¦ç»™ç”¨æ¥è®­ç»ƒçš„å›¾ç‰‡æ ‡æ³¨å¥½å±äºä»€ä¹ˆç±»ã€‚

è¿™å¸¦æ¥ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯å¦‚æœæˆ‘ä»¬éœ€è¦å¢åŠ ä¸€ä¸ªåˆ†ç±»ï¼Œå°±è¦é‡æ–°è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚æ¯”å¦‚æˆ‘ä»¬å‘ç°æ•°æ®é‡Œé¢æ²¡æœ‰æ ‡æ³¨â€œæ²™å‘â€ï¼Œä¸ºäº†èƒ½å¤Ÿè¯†åˆ«å‡ºæ²™å‘ï¼Œå°±å¾—æ ‡æ³¨ä¸€å †æ•°æ®ï¼ŒåŒæ—¶éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æ¥è°ƒæ•´æ¨¡å‹å‚æ•°çš„æƒé‡ï¼Œéœ€è¦èŠ±è´¹å¾ˆå¤šæ—¶é—´ã€‚

ä½†æ˜¯ï¼Œåœ¨CLIPè¿™æ ·çš„æ¨¡å‹é‡Œï¼Œå¹¶ä¸éœ€è¦è¿™æ ·åšã€‚å› ä¸ºå¯¹åº”çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæ˜¯ä»æµ·é‡å›¾ç‰‡è‡ªå¸¦çš„æ–‡æœ¬ä¿¡æ¯é‡Œæ¥çš„ã€‚å¹¶ä¸”å› ä¸ºåœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¹Ÿå­¦ä¹ åˆ°äº†æ–‡æœ¬ä¹‹é—´çš„å…³è”ï¼Œæ‰€ä»¥å¦‚æœè¦å¯¹ä¸€å¼ å›¾ç‰‡åœ¨å¤šä¸ªç±»åˆ«ä¸­è¿›è¡Œåˆ†ç±»ï¼Œåªéœ€è¦ç®€å•åœ°åˆ—å‡ºåˆ†ç±»çš„æ–‡æœ¬åç§°ï¼Œç„¶åæ¯ä¸€ä¸ªéƒ½å’Œå›¾ç‰‡ç®—ä¸€ä¸‹å‘é‡è¡¨ç¤ºä¹‹é—´çš„ä¹˜ç§¯ï¼Œå†é€šè¿‡Softmaxç®—æ³•åšä¸€ä¸‹å¤šåˆ†ç±»çš„åˆ¤åˆ«å°±å¥½äº†ã€‚

ä¸‹é¢å°±æ˜¯è¿™æ ·ä¸€æ®µç¤ºä¾‹ä»£ç ï¼š

```python
from PIL import Image

from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

image_file = "./data/cat.jpg"
image =  Image.open(image_file)

categories = ["cat", "dog", "truck", "couch"]
categories_text = list(map(lambda x: f"a photo of a {x}", categories))
inputs = processor(text=categories_text, images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1) 

for i in range(len(categories)):
    print(f"{categories[i]}\t{probs[0][i].item():.2%}")
```

è¾“å‡ºç»“æœï¼š

```python
cat	74.51%
dog	0.39%
truck	0.04%
couch	25.07%
```

ä»£ç éå¸¸ç®€å•ï¼Œæˆ‘ä»¬è¿˜æ˜¯å…ˆåŠ è½½modelå’Œprocessorã€‚ä¸è¿‡è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬ä¸å†æ˜¯é€šè¿‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¿›è¡Œåˆ†ç±»äº†ã€‚è€Œæ˜¯ç›´æ¥é€šè¿‡ä¸€ä¸ªåˆ†ç±»çš„åç§°ï¼Œç”¨softmaxç®—æ³•æ¥è®¡ç®—å›¾ç‰‡åº”è¯¥åˆ†ç±»åˆ°å…·ä½“æŸä¸€ä¸ªç±»çš„åç§°çš„æ¦‚ç‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç»™æ‰€æœ‰åç§°éƒ½åŠ ä¸Šäº†ä¸€ä¸ªâ€œa photo of a â€çš„å‰ç¼€ã€‚è¿™æ˜¯ä¸ºäº†è®©æ–‡æœ¬æ•°æ®æ›´æ¥è¿‘CLIPæ¨¡å‹æ‹¿æ¥è®­ç»ƒçš„è¾“å…¥æ•°æ®ï¼Œå› ä¸ºå¤§éƒ¨åˆ†é‡‡é›†åˆ°çš„å›¾ç‰‡ç›¸å…³çš„altå’Œtitleä¿¡æ¯éƒ½ä¸å¤§å¯èƒ½ä¼šæ˜¯ä¸€ä¸ªå•è¯ï¼Œè€Œæ˜¯ä¸€å¥å®Œæ•´çš„æè¿°ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/30/e6/308bf699e76871a5f4c59fe6d26cc6e6.png?wh=978x764 "CLIPæ¨¡å‹çš„è®ºæ–‡ä¸­ï¼Œå…³äºé›¶æ ·æœ¬åˆ†ç±»çš„ç¤ºæ„å›¾")

æˆ‘ä»¬æŠŠå›¾ç‰‡å’Œæ–‡æœ¬éƒ½ä¼ å…¥åˆ°Processorï¼Œå®ƒä¼šè¿›è¡Œæ•°æ®é¢„å¤„ç†ã€‚ç„¶åç›´æ¥æŠŠè¿™ä¸ªinputså¡ç»™Modelï¼Œå°±å¯ä»¥æ‹¿åˆ°è¾“å‡ºç»“æœäº†ã€‚è¾“å‡ºç»“æœçš„logits\_per\_imageå­—æ®µå°±æ˜¯æ¯ä¸€æ®µæ–‡æœ¬å’Œæˆ‘ä»¬è¦åˆ†ç±»çš„å›¾ç‰‡åœ¨è®¡ç®—å®Œå†…ç§¯ä¹‹åçš„ç»“æœã€‚æˆ‘ä»¬åªè¦å†æŠŠè¿™ä¸ªç»“æœé€šè¿‡Softmaxè®¡ç®—ä¸€ä¸‹ï¼Œå°±èƒ½å¾—åˆ°å›¾ç‰‡å±äºå„ä¸ªåˆ†ç±»çš„æ¦‚ç‡ã€‚

ä»æˆ‘ä»¬ä¸Šé¢è¿è¡Œçš„ç»“æœå¯ä»¥çœ‹åˆ°ï¼Œç»“æœè¿˜æ˜¯éå¸¸å‡†ç¡®çš„ï¼Œæ¨¡å‹åˆ¤æ–­æœ‰75%çš„æ¦‚ç‡æ˜¯ä¸€åªçŒ«ï¼Œ25%çš„æ¦‚ç‡æ˜¯æ²™å‘ã€‚è¿™çš„ç¡®ä¹Ÿæ˜¯å›¾ç‰‡ä¸­å®é™…æœ‰çš„å…ƒç´ ï¼Œè€Œä¸”ä»å›¾ç‰‡æ¥çœ‹ï¼ŒçŒ«æ‰æ˜¯å›¾ç‰‡é‡Œçš„ä¸»è§’ã€‚

ä½ å¯ä»¥è‡ªå·±æ‰¾ä¸€äº›çš„å›¾ç‰‡ï¼Œå®šä¹‰ä¸€äº›è‡ªå·±çš„åˆ†ç±»ï¼Œæ¥çœ‹çœ‹åˆ†ç±»æ•ˆæœå¦‚ä½•ã€‚ä¸è¿‡éœ€è¦æ³¨æ„ï¼ŒCLIPæ˜¯ç”¨è‹±æ–‡æ–‡æœ¬è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œåˆ†ç±»çš„åå­—ä½ ä¹Ÿéœ€è¦ç”¨è‹±æ–‡ã€‚

## é€šè¿‡CLIPè¿›è¡Œç›®æ ‡æ£€æµ‹

é™¤äº†èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬çš„å›¾åƒåˆ†ç±»ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†å®ƒåº”ç”¨åˆ°é›¶æ ·æœ¬ä¸‹çš„ç›®æ ‡æ£€æµ‹ä¸­ã€‚ç›®æ ‡æ£€æµ‹å…¶å®å°±æ˜¯æ˜¯åœ¨å›¾åƒä¸­æ¡†å‡ºç‰¹å®šåŒºåŸŸï¼Œç„¶åå¯¹è¿™ä¸ªåŒºåŸŸå†…çš„å›¾åƒå†…å®¹è¿›è¡Œåˆ†ç±»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åŒæ ·å¯ä»¥ç”¨CLIPæ¥å®ç°ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚

äº‹å®ä¸Šï¼ŒGoogleå°±åŸºäºCLIPï¼Œå¼€å‘äº†OWL-ViTè¿™ä¸ªæ¨¡å‹æ¥åšé›¶æ ·æœ¬çš„ç›®æ ‡æ£€æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨[ä¸Šä¸€è®²](https://time.geekbang.org/column/article/652734)å­¦è¿‡çš„Pipelineæ¥è¯•ä¸€è¯•å®ƒæ˜¯æ€ä¹ˆå¸®åŠ©æˆ‘ä»¬åšç›®æ ‡æ£€æµ‹çš„ã€‚

ç›®æ ‡æ£€æµ‹ï¼š

```python
from transformers import pipeline

detector = pipeline(model="google/owlvit-base-patch32", task="zero-shot-object-detection")
detected = detector(
    "./data/cat.jpg",
    candidate_labels=["cat", "dog", "truck", "couch", "remote"],
)

print(detected)
```

è¾“å‡ºç»“æœï¼š

```python
[{'score': 0.2868116796016693, 'label': 'cat', 'box': {'xmin': 324, 'ymin': 20, 'xmax': 640, 'ymax': 373}}, {'score': 0.2770090401172638, 'label': 'remote', 'box': {'xmin': 40, 'ymin': 72, 'xmax': 177, 'ymax': 115}}, {'score': 0.2537277638912201, 'label': 'cat', 'box': {'xmin': 1, 'ymin': 55, 'xmax': 315, 'ymax': 472}}, {'score': 0.14742951095104218, 'label': 'remote', 'box': {'xmin': 335, 'ymin': 74, 'xmax': 371, 'ymax': 187}}, {'score': 0.12083035707473755, 'label': 'couch', 'box': {'xmin': 4, 'ymin': 0, 'xmax': 642, 'ymax': 476}}]
```

å¯ä»¥çœ‹åˆ°ä¸€æ—¦ç”¨ä¸ŠPipelineï¼Œä»£ç å°±å˜å¾—ç‰¹åˆ«ç®€å•äº†ã€‚æˆ‘ä»¬å…ˆå®šä¹‰äº†ä¸€ä¸‹modelå’Œtaskï¼Œç„¶åè¾“å…¥äº†æˆ‘ä»¬ç”¨æ¥æ£€æµ‹çš„å›¾ç‰‡ï¼Œä»¥åŠæä¾›çš„ç±»åˆ«å°±å®Œäº‹äº†ã€‚ä»æ‰“å°å‡ºæ¥çš„ç»“æœä¸­å¯ä»¥çœ‹åˆ°ï¼Œé‡Œé¢åŒ…å«äº†æ¨¡å‹æ£€æµ‹å‡ºæ¥çš„æ‰€æœ‰ç‰©å“çš„è¾¹æ¡†ä½ç½®ã€‚è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬è¿˜ç‰¹åœ°å¢åŠ äº†ä¸€ä¸ªremoteï¼Œä¹Ÿå°±æ˜¯é¥æ§å™¨çš„ç±»åˆ«ï¼Œçœ‹çœ‹è¿™æ ·çš„å°ç‰©ä½“æ¨¡å‹æ˜¯ä¸æ˜¯ä¹Ÿèƒ½è¯†åˆ«å‡ºæ¥ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±æŠŠè¾¹æ¡†æ ‡æ³¨åˆ°å›¾ç‰‡ä¸Šï¼Œçœ‹çœ‹æ£€æµ‹çš„ç»“æœæ˜¯å¦å‡†ç¡®ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ä¸€ä¸‹OpenCVã€‚

```python
pip install opencv-python
```

åé¢çš„ä»£ç ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯éå†ä¸€ä¸‹ä¸Šé¢æ£€æµ‹æ‹¿åˆ°çš„ç»“æœï¼Œç„¶åé€šè¿‡OpenCVæŠŠè¾¹æ¡†ç»˜åˆ¶åˆ°å›¾ç‰‡ä¸Šå°±å¥½äº†ã€‚

è¾“å‡ºç›®æ ‡æ£€æµ‹ç»“æœï¼š

```python
import cv2
from matplotlib import pyplot as plt

# Read the image
image_path = "./data/cat.jpg"
image = cv2.imread(image_path)

# Convert the image from BGR to RGB format
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Draw the bounding box and label for each detected object
for detection in detected:
    box = detection['box']
    label = detection['label']
    score = detection['score']
    
    # Draw the bounding box and label on the image
    xmin, ymin, xmax, ymax = box['xmin'], box['ymin'], box['xmax'], box['ymax']
    cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
    cv2.putText(image, f"{label}: {score:.2f}", (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Display the image in Jupyter Notebook
plt.imshow(image)
plt.axis('off')
plt.show()
```

è¾“å‡ºç»“æœï¼š  
![å›¾ç‰‡](https://static001.geekbang.org/resource/image/4c/3b/4c42b82ffd8286b225739bbf778f673b.png?wh=512x389)

ä»æœ€åçš„è¾“å‡ºç»“æœæ¥çœ‹ï¼Œæ— è®ºæ˜¯çŒ«å’ªã€é¥æ§å™¨è¿˜æ˜¯æ²™å‘ï¼Œéƒ½è¢«å‡†ç¡®åœ°æ¡†é€‰å‡ºæ¥äº†ã€‚

## å•†å“æœç´¢ä¸ä»¥å›¾æœå›¾

CLIPæ¨¡å‹èƒ½æŠŠæ–‡æœ¬å’Œå›¾ç‰‡éƒ½å˜æˆåŒä¸€ä¸ªç©ºé—´é‡Œé¢çš„å‘é‡ã€‚è€Œä¸”ï¼Œæ–‡æœ¬å’Œå›¾ç‰‡ä¹‹é—´è¿˜æœ‰å…³è”ï¼Œè¿™å°±è®©æˆ‘ä»¬æƒ³åˆ°äº†[ç¬¬ 9 è®²](https://time.geekbang.org/column/article/644795)å­¦è¿‡çš„å†…å®¹ã€‚æˆ‘ä»¬æ˜¯ä¸æ˜¯å¯ä»¥åˆ©ç”¨è¿™ä¸ªå‘é‡æ¥è¿›è¡Œè¯­ä¹‰æ£€ç´¢ï¼Œå®ç°æœç´¢å›¾ç‰‡çš„åŠŸèƒ½ï¼Ÿç­”æ¡ˆå½“ç„¶æ˜¯å¯ä»¥çš„ï¼Œå…¶å®è¿™ä¹Ÿæ˜¯CLIPçš„ä¸€ä¸ªå¸¸ç”¨åŠŸèƒ½ã€‚æˆ‘ä»¬æ¥ä¸‹æ¥å°±è¦é€šè¿‡ä»£ç æ¥æ¼”ç¤ºè¿™ä¸ªæœç´¢çš„ç”¨æ³•ã€‚

è¦æ¼”ç¤ºå•†å“æœç´¢åŠŸèƒ½ï¼Œæˆ‘ä»¬è¦å…ˆæ‰¾åˆ°ä¸€ä¸ªæ•°æ®é›†ã€‚è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦çš„æ•°æ®æ˜¯å›¾ç‰‡ï¼Œè¿™æˆ‘ä»¬å°±æ²¡åŠæ³•ç›´æ¥é€šè¿‡ChatGPTæ¥é€ äº†ã€‚ä¸è¿‡ï¼Œæ­£å¥½æˆ‘ä»¬å¯ä»¥å­¦ä¹ HuggingFaceæä¾›çš„ [Datasetæ¨¡å—](https://huggingface.co/datasets)ã€‚

æ‰€æœ‰çš„æœºå™¨å­¦ä¹ é—®é¢˜éƒ½éœ€è¦æœ‰ä¸€å¥—æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡æ•°æ®æ¥è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ¨¡å‹ã€‚æ‰€ä»¥ä½œä¸ºæœ€å¤§çš„å¼€æºæœºå™¨å­¦ä¹ ç¤¾åŒºï¼ŒHuggingFaceå°±æä¾›äº†è¿™æ ·ä¸€ä¸ªæ¨¡å—ï¼Œè®©å¼€å‘äººå‘˜å¯ä»¥æŠŠä»–ä»¬çš„æ•°æ®é›†åˆ†äº«å‡ºæ¥ã€‚å¹¶ä¸”è¿™äº›æ•°æ®é›†ï¼Œéƒ½å¯ä»¥é€šè¿‡ datasets åº“çš„ load\_dataset æ–¹æ³•åŠ è½½åˆ°å†…å­˜é‡Œé¢æ¥ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/25/9e/25bd82c324cff8ee8ff375ac0e19b49e.png?wh=1157x400 "åœ¨æœç´¢æ é‡Œé€šè¿‡å…³é”®å­—æŸ¥æ‰¾æˆ‘ä»¬æƒ³è¦çš„å•†å“å›¾ç‰‡æ•°æ®é›†")

æˆ‘ä»¬æƒ³è¦æ‰¾ä¸€äº›å•†å“å›¾ç‰‡ï¼Œé‚£ä¹ˆå°±å¯ä»¥åœ¨HuggingFaceçš„æœç´¢æ é‡Œè¾“å…¥ product imageã€‚ç„¶åç‚¹å‡»Datasetsä¸‹æ‰¾åˆ°çš„æ•°æ®é›†ï¼Œè¿›å…¥æ•°æ®é›†çš„è¯¦æƒ…é¡µã€‚å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªå«åš [ecommece\_products\_clip çš„æ•°æ®é›†é‡Œ](https://huggingface.co/datasets/rajuptvs/ecommerce_products_clip)ï¼Œçš„ç¡®æ¯ä¸€æ¡è®°å½•éƒ½æœ‰å•†å“å›¾ç‰‡ï¼Œé‚£æ‹¿æ¥åšæˆ‘ä»¬çš„å›¾ç‰‡æœç´¢æ¼”ç¤ºå†åˆé€‚ä¸è¿‡äº†ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/0e/a8/0eaa63e4e4611e5484a809f6504d34a8.png?wh=684x780)

åŠ è½½æ•°æ®é›†éå¸¸ç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦è°ƒç”¨ä¸€ä¸‹ load\_dataset æ–¹æ³•ï¼Œå¹¶ä¸”æŠŠæ•°æ®é›†çš„åå­—ä½œä¸ºå‚æ•°å°±å¯ä»¥äº†ã€‚å¯¹äºæ‹¿åˆ°çš„æ•°æ®é›†ï¼Œä½ å¯ä»¥çœ‹åˆ°é‡Œé¢ä¸€å…±æœ‰1913æ¡æ•°æ®ï¼Œå¹¶ä¸”åˆ—å‡ºäº†æ‰€æœ‰featureçš„åå­—ã€‚

```python
from datasets import load_dataset

dataset = load_dataset("rajuptvs/ecommerce_products_clip")
dataset
```

è¾“å‡ºç»“æœï¼š

```python
DatasetDict({
    train: Dataset({
        features: ['image', 'Product_name', 'Price', 'colors', 'Pattern', 'Description', 'Other Details', 'Clipinfo'],
        num_rows: 1913
    })
})
```

æ•°æ®é›†ä¸€èˆ¬éƒ½ä¼šé¢„å…ˆåˆ†ç‰‡ï¼Œåˆ†æˆ**è®­ç»ƒé›†ï¼ˆtrainï¼‰ã€éªŒè¯é›†ï¼ˆvalidationï¼‰å’Œæµ‹è¯•é›†ï¼ˆtestï¼‰**ä¸‰ç§ã€‚æˆ‘ä»¬è¿™é‡Œä¸æ˜¯åšæœºå™¨å­¦ä¹ è®­ç»ƒï¼Œè€Œæ˜¯æ¼”ç¤ºä¸€ä¸‹é€šè¿‡CLIPæ¨¡å‹åšæœç´¢ï¼Œæ‰€ä»¥æˆ‘ä»¬é€‰ç”¨äº†æ•°æ®æœ€å¤šçš„trainè¿™ä¸ªæ•°æ®åˆ†ç‰‡ã€‚æˆ‘ä»¬é€šè¿‡Matplotlibè¿™ä¸ªåº“ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸‹å‰10ä¸ªå•†å“çš„å›¾ç‰‡ï¼Œç¡®è®¤æ•°æ®å’Œæˆ‘ä»¬æƒ³çš„æ˜¯ä¸€æ ·çš„ã€‚

```python
import matplotlib.pyplot as plt

training_split = dataset["train"]

def display_images(images):
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    axes = axes.ravel()

    for idx, img in enumerate(images):
        axes[idx].imshow(img)
        axes[idx].axis('off')

    plt.subplots_adjust(wspace=0.2, hspace=0.2)
    plt.show()

images = [example["image"] for example in training_split.select(range(10))]
display_images(images)
```

è¾“å‡ºç»“æœï¼š

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/67/89/678702faf0b9969098523749e8b12a89.png?wh=1159x482)

æœ‰äº†æ•°æ®é›†ï¼Œæˆ‘ä»¬è¦åšçš„ç¬¬ä¸€ä»¶äº‹æƒ…ï¼Œå°±æ˜¯é€šè¿‡CLIPæ¨¡å‹æŠŠæ‰€æœ‰çš„å›¾ç‰‡éƒ½è½¬æ¢æˆå‘é‡å¹¶ä¸”è®°å½•ä¸‹æ¥ã€‚è·å–å›¾ç‰‡å‘é‡çš„æ–¹æ³•å’Œæˆ‘ä»¬ä¸Šé¢åšé›¶æ ·æœ¬åˆ†ç±»ç±»ä¼¼ï¼Œæˆ‘ä»¬åŠ è½½äº†CLIPModelå’ŒCLIPProcessorï¼Œé€šè¿‡get\_image\_featureså‡½æ•°æ‹¿åˆ°å‘é‡ï¼Œå†é€šè¿‡add\_image\_featureå‡½æ•°æŠŠè¿™äº›å‘é‡åŠ å…¥åˆ°featuresç‰¹å¾é‡Œé¢ã€‚

æˆ‘ä»¬ä¸€æ¡è®°å½•ä¸€æ¡è®°å½•åœ°æ¥å¤„ç†è®­ç»ƒé›†é‡Œé¢çš„å›¾ç‰‡ç‰¹å¾ï¼Œå¹¶ä¸”æŠŠå¤„ç†å®Œæˆçš„ç‰¹å¾ä¹ŸåŠ å…¥åˆ°æ•°æ®é›†çš„featureså±æ€§é‡Œé¢å»ã€‚

```python
import torch
import torchvision.transforms as transforms
from PIL import Image
from datasets import load_dataset
from transformers import CLIPProcessor, CLIPModel

device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def get_image_features(image):
    with torch.no_grad():
        inputs = processor(images=[image], return_tensors="pt", padding=True)
        inputs.to(device)
        features = model.get_image_features(**inputs)
    return features.cpu().numpy()

def add_image_features(example):
    example["features"] = get_image_features(example["image"])
    return example

# Apply the function to the training_split
training_split = training_split.map(add_image_features)
```

æœ‰äº†å¤„ç†å¥½çš„å‘é‡ï¼Œé—®é¢˜å°±å¥½åŠäº†ã€‚æˆ‘ä»¬å¯ä»¥ä»¿ç…§[ç¬¬ 9 è®²](https://time.geekbang.org/column/article/644795)çš„åŠæ³•ï¼ŒæŠŠè¿™äº›å‘é‡éƒ½æ”¾åˆ°Faissçš„ç´¢å¼•é‡Œé¢å»ã€‚

```python
import numpy as np
import faiss

features = [example["features"] for example in training_split]
features_matrix = np.vstack(features)

dimension = features_matrix.shape[1]

index = faiss.IndexFlatL2(dimension)
index.add(features_matrix.astype('float32'))
```

æœ‰äº†è¿™ä¸ªç´¢å¼•ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦æ¥æœç´¢å›¾ç‰‡äº†ã€‚æˆ‘ä»¬é€šè¿‡ä¸‹é¢å››ä¸ªæ­¥éª¤æ¥å®Œæˆè¿™ä¸ªç”¨æ–‡å­—æœç´¢å›¾ç‰‡çš„åŠŸèƒ½ã€‚

1. é¦–å…ˆ get\_text\_features è¿™ä¸ªå‡½æ•°ä¼šé€šè¿‡CLIPModelå’ŒCLIPProcessoræ‹¿åˆ°ä¸€æ®µæ–‡æœ¬è¾“å…¥çš„å‘é‡ã€‚
2. å…¶æ¬¡æ˜¯ search å‡½æ•°ã€‚å®ƒæ¥æ”¶ä¸€æ®µæœç´¢æ–‡æœ¬ï¼Œç„¶åå°†æ–‡æœ¬é€šè¿‡ get\_text\_features è½¬æ¢æˆå‘é‡ï¼Œå»Faissé‡Œé¢æœç´¢å¯¹åº”çš„å‘é‡ç´¢å¼•ã€‚ç„¶åé€šè¿‡è¿™ä¸ªç´¢å¼•é‡æ–°ä»training\_splité‡Œé¢æ‰¾åˆ°å¯¹åº”çš„å›¾ç‰‡ï¼ŒåŠ å…¥åˆ°è¿”å›ç»“æœé‡Œé¢å»ã€‚
3. ç„¶åæˆ‘ä»¬å°±ä»¥A red dressä½œä¸ºæœç´¢è¯ï¼Œè°ƒç”¨searchå‡½æ•°æ‹¿åˆ°æœç´¢ç»“æœã€‚
4. æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ display\_search\_results è¿™ä¸ªå‡½æ•°ï¼Œå°†æœç´¢åˆ°çš„å›¾ç‰‡ä»¥åŠåœ¨Faissç´¢å¼•ä¸­çš„è·ç¦»å±•ç¤ºå‡ºæ¥ã€‚

ä¸Šé¢è¿™å››ä¸ªæ­¥éª¤ï¼Œå…¶å®åœ¨ä¹‹å‰çš„è¯¾ç¨‹ä¸­éƒ½æˆ‘ä»¬éƒ½è®²è¿‡ã€‚æˆ‘ä»¬é€šè¿‡è¿™äº›æ–¹æ³•çš„ç»„åˆï¼Œå°±å®ç°äº†ä¸€ä¸ªé€šè¿‡å…³é”®è¯æœç´¢å•†å“å›¾ç‰‡çš„åŠŸèƒ½ã€‚è€Œä»æœç´¢ç»“æœä¸­å¯ä»¥çœ‹åˆ°ï¼Œæ’åé å‰çš„çš„ç¡®éƒ½æ˜¯çº¢è‰²çš„è£™å­ã€‚

```python
def get_text_features(text):
    with torch.no_grad():
        inputs = processor(text=[text], return_tensors="pt", padding=True)
        inputs.to(device)
        features = model.get_text_features(**inputs)
    return features.cpu().numpy()

def search(query_text, top_k=5):
    # Get the text feature vector for the input query
    text_features = get_text_features(query_text)

    # Perform a search using the FAISS index
    distances, indices = index.search(text_features.astype("float32"), top_k)

    # Get the corresponding images and distances
    results = [
        {"image": training_split[i]["image"], "distance": distances[0][j]}
        for j, i in enumerate(indices[0])
    ]

    return results

query_text = "A red dress"
results = search(query_text)

# Display the search results
def display_search_results(results):
    fig, axes = plt.subplots(1, len(results), figsize=(15, 5))
    axes = axes.ravel()

    for idx, result in enumerate(results):
        axes[idx].imshow(result["image"])
        axes[idx].set_title(f"Distance: {result['distance']:.2f}")
        axes[idx].axis('off')

    plt.subplots_adjust(wspace=0.2, hspace=0.2)
    plt.show()

display_search_results(results)
```

è¾“å‡ºç»“æœï¼š

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/27/8d/277ca44840ff7efa314c46ccf6221b8d.png?wh=1182x295)

æœ‰äº†é€šè¿‡æ–‡æœ¬æœç´¢å•†å“ï¼Œç›¸ä¿¡ä½ ä¹ŸçŸ¥é“å¦‚ä½•ä»¥å›¾æœå›¾äº†ã€‚æˆ‘ä»¬åªéœ€è¦æŠŠ get\_text\_features æ¢æˆä¸€ä¸ª get\_image\_features å°±èƒ½åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä¹ŸæŠŠå¯¹åº”çš„ä»£ç æ”¾åœ¨ä¸‹é¢ã€‚

```python
def get_image_features(image_path):
    # Load the image from the file
    image = Image.open(image_path).convert("RGB")
    
    with torch.no_grad():
        inputs = processor(images=[image], return_tensors="pt", padding=True)
        inputs.to(device)
        features = model.get_image_features(**inputs)
    return features.cpu().numpy()

def search(image_path, top_k=5):
    # Get the image feature vector for the input image
    image_features = get_image_features(image_path)

    # Perform a search using the FAISS index
    distances, indices = index.search(image_features.astype("float32"), top_k)

    # Get the corresponding images and distances
    results = [
        {"image": training_split[i.item()]["image"], "distance": distances[0][j]}
        for j, i in enumerate(indices[0])
    ]

    return results

image_path = "./data/shirt.png"
results = search(image_path)

display(IPyImage(filename=image_path, width=300, height=200))
display_search_results(results)

```

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/32/69/32f22ed545be30abe43a8f39cf5b8369.png?wh=1920x757 "ç”¨æ¥æœç´¢çš„è¡¬è¡«å›¾ç‰‡")

è¾“å‡ºç»“æœï¼š

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/48/d3/48e900a88c63d7253dbc7e9e4aa0bcd3.png?wh=1182x306)

ä»æœç´¢ç»“æœå¯ä»¥çœ‹åˆ°ï¼Œå°½ç®¡ç”¨æ¥æœç´¢çš„è¡¬è¡«å›¾ç‰‡çš„è§†è§’å’Œé£æ ¼ä¸å•†å“åº“é‡Œé¢çš„å›¾ç‰‡å®Œå…¨ä¸åŒï¼Œä½†æ˜¯æœç´¢åˆ°çš„å›¾ç‰‡ä¹Ÿéƒ½æ˜¯æœ‰è“è‰²å…ƒç´ çš„è¡¬è¡«ï¼Œç”±æ­¤å¯è§ï¼ŒCLIPæ¨¡å‹å¯¹äºè¯­ä¹‰çš„æ•æ‰è¿˜æ˜¯éå¸¸å‡†ç¡®çš„ã€‚

## å°ç»“

å¥½äº†ï¼Œè¿™ä¸€è®²åˆ°è¿™é‡Œå°±ç»“æŸäº†ï¼Œæœ€åæˆ‘ä»¬ä¸€èµ·æ¥æ€»ç»“å¤ä¹ ä¸€ä¸‹ã€‚

è¿™ä¸€è®²ï¼Œæˆ‘ä¸ºä½ ä»‹ç»äº†OpenAIå¼€æºçš„CLIPæ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯é€šè¿‡äº’è”ç½‘ä¸Šçš„æµ·é‡å›¾ç‰‡æ•°æ®ï¼Œä»¥åŠå›¾ç‰‡å¯¹åº”çš„imgæ ‡ç­¾é‡Œé¢çš„altå’Œtitleå­—æ®µä¿¡æ¯è®­ç»ƒå‡ºæ¥çš„ã€‚è¿™ä¸ªæ¨¡å‹æ— éœ€é¢å¤–çš„æ ‡æ³¨ï¼Œå°±èƒ½å°†å›¾ç‰‡å’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ä¸ªå‘é‡ç©ºé—´ï¼Œè®©æˆ‘ä»¬èƒ½æŠŠæ–‡æœ¬å’Œå›¾ç‰‡å…³è”èµ·æ¥ã€‚

é€šè¿‡CLIPæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ä»»æ„ç‰©å“åç§°è¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ã€‚è¿›ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬è¿˜èƒ½è¿›è¡Œé›¶æ ·æœ¬çš„ç›®æ ‡æ£€æµ‹ã€‚è€Œæ–‡æœ¬å’Œå›¾ç‰‡åœ¨åŒä¸€ä¸ªå‘é‡ç©ºé—´çš„è¿™ä¸ªç‰¹æ€§ï¼Œä¹Ÿèƒ½å¤Ÿè®©æˆ‘ä»¬ç›´æ¥åˆ©ç”¨è¿™ä¸ªæ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–æˆ‘ä»¬çš„å•†å“æœç´¢åŠŸèƒ½ã€‚æˆ‘ä»¬å¯ä»¥æ‹¿æ–‡æœ¬çš„å‘é‡ï¼Œé€šè¿‡æ‰¾åˆ°ä½™å¼¦è·ç¦»æœ€è¿‘çš„å•†å“å›¾ç‰‡æ¥ä¼˜åŒ–æœç´¢çš„å¬å›è¿‡ç¨‹ã€‚æˆ‘ä»¬ä¹Ÿèƒ½ç›´æ¥æ‹¿å›¾ç‰‡å‘é‡ï¼Œå®ç°ä»¥å›¾æœå›¾è¿™æ ·çš„åŠŸèƒ½ã€‚

CLIPè¿™æ ·çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æ‹“å±•äº†æˆ‘ä»¬AIçš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç°åœ¨å†™å‡ ä¸ªæç¤ºè¯­ï¼Œå°±èƒ½è®©AIæ‹¥æœ‰ç»˜ç”»çš„èƒ½åŠ›ï¼Œè¿™ä¸€ç‚¹ä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯å‘ç«¯äºæ­¤çš„ã€‚è€Œåœ¨æ¥ä¸‹æ¥çš„å‡ è®²é‡Œé¢ï¼Œæˆ‘ä»¬å°±è¦çœ‹çœ‹åº”è¯¥æ€ä¹ˆä½¿ç”¨AIæ¥ç”»ç”»äº†ã€‚

## æ€è€ƒé¢˜

ä½ èƒ½è¯•ä¸€è¯•ï¼Œé€šè¿‡Pipelineæ¥å®ç°æˆ‘ä»¬ä»Šå¤©ä»‹ç»çš„å›¾ç‰‡é›¶æ ·æœ¬åˆ†ç±»å—ï¼Ÿè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»çš„æ—¶å€™ï¼Œä½ é€‰å–äº†å“ªä¸€ä¸ªæ¨¡å‹å‘¢ï¼Ÿæ¬¢è¿ä½ åœ¨è¯„è®ºåŒºå’Œæˆ‘äº¤æµè®¨è®ºï¼Œä¹Ÿæ¬¢è¿ä½ æŠŠè¿™ä¸€è®²åˆ†äº«ç»™éœ€è¦çš„æœ‹å‹ï¼Œæˆ‘ä»¬ä¸‹ä¸€è®²å†è§ï¼

## æ¨èé˜…è¯»

å¦‚æœä½ æƒ³è¦å¯¹è®¡ç®—æœºè§†è§‰çš„æ·±åº¦å­¦ä¹ æœ‰ä¸€ä¸ªå¿«é€Ÿåœ°äº†è§£ï¼Œé‚£ä¹ˆPineconeæä¾›çš„è¿™ä»½ [Embedding Methods for Image Search](https://www.pinecone.io/learn/image-search/) æ˜¯ä¸€ä»½å¾ˆå¥½çš„æ•™ç¨‹ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ10ï¼‰</strong></div><ul>
<li><span>pomyin</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>å•†å“æœç´¢ä¸ä»¥å›¾æœå›¾éƒ¨åˆ†ï¼Œå…¶ä¸­searchå‡½æ•°éƒ¨åˆ†ä»£ç æœ‰è¯¯ï¼ˆ training_split[i][&quot;image&quot;] ï¼‰ï¼Œæˆ‘æŸ¥äº†huggingfaceçš„Datasetæ•°æ®ç±»å‹ï¼Œåº”è¯¥æ”¹æˆï¼štraining_split.select([i])[&quot;image&quot;]ã€‚æˆ‘çš„python 3.7ï¼Œtransformersæ˜¯4.28.1ã€‚</p>2023-05-09</li><br/><li><span>peter</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>Q1ï¼šç¬”è®°æœ¬ï¼Œwin10ï¼Œå®‰è£…äº†Anacondaï¼Œè¿™ä¸ªç¯å¢ƒå¯ä»¥å—ï¼Ÿæˆ‘å®‰è£…äº†Pycharmã€‚åº”è¯¥æ˜¯Pycharmç”¨æ¥ç¼–å†™ä»£ç ï¼ŒAnacondaæä¾›åº•å±‚æ”¯æŒï¼Œæ˜¯è¿™æ ·å—ï¼Ÿ
Q2ï¼šç”¨æŸä¸ªäººçš„å£°éŸ³æ¥æ’­éŸ³æˆ–è€…é˜…è¯»ä¸€æ®µæ–‡å­—ï¼Œæœ‰æˆç†Ÿçš„æ–¹æ¡ˆå—ï¼Ÿ
æ¯”å¦‚ï¼Œæƒ³ç”¨ç‰¹æœ—æ™®çš„å£°éŸ³æ¥æ’­æŠ¥ä¸€æ®µæ–°é—»ï¼Œæˆ–è€…è¯»ä¸€ç¯‡æ–‡ç« ï¼Œæ˜¯å¦æœ‰æˆç†Ÿçš„æ–¹æ¡ˆï¼Ÿå¼€æºæˆ–å•†ç”¨çš„éƒ½å¯ä»¥ã€‚</p>2023-05-03</li><br/><li><span>Steven</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç›®æ ‡æ£€æµ‹éƒ¨åˆ†ï¼Œæˆ‘çš„ transformers ç‰ˆæœ¬æ˜¯ 4.24.0ï¼Œæ–‡ä¸­ä»£ç æ‰§è¡Œå¼‚å¸¸ï¼Œä¿®æ”¹æˆä¸‹é¢çš„ä»£ç å¾—åˆ°ç»“æœï¼š
detected = detector(&quot;.&#47;data&#47;cat.jpg&quot;,
    text_queries=[&quot;cat&quot;, &quot;dog&quot;, &quot;truck&quot;, &quot;couch&quot;, &quot;remote&quot;])[0]
</p>2023-04-28</li><br/><li><span>Toni</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è°ƒç”¨äº†å‡ ä¸ªæ¨¡å‹å¯¹å›¾ç‰‡è¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ï¼Œè¯•éªŒå›¾åƒä½¿ç”¨çš„æ˜¯æœ¬è¯¾ä¸­çš„&#39;ä¸¤åªçŒ«å’ª&#39;ï¼Œä¸ºä¾¿äºæ¨¡å‹é—´çš„æ¯”è¾ƒï¼Œå–å®Œå…¨ç›¸åŒçš„åˆ†ç±»å‚æ•°: candidate_labels=[&quot;cat&quot;, &quot;dog&quot;, &quot;truck&quot;, &quot;couch&quot;, &quot;remote&quot;]ã€‚

æ¨¡å‹1: task=&quot;zero-shot-image-classification&quot;, model=&quot;openai&#47;clip-vit-large-patch14&quot;
æ¨¡å‹2: task=&quot;zero-shot-image-classification&quot;, model=&quot;laion&#47;CLIP-ViT-H-14-laion2B-s32B-b79K&quot;
æ¨¡å‹3: task=&quot;zero-shot-classification&quot;, model=&quot;typeform&#47;distilbert-base-uncased-mnli&quot;
æ¨¡å‹4: task=&quot;zero-shot-classification&quot;, model=&quot;MoritzLaurer&#47;mDeBERTa-v3-base-mnli-xnli&quot;

ä¸‹é¢æ˜¯å„ä¸ªæ¨¡å‹è¿ç®—çš„ç»“æœ:

model             cat          dog         truck        couch       remote
1              15.03%      0.02%      0.01%      78.11%      6.82% 
2              15.04%      0.00%      0.00%      84.95%      0.01%
3              13.22%     11.75%    18.13%      36.99%     19.90%
4              18.52%     16.02%    13.60%      24.22%     27.64%

è¿™å‡ ä¸ªæ¨¡å‹çš„ç»“æœéƒ½ä¸å¦‚ task=&quot;zero-shot-object-detection&quot;, model=&quot;google&#47;owlvit-base-patch32&quot;
è°·æ­Œè¿™ä¸ªæ¨¡å‹çš„ä¼˜ç‚¹æ˜¯ä»å›¾ä¸­åˆ†è¾¨å‡ºä¸¤åªçŒ«å’Œä¸¤ä¸ªé¥æ§å™¨ã€‚

æœ€åæ˜¯ä¸€ä¸ªä¸­æ–‡æ¨¡å‹ï¼Œmodel_id=&quot;lyua1225&#47;clip-huge-zh-75k-steps-bs4096&quot;
https:&#47;&#47;huggingface.co&#47;models?pipeline_tag=zero-shot-image-classification&amp;language=zh&amp;sort=downloads

[&quot;çŒ«&quot;,    &quot;ç‹—&quot;,   &quot;æ‹–è½¦&quot;,   &quot;é•¿æ²™å‘&quot;,  &quot;é¥æ§å™¨&quot;]
[0.996     0.       0.          0.004          0.   ]

æ²¡æœ‰è¯†åˆ«å‡ºé¥æ§å™¨ã€‚</p>2023-04-29</li><br/><li><span>å°ç†æƒ³ã€‚</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è€å¸ˆï¼Œæ–‡æœ¬å‘é‡æœç´ å›¾ç‰‡é‚£é‡Œä¸‹é¢è¿™æ®µä»£ç æ˜¯æŠ¥é”™çš„
results = [ {&quot;image&quot;: training_split[i][&quot;image&quot;], &quot;distance&quot;: distances[0][j]} for j, i in enumerate(indices[0]) ]

æˆ‘è¯•äº†å›¾ç‰‡æœç´ å›¾ç‰‡ç»“æœå‘ç°å¯ä»¥å°±æŠŠé‚£æ®µä»£ç æ‹¿è¿‡æ¥å‘ç°æ–‡æœ¬æœç´ å›¾ç‰‡äº§å“å¯¹äº†
 results = [  
      {&quot;image&quot;: training_split[i.item()][&quot;image&quot;], &quot;distance&quot;: distances[0][j]} 
        for j, i in enumerate(indices[0])
    ]
è¿™æ®µä»£ç å°±å¯ä»¥äº†</p>2023-11-21</li><br/><li><span>å°ç†æƒ³ã€‚</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>æ–‡æœ¬å‘é‡æœç´ å•†å“æ—¶
 results = [  
      {&quot;image&quot;: training_split[i][&quot;image&quot;], &quot;distance&quot;: distances[0][int(j)]} 
        for j, i in enumerate(indices[0])
    ]
è¿™æ®µä»£ç æŠ¥é”™ï¼š
TypeError: Wrong key type: &#39;1461&#39; of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;. Expected one of int, slice, range, str or Iterable.
è¯·é—®å¤§å®¶æœ‰ä»€ä¹ˆæ–¹æ¡ˆå—ï¼Œæœäº†gptä¹Ÿæ²¡æœ‰ç»™è§£å†³æ–¹æ¡ˆ
</p>2023-11-21</li><br/><li><span>Tang</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>å¾è€å¸ˆä½ å¥½ï¼Œæˆ‘æµ‹è¯•äº†ä¸‹çŒ«ç‹—å›¾ç‰‡çš„ç›®æ ‡æ£€æµ‹ï¼Œç»“æœçŒ«å’Œç‹—éƒ½æ£€æµ‹æˆäº†ç‹—</p>2023-11-08</li><br/><li><span>ç³–ç³–ä¸¸</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è¯·é—®è¦è¿è¡Œå¯ç”Ÿäº§ä½¿ç”¨çš„CLIPæ¨¡å‹ï¼Œå¤§æ¦‚éœ€è¦æ€ä¹ˆæ ·çš„æœºå™¨é…ç½®ï¼ˆä¸»è¦æŒ‡æ˜¾å¡ï¼‰å‘¢ï¼Ÿ
å¦å¤–ä¸çŸ¥é“å›¾ç‰‡æœç´¢çš„å“åº”æ—¶é—´å¤§æ¦‚åœ¨ä»€ä¹ˆé‡çº§ï¼Ÿ æ˜¯100msçº§åˆ«ï¼Œè¿˜æ˜¯10sçº§åˆ«çš„ï¼Ÿ</p>2023-10-20</li><br/><li><span>æ–°ç”°å°é£çŒª</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è€å¸ˆå¥½ï¼Œæœ‰æ²¡æœ‰å¯¹è®¡ç®—æœºéŸ³é¢‘&#47;éŸ³ä¹å¤„ç†ç›¸å…³çš„æ·±åº¦å­¦ä¹ èµ„æ–™çš„æ¨èå•Š</p>2023-07-28</li><br/><li><span>Santiago</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>æ¬¢åº¦äº”ä¸€</p>2023-04-28</li><br/>
</ul>