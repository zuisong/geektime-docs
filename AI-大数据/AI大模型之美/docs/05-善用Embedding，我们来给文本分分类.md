ä½ å¥½ï¼Œæˆ‘æ˜¯å¾æ–‡æµ©ã€‚

ä¸Šä¸€è®²é‡Œæˆ‘ä»¬çœ‹åˆ°å¤§æ¨¡å‹çš„ç¡®æœ‰æ•ˆã€‚åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æçš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡OpenAIçš„APIæ‹¿åˆ°çš„Embeddingï¼Œæ¯”T5-baseè¿™æ ·å•æœºå¯ä»¥è¿è¡Œçš„å°æ¨¡å‹ï¼Œæ•ˆæœè¿˜æ˜¯å¥½å¾ˆå¤šçš„ã€‚

ä¸è¿‡ï¼Œæˆ‘ä»¬ä¹‹å‰é€‰ç”¨çš„é—®é¢˜çš„ç¡®æœ‰ç‚¹å¤ªç®€å•äº†ã€‚æˆ‘ä»¬æŠŠ5ä¸ªä¸åŒçš„åˆ†æ•°åˆ†æˆäº†æ­£é¢ã€è´Ÿé¢å’Œä¸­æ€§ï¼Œè¿˜å»æ‰äº†ç›¸å¯¹éš¾ä»¥åˆ¤æ–­çš„â€œä¸­æ€§â€è¯„ä»·ï¼Œè¿™æ ·æˆ‘ä»¬åˆ¤æ–­çš„å‡†ç¡®ç‡é«˜çš„ç¡®æ˜¯æ¯”è¾ƒå¥½å®ç°çš„ã€‚ä½†å¦‚æœæˆ‘ä»¬æƒ³è¦å‡†ç¡®åœ°é¢„æµ‹å‡ºå…·ä½“çš„åˆ†æ•°å‘¢ï¼Ÿ

## åˆ©ç”¨Embeddingï¼Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹

æœ€ç®€å•çš„åŠæ³•å°±æ˜¯åˆ©ç”¨æˆ‘ä»¬æ‹¿åˆ°çš„æ–‡æœ¬Embeddingçš„å‘é‡ã€‚è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬ä¸ç›´æ¥ç”¨å‘é‡ä¹‹é—´çš„è·ç¦»ï¼Œè€Œæ˜¯ä½¿ç”¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ çš„æ–¹æ³•æ¥è¿›è¡Œåˆ†ç±»ã€‚æ¯•ç«Ÿï¼Œå¦‚æœåªæ˜¯ç”¨å‘é‡ä¹‹é—´çš„è·ç¦»ä½œä¸ºè¡¡é‡æ ‡å‡†ï¼Œå°±æ²¡åŠæ³•æœ€å¤§åŒ–åœ°åˆ©ç”¨å·²ç»æ ‡æ³¨å¥½çš„åˆ†æ•°ä¿¡æ¯äº†ã€‚

äº‹å®ä¸Šï¼ŒOpenAIåœ¨è‡ªå·±çš„å®˜æ–¹æ•™ç¨‹é‡Œä¹Ÿç›´æ¥ç»™å‡ºäº†è¿™æ ·ä¸€ä¸ªä¾‹å­ã€‚æˆ‘åœ¨è¿™é‡Œä¹Ÿæ”¾ä¸Šäº†ç›¸åº”çš„GitHubçš„ä»£ç [é“¾æ¥](https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb)ï¼Œä½ å¯ä»¥å»çœ‹ä¸€ä¸‹ã€‚ä¸è¿‡ï¼Œä¸ºäº†é¿å…OpenAIç‹å©†å–ç“œè‡ªå–è‡ªå¤¸ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›èƒ½å’Œå…¶ä»–äººç”¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹å¼å¾—åˆ°çš„ç»“æœåšä¸ªæ¯”è¾ƒã€‚

å› æ­¤æˆ‘é‡æ–°æ‰¾äº†ä¸€ä¸ªä¸­æ–‡çš„æ•°æ®é›†æ¥è¯•ä¸€è¯•ã€‚è¿™ä¸ªæ•°æ®é›†æ˜¯åœ¨ä¸­æ–‡äº’è”ç½‘ä¸Šæ¯”è¾ƒå®¹æ˜“æ‰¾åˆ°çš„ä¸€ä»½ä»Šæ—¥å¤´æ¡çš„æ–°é—»æ ‡é¢˜å’Œæ–°é—»å…³é”®è¯ï¼Œåœ¨GitHubä¸Šå¯ä»¥ç›´æ¥æ‰¾åˆ°æ•°æ®ï¼Œæˆ‘æŠŠ[é“¾æ¥](https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset)ä¹Ÿæ”¾åœ¨è¿™é‡Œã€‚ç”¨è¿™ä¸ªæ•°æ®é›†çš„å¥½å¤„æ˜¯ï¼Œæœ‰äººåŒæ­¥æ”¾å‡ºäº†é¢„æµ‹çš„å®éªŒæ•ˆæœã€‚æˆ‘ä»¬å¯ä»¥æ‹¿è‡ªå·±è®­ç»ƒçš„ç»“æœå’Œä»–åšä¸ªå¯¹æ¯”ã€‚

## æ•°æ®å¤„ç†ï¼Œå°å‘ä¹Ÿä¸å°‘

åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬è¦å…ˆè·å–æ¯ä¸€ä¸ªæ–°é—»æ ‡é¢˜çš„Embeddingã€‚æˆ‘ä»¬é€šè¿‡Pandasè¿™ä¸ªPythonæ•°æ®å¤„ç†åº“ï¼ŒæŠŠå¯¹åº”çš„æ–‡æœ¬åŠ è½½åˆ°å†…å­˜é‡Œã€‚æ¥ç€å»è°ƒç”¨ä¹‹å‰æˆ‘ä»¬ä½¿ç”¨è¿‡çš„OpenAIçš„Embeddingæ¥å£ï¼Œç„¶åæŠŠè¿”å›ç»“æœä¸€å¹¶å­˜ä¸‹æ¥å°±å¥½äº†ã€‚è¿™ä¸ªå¬èµ·æ¥éå¸¸ç®€å•ç›´æ¥ï¼Œæˆ‘ä¹ŸæŠŠå¯¹åº”çš„ä»£ç å…ˆæ”¾åœ¨ä¸‹é¢ï¼Œä¸è¿‡ä½ å…ˆåˆ«ç€æ€¥è¿è¡Œã€‚

æ³¨ï¼šå› ä¸ºåé¢çš„ä»£ç å¯èƒ½ä¼šè€—è´¹æ¯”è¾ƒå¤šçš„Tokenæ•°é‡ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯å…è´¹çš„5ç¾å…ƒé¢åº¦çš„è¯ï¼Œå¯ä»¥ç›´æ¥å»æ‹¿æˆ‘æ”¾åœ¨Githubé‡Œçš„[æ•°æ®æ–‡ä»¶](https://github.com/xuwenhao/geektime-ai-course)ï¼Œç”¨æˆ‘å·²ç»å¤„ç†å¥½çš„æ•°æ®ã€‚

```python
import pandas as pd
import tiktoken
import openai
import os

from openai.embeddings_utils import get_embedding, get_embeddings

openai.api_key = os.environ.get("OPENAI_API_KEY")

# embedding model parameters
embedding_model = "text-embedding-ada-002"
embedding_encoding = "cl100k_base"  # this the encoding for text-embedding-ada-002
max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191

# import data/toutiao_cat_data.txt as a pandas dataframe
df = pd.read_csv('data/toutiao_cat_data.txt', sep='_!_', names=['id', 'code', 'category', 'title', 'keywords'])
df = df.fillna("")
df["combined"] = (
    "æ ‡é¢˜: " + df.title.str.strip() + "; å…³é”®å­—: " + df.keywords.str.strip()
)

print("Lines of text before filtering: ", len(df))

encoding = tiktoken.get_encoding(embedding_encoding)
# omit reviews that are too long to embed
df["n_tokens"] = df.combined.apply(lambda x: len(encoding.encode(x)))
df = df[df.n_tokens <= max_tokens]

print("Lines of text after filtering: ", len(df))
```

æ³¨ï¼šè¿™ä¸ªæ˜¯åŠ è½½æ•°æ®å¹¶åšä¸€äº›ç®€å•é¢„å¤„ç†çš„ä»£ç ï¼Œä½ å¯ä»¥ç›´æ¥è¿è¡Œã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦å’Œå‰å‡ è®²çš„ä»£ç ä¸€æ ·ï¼Œå®šä¹‰ä¸€ä¸ª get\_embedding çš„å‡½æ•°ï¼Œæ–¹ä¾¿åé¢è°ƒç”¨ã€‚è¿™ä¸ªå‡½æ•°åŸæœ¬åœ¨æ—©æœŸçš„openaiåº“é‡Œæ˜¯ç›´æ¥æä¾›çš„ï¼Œä½†æ˜¯éšç€APIçš„ä¸æ–­æ›´æ–°ï¼Œè¿™ä¸ªåº“å·²ç»è¢«ç§»é™¤äº†ï¼Œä¸è¿‡ä»£ç éå¸¸ç®€å•ï¼Œæˆ‘ä»¬è‡ªå·±æ¥å®šä¹‰ä¸€ä¸‹å°±å¥½ã€‚

```
from openai import OpenAI
import os

client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

EMBEDDING_MODEL = "text-embedding-ada-002"

def get_embedding(text, model=EMBEDDING_MODEL):
   text = text.replace("\n", " ")
   return client.embeddings.create(input = [text], model=model).data[0].embedding
```

```python
# randomly sample 1k rows
df_1k = df.sample(1000, random_state=42)

df_1k["embedding"] = df_1k.combined.apply(lambda x : get_embedding(x, engine=embedding_model))
df_1k.to_csv("data/toutiao_cat_data_10k_with_embeddings.csv", index=False)
```

æ³¨ï¼šè¿™ä¸ªæ˜¯ä¸€æ¡æ¡æ•°æ®è¯·æ±‚OpenAIçš„APIè·å–Embeddingçš„ä»£ç ï¼Œä½†æ˜¯ä½ åœ¨è¿è¡Œä¸­ä¼šé‡åˆ°æŠ¥é”™ã€‚

ç›´æ¥è¿è¡Œè¿™ä¸ªä»£ç ï¼Œä½ å¤šåŠä¼šé‡åˆ°ä¸€ä¸ªæŠ¥é”™ï¼Œå› ä¸ºåœ¨è¿™ä¸ªæ•°æ®å¤„ç†è¿‡ç¨‹ä¸­ä¹Ÿæ˜¯æœ‰å‡ ä¸ªå‘çš„ã€‚

ç¬¬ä¸€ä¸ªå‘æ˜¯ **OpenAIæä¾›çš„æ¥å£é™åˆ¶äº†æ¯æ¡æ•°æ®çš„é•¿åº¦**ã€‚æˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„ text-embedding-ada-002 çš„æ¨¡å‹ï¼Œæ”¯æŒçš„é•¿åº¦æ˜¯æ¯æ¡è®°å½•8191ä¸ªTokenã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨å®é™…å‘é€è¯·æ±‚å‰ï¼Œéœ€è¦è®¡ç®—ä¸€ä¸‹æ¯æ¡è®°å½•æœ‰å¤šå°‘Tokenï¼Œè¶…è¿‡8000ä¸ªçš„éœ€è¦è¿‡æ»¤æ‰ã€‚ä¸è¿‡ï¼Œåœ¨æˆ‘ä»¬è¿™ä¸ªæ•°æ®é›†é‡Œï¼Œåªæœ‰æ–°é—»çš„æ ‡é¢˜ï¼Œæ‰€ä»¥ä¸ä¼šè¶…è¿‡è¿™ä¸ªé•¿åº¦ã€‚ä½†æ˜¯ä½ åœ¨ä½¿ç”¨å…¶ä»–æ•°æ®é›†çš„æ—¶å€™ï¼Œå¯èƒ½å°±éœ€è¦è¿‡æ»¤ä¸‹æ•°æ®ï¼Œæˆ–è€…é‡‡ç”¨æˆªæ–­çš„æ–¹æ³•ï¼Œåªç”¨æ–‡æœ¬æœ€å8000ä¸ªTokenã€‚

æˆ‘ä»¬åœ¨è¿™é‡Œï¼Œè°ƒç”¨äº†Tiktokenè¿™ä¸ªåº“ï¼Œä½¿ç”¨äº† cl100k\_base è¿™ç§ç¼–ç æ–¹å¼ï¼Œè¿™ç§ç¼–ç æ–¹å¼å’Œ text-embedding-ada-002 æ¨¡å‹æ˜¯ä¸€è‡´çš„ã€‚å¦‚æœé€‰é”™äº†ç¼–ç æ–¹å¼ï¼Œä½ è®¡ç®—å‡ºæ¥çš„Tokenæ•°é‡å¯èƒ½å’ŒOpenAIçš„ä¸ä¸€æ ·ã€‚

ç¬¬äºŒä¸ªå‘æ˜¯ï¼Œå¦‚æœä½ ç›´æ¥ä¸€æ¡æ¡è°ƒç”¨OpenAIçš„APIï¼Œå¾ˆå¿«å°±ä¼šé‡åˆ°æŠ¥é”™ã€‚è¿™æ˜¯å› ä¸º **OpenAIå¯¹APIçš„è°ƒç”¨è¿›è¡Œäº†é™é€Ÿ**ï¼ˆRate Limitï¼‰ã€‚å¦‚æœä½ è¿‡äºé¢‘ç¹åœ°è°ƒç”¨ï¼Œå°±ä¼šé‡åˆ°é™é€Ÿçš„æŠ¥é”™ã€‚è€Œå¦‚æœä½ åœ¨æŠ¥é”™ä¹‹åç»§ç»­æŒç»­è°ƒç”¨ï¼Œé™é€Ÿçš„æ—¶é—´è¿˜ä¼šè¢«å»¶é•¿ã€‚é‚£æ€ä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿæˆ‘ä¹ æƒ¯é€‰ç”¨ backoff è¿™ä¸ªPythonåº“ï¼Œåœ¨è°ƒç”¨çš„æ—¶å€™å¦‚æœé‡åˆ°æŠ¥é”™äº†ï¼Œå°±ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œå¦‚æœè¿ç»­æŠ¥é”™ï¼Œå°±æ‹‰é•¿ç­‰å¾…æ—¶é—´ã€‚é€šè¿‡backoffæ”¹é€ çš„ä»£ç æˆ‘æ”¾åœ¨äº†ä¸‹é¢ï¼Œä¸è¿‡è¿™è¿˜æ²¡æœ‰å½»åº•è§£å†³é—®é¢˜ã€‚

```python
conda install backoff
```

ä½ éœ€è¦å…ˆå®‰è£…ä¸€ä¸‹backoffåº“ã€‚

```python
import backoff

@backoff.on_exception(backoff.expo, openai.error.RateLimitError)
def get_embedding_with_backoff(**kwargs):
    return get_embedding(**kwargs)

# randomly sample 10k rows
df_10k = df.sample(10000, random_state=42)

df_10k["embedding"] = df_10k.combined.apply(lambda x : get_embedding_with_backoff(text=x, engine=embedding_model))
df_10k.to_csv("data/toutiao_cat_data_10k_with_embeddings.csv", index=False)
```

é€šè¿‡backoffåº“ï¼Œæˆ‘ä»¬æŒ‡å®šäº†åœ¨é‡åˆ°RateLimitErrorçš„æ—¶å€™ï¼ŒæŒ‰ç…§æŒ‡æ•°çº§åˆ«å¢åŠ ç­‰å¾…æ—¶é—´ã€‚

å¦‚æœä½ ç›´æ¥è¿è¡Œä¸Šé¢é‚£ä¸ªä»£ç ï¼Œå¤§çº¦éœ€è¦2ä¸ªå°æ—¶æ‰èƒ½å¤„ç†å®Œ1ä¸‡æ¡æ•°æ®ã€‚æˆ‘ä»¬çš„æ•°æ®é›†é‡Œæœ‰38ä¸‡æ¡æ•°æ®ï¼ŒçœŸè¦è¿™ä¹ˆå¹²ï¼Œéœ€è¦3å¤©3å¤œæ‰èƒ½æŠŠè®­ç»ƒæ•°æ®å¤„ç†å®Œï¼Œè¿™æ ·æ˜¾ç„¶ä¸æ€ä¹ˆå®ç”¨ã€‚è¿™ä¹ˆæ…¢çš„åŸå› æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯é™é€Ÿï¼Œbackoffåªæ˜¯è®©æˆ‘ä»¬çš„è°ƒç”¨ä¸ä¼šå› ä¸ºå¤±è´¥è€Œç»ˆæ­¢ï¼Œä½†æ˜¯æˆ‘è¿˜æ˜¯å—åˆ°äº†æ¯åˆ†é’ŸAPIè°ƒç”¨æ¬¡æ•°çš„é™åˆ¶ã€‚ç¬¬äºŒä¸ªæ˜¯å»¶æ—¶ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯æŒ‰ç…§é¡ºåºä¸€ä¸ªä¸ªè°ƒç”¨Embeddingæ¥å£ï¼Œæ¯ä¸€æ¬¡è°ƒç”¨éƒ½è¦ç­‰å‰ä¸€æ¬¡è°ƒç”¨ç»“æŸåæ‰ä¼šå‘èµ·è¯·æ±‚ï¼Œè€Œä¸æ˜¯å¤šæ¡æ•°æ®å¹¶è¡Œè¯·æ±‚ï¼Œè¿™æ›´è¿›ä¸€æ­¥**æ‹–é•¿äº†å¤„ç†æ•°æ®æ‰€éœ€è¦çš„æ—¶é—´**ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/07/a5/07e5bf0bdfce40a3f1b5a89cb43010a5.png?wh=1589x666)

æ³¨ï¼šä½ å¯ä»¥ç‚¹å¼€è¿™ä¸ª[é“¾æ¥](https://platform.openai.com/docs/guides/rate-limits/overview)ï¼Œçœ‹çœ‹ç›®å‰OpenAIå¯¹ä¸åŒæ¨¡å‹çš„é™é€Ÿã€‚

è¦è§£å†³è¿™ä¸ªé—®é¢˜ä¹Ÿä¸å›°éš¾ï¼ŒOpenAIæ˜¯æ”¯æŒbatchè°ƒç”¨æ¥å£çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä½ å¯ä»¥åœ¨ä¸€ä¸ªè¯·æ±‚é‡Œä¸€æ¬¡æ‰¹é‡å¤„ç†å¾ˆå¤šä¸ªè¯·æ±‚ã€‚æˆ‘ä»¬æŠŠ1000æ¡è®°å½•æ‰“åŒ…åœ¨ä¸€èµ·å¤„ç†ï¼Œé€Ÿåº¦å°±ä¼šå¿«å¾ˆå¤šã€‚æˆ‘æŠŠå¯¹åº”çš„ä»£ç æ”¾åœ¨ä¸‹é¢ï¼Œä½ å¯ä»¥è¯•ç€æ‰§è¡Œä¸€ä¸‹ï¼Œå¤„ç†è¿™38ä¸‡å¤šæ¡çš„æ•°æ®ï¼Œä¹Ÿå°±ä¸ªæŠŠå°æ—¶ã€‚ä¸è¿‡ï¼Œä½ ä¹Ÿä¸èƒ½ä¸€æ¬¡æ€§æ‰“åŒ…å¤ªå¤šæ¡è®°å½•ï¼Œå› ä¸ºOpenAIçš„é™é€Ÿä¸ä»…ä»…æ˜¯é’ˆå¯¹è¯·æ±‚æ•°çš„ï¼Œä¹Ÿ**é™åˆ¶ä½ æ¯åˆ†é’Ÿå¯ä»¥å¤„ç†çš„ Token æ•°é‡**ï¼Œå…·ä½“ä¸€æ¬¡æ‰“åŒ…å‡ æ¡ï¼Œä½ å¯ä»¥æ ¹æ®æ¯æ¡æ•°æ®åŒ…å«çš„Tokenæ•°è‡ªå·±æµ‹ç®—ä¸€ä¸‹ã€‚

```python
import backoff
from openai.embeddings_utils import get_embeddings

batch_size = 1000

@backoff.on_exception(backoff.expo, openai.error.RateLimitError)
def get_embeddings_with_backoff(prompts, engine):
    embeddings = []
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i+batch_size]
        embeddings += get_embeddings(list_of_text=batch, engine=engine)
    return embeddings

# randomly sample 10k rows
df_all = df
# group prompts into batches of 100
prompts = df_all.combined.tolist()
prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]

embeddings = []
for batch in prompt_batches:
    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)
    embeddings += batch_embeddings

df_all["embedding"] = embeddings
df_all.to_parquet("data/toutiao_cat_data_all_with_embeddings.parquet", index=True)
```

æœ€åä¸€ä¸ªä½ éœ€è¦æ³¨æ„çš„ç‚¹æ˜¯ï¼Œå¯¹äºè¿™æ ·çš„å¤§æ•°æ®é›†ï¼Œ**ä¸è¦å­˜å‚¨æˆCSVæ ¼å¼**ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬è·å–åˆ°çš„Embeddingæ•°æ®ï¼Œæ˜¯å¾ˆå¤šæµ®ç‚¹æ•°ï¼Œå­˜å‚¨æˆCSVæ ¼å¼ä¼šæŠŠæœ¬æ¥åªéœ€è¦4ä¸ªå­—èŠ‚çš„æµ®ç‚¹æ•°ï¼Œéƒ½ç”¨å­—ç¬¦ä¸²çš„å½¢å¼å­˜å‚¨ä¸‹æ¥ï¼Œä¼šæµªè´¹å¥½å‡ å€çš„ç©ºé—´ï¼Œå†™å…¥çš„é€Ÿåº¦ä¹Ÿå¾ˆæ…¢ã€‚æˆ‘åœ¨è¿™é‡Œ**é‡‡ç”¨äº†parquetè¿™ä¸ªåºåˆ—åŒ–çš„æ ¼å¼**ï¼Œæ•´ä¸ªå­˜å‚¨çš„è¿‡ç¨‹åªéœ€è¦1åˆ†é’Ÿã€‚

## è®­ç»ƒæ¨¡å‹ï¼Œçœ‹çœ‹æ•ˆæœæ€ä¹ˆæ ·

æ•°æ®å¤„ç†å®Œäº†ï¼Œæˆ‘ä»¬å°±ä¸å¦¨è¯•ä¸€è¯•æ¨¡å‹è®­ç»ƒã€‚å¦‚æœä½ æ‹…å¿ƒæµªè´¹å¤ªå¤šçš„APIè°ƒç”¨ï¼Œæˆ‘æŠŠæˆ‘å¤„ç†å¥½çš„æ•°æ®é›†ï¼Œæ”¾åœ¨äº†æˆ‘çš„ [GitHub](https://github.com/xuwenhao/geektime-ai-course) ä¸Šï¼Œæˆ‘æŠŠé“¾æ¥ä¹Ÿæ”¾åœ¨äº†è¿™é‡Œï¼Œä½ å¯ä»¥ç›´æ¥ä¸‹è½½ã€ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†ã€‚

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

training_data = pd.read_parquet("data/toutiao_cat_data_all_with_embeddings.parquet")
training_data.head()

df =  training_data.sample(50000, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(
    list(df.embedding.values), df.category, test_size=0.2, random_state=42
)

clf = RandomForestClassifier(n_estimators=300)
clf.fit(X_train, y_train)
preds = clf.predict(X_test)
probas = clf.predict_proba(X_test)

report = classification_report(y_test, preds)
print(report)
```

æ¨¡å‹è®­ç»ƒçš„ä»£ç ä¹Ÿéå¸¸ç®€å•ï¼Œè€ƒè™‘åˆ°è¿è¡Œæ—¶é—´çš„å› ç´ ï¼Œæˆ‘è¿™é‡Œç›´æ¥éšæœºé€‰å–äº†é‡Œé¢çš„5ä¸‡æ¡æ•°æ®ï¼Œ4ä¸‡æ¡ä½œä¸ºè®­ç»ƒé›†ï¼Œ1ä¸‡æ¡ä½œä¸ºæµ‹è¯•é›†ã€‚ç„¶åé€šè¿‡æœ€å¸¸ç”¨çš„scikit-learnè¿™ä¸ªæœºå™¨å­¦ä¹ å·¥å…·åŒ…é‡Œé¢çš„éšæœºæ£®æ—ï¼ˆRandomForestï¼‰ç®—æ³•ï¼Œåšäº†ä¸€æ¬¡è®­ç»ƒå’Œæµ‹è¯•ã€‚åœ¨æˆ‘çš„ç”µè„‘ä¸Šï¼Œå¤§æ¦‚10åˆ†é’Ÿå¯ä»¥è·‘å®Œï¼Œæ•´ä½“çš„å‡†ç¡®ç‡å¯ä»¥è¾¾åˆ°84%ã€‚

```python
                    precision    recall  f1-score   support
  news_agriculture       0.83      0.85      0.84       495
          news_car       0.88      0.94      0.91       895
      news_culture       0.86      0.76      0.81       741
          news_edu       0.86      0.89      0.87       708
news_entertainment       0.71      0.92      0.80      1051
      news_finance       0.81      0.76      0.78       735
         news_game       0.91      0.82      0.86       742
        news_house       0.91      0.86      0.89       450
     news_military       0.89      0.82      0.85       688
       news_sports       0.90      0.92      0.91       968
        news_story       0.95      0.46      0.62       197
         news_tech       0.82      0.86      0.84      1052
       news_travel       0.80      0.77      0.78       599
        news_world       0.83      0.73      0.78       671
             stock       0.00      0.00      0.00         8
          accuracy                           0.84     10000
         macro avg       0.80      0.76      0.77     10000
      weighted avg       0.84      0.84      0.84     10000
```

éšæœºæ£®æ—è¿™ä¸ªç®—æ³•ï¼Œè™½ç„¶æ•ˆæœä¸é”™ï¼Œä½†æ˜¯è·‘èµ·æ¥æœ‰äº›æ…¢ã€‚æˆ‘ä»¬æ¥ä¸‹æ¥ç”¨ä¸ªæ›´ç®€å•çš„é€»è¾‘å›å½’ï¼ˆLogisticRegressionï¼‰ç®—æ³•ï¼Œä½†æˆ‘ä»¬è¿™æ¬¡è¦è·‘åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šã€‚ä¸€æ ·çš„ï¼Œæˆ‘ä»¬æ‹¿80%ä½œä¸ºè®­ç»ƒï¼Œ20%ä½œä¸ºæµ‹è¯•ã€‚è¿™ä¸€æ¬¡ï¼Œè™½ç„¶æ•°æ®é‡æ˜¯åˆšæ‰4ä¸‡æ¡æ•°æ®çš„å¥½å‡ å€ï¼Œä½†æ˜¯æ—¶é—´ä¸Šå´åªè¦3ï½4åˆ†é’Ÿï¼Œè€Œæœ€ç»ˆçš„å‡†ç¡®ç‡ä¹Ÿèƒ½è¾¾åˆ°86%ã€‚

```python
from sklearn.linear_model import LogisticRegression

df =  training_data

X_train, X_test, y_train, y_test = train_test_split(
    list(df.embedding.values), df.category, test_size=0.2, random_state=42
)

clf = LogisticRegression()
clf.fit(X_train, y_train)
preds = clf.predict(X_test)
probas = clf.predict_proba(X_test)

report = classification_report(y_test, preds)
print(report)
```

è¾“å‡ºç»“æœï¼š

```python
                    precision    recall  f1-score   support
  news_agriculture       0.86      0.88      0.87      3908
          news_car       0.92      0.92      0.92      7101
      news_culture       0.83      0.85      0.84      5719
          news_edu       0.89      0.89      0.89      5376
news_entertainment       0.86      0.88      0.87      7908
      news_finance       0.81      0.79      0.80      5409
         news_game       0.91      0.88      0.89      5899
        news_house       0.91      0.91      0.91      3463
     news_military       0.86      0.82      0.84      4976
       news_sports       0.93      0.93      0.93      7611
        news_story       0.83      0.82      0.83      1308
         news_tech       0.84      0.86      0.85      8168
       news_travel       0.80      0.80      0.80      4252
        news_world       0.79      0.81      0.80      5370
             stock       0.00      0.00      0.00        70
          accuracy                           0.86     76538
         macro avg       0.80      0.80      0.80     76538
      weighted avg       0.86      0.86      0.86     76538
```

æ³¨ï¼šä¸‹è½½çš„æ•°æ®é›†æ‰€åœ¨çš„æµ‹è¯•ç»“æœå¯ä»¥çœ‹[è¿™é‡Œ](https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset)ã€‚

è¿™ä¸ªç»“æœå·²ç»æ¯”æˆ‘ä»¬ä¸‹è½½æ•°æ®é›†çš„GitHubé¡µé¢é‡Œçœ‹åˆ°çš„æ•ˆæœå¥½äº†ï¼Œé‚£ä¸ªçš„å‡†ç¡®ç‡åªæœ‰85%ã€‚

å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡OpenAIçš„APIè·å–åˆ°Embeddingï¼Œç„¶åé€šè¿‡ä¸€äº›ç®€å•çš„çº¿æ€§æ¨¡å‹ï¼Œæˆ‘ä»¬å°±èƒ½è·å¾—å¾ˆå¥½çš„åˆ†ç±»æ•ˆæœã€‚æˆ‘ä»¬æ—¢ä¸éœ€è¦æå‰å‚¨å¤‡å¾ˆå¤šè‡ªç„¶è¯­è¨€å¤„ç†çš„çŸ¥è¯†ï¼Œå¯¹æ•°æ®è¿›è¡Œå¤§é‡çš„åˆ†æå’Œæ¸…ç†ï¼›ä¹Ÿä¸éœ€è¦æä¸€å—æ˜‚è´µçš„æ˜¾å¡ï¼Œå»ä½¿ç”¨ä»€ä¹ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚åªè¦1ï½2ä¸ªå°æ—¶ï¼Œæˆ‘ä»¬å°±èƒ½åœ¨ä¸€ä¸ªå‡ åä¸‡æ¡æ–‡æœ¬çš„æ•°æ®é›†ä¸Šè®­ç»ƒå‡ºä¸€ä¸ªéå¸¸ä¸é”™çš„åˆ†ç±»æ¨¡å‹ã€‚

## ç†è§£æŒ‡æ ‡ï¼Œå­¦ä¸€ç‚¹æœºå™¨å­¦ä¹ å°çŸ¥è¯†

åˆšåˆšæˆ‘è¯´äº†ï¼Œå°±ç®—ä½ æ²¡æœ‰æœºå™¨å­¦ä¹ çš„ç›¸å…³çŸ¥è¯†ï¼Œä¹Ÿæ²¡æœ‰å…³ç³»ï¼Œè¿™é‡Œæˆ‘æ¥ç»™ä½ è¡¥è¡¥è¯¾ã€‚ç†è§£ä¸€ä¸‹ä¸Šé¢æ¨¡å‹è¾“å‡ºçš„æŠ¥å‘Šæ˜¯ä»€ä¹ˆæ„æ€ã€‚æŠ¥å‘Šçš„æ¯ä¸€è¡Œéƒ½æœ‰å››ä¸ªæŒ‡æ ‡ï¼Œåˆ†åˆ«æ˜¯å‡†ç¡®ç‡ï¼ˆPrecisionï¼‰ã€å¬å›ç‡ï¼ˆRecallï¼‰ã€F1åˆ†æ•°ï¼Œä»¥åŠæ”¯æŒæ ·æœ¬é‡ï¼ˆSupportï¼‰ã€‚æˆ‘è¿˜æ˜¯ç”¨ä»Šæ—¥å¤´æ¡çš„æ–°é—»æ ‡é¢˜è¿™ä¸ªæ•°æ®é›†æ¥è§£é‡Šè¿™äº›æ¦‚å¿µã€‚

1. **å‡†ç¡®ç‡**ï¼Œä»£è¡¨æ¨¡å‹åˆ¤å®šå±äºè¿™ä¸ªåˆ†ç±»çš„æ ‡é¢˜é‡Œé¢åˆ¤æ–­æ­£ç¡®çš„æœ‰å¤šå°‘ï¼Œæœ‰å¤šå°‘çœŸçš„æ˜¯å±äºè¿™ä¸ªåˆ†ç±»çš„ã€‚æ¯”å¦‚ï¼Œæ¨¡å‹åˆ¤æ–­é‡Œé¢æœ‰ 100ä¸ªéƒ½æ˜¯å†œä¸šæ–°é—»ï¼Œä½†æ˜¯è¿™100ä¸ªé‡Œé¢å…¶å®åªæœ‰83ä¸ªæ˜¯å†œä¸šæ–°é—»ï¼Œé‚£ä¹ˆå‡†ç¡®ç‡å°±æ˜¯0.83ã€‚å‡†ç¡®ç‡è‡ªç„¶æ˜¯è¶Šé«˜è¶Šå¥½ï¼Œä½†æ˜¯å¹¶ä¸æ˜¯å‡†ç¡®ç‡è¾¾åˆ°100%å°±ä»£è¡¨æ¨¡å‹å…¨å¯¹äº†ã€‚å› ä¸ºæ¨¡å‹å¯èƒ½ä¼šæ¼ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜è¦è€ƒè™‘å¬å›ç‡ã€‚
2. **å¬å›ç‡**ï¼Œä»£è¡¨æ¨¡å‹åˆ¤å®šå±äºè¿™ä¸ªåˆ†ç±»çš„æ ‡é¢˜å å®é™…è¿™ä¸ªåˆ†ç±»ä¸‹æ‰€æœ‰æ ‡é¢˜çš„æ¯”ä¾‹ï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰æ¼æ‰çš„æ¯”ä¾‹ã€‚æ¯”å¦‚ï¼Œæ¨¡å‹åˆ¤æ–­100ä¸ªéƒ½æ˜¯å†œä¸šæ–°é—»ï¼Œè¿™100ä¸ªçš„ç¡®éƒ½æ˜¯å†œä¸šæ–°é—»ã€‚å‡†ç¡®ç‡å·²ç»100%äº†ã€‚ä½†æ˜¯ï¼Œå®é™…æˆ‘ä»¬ä¸€å…±æœ‰200æ¡å†œä¸šæ–°é—»ã€‚é‚£ä¹ˆæœ‰100æ¡å…¶å®è¢«æ”¾åˆ°åˆ«çš„ç±»ç›®é‡Œé¢å»äº†ã€‚é‚£ä¹ˆåœ¨å†œä¸šæ–°é—»è¿™ä¸ªç±»ç›®ï¼Œæˆ‘ä»¬çš„å¬å›ç‡ï¼Œå°±åªæœ‰ 100/200 = 50%ã€‚
3. æ‰€ä»¥æ¨¡å‹æ•ˆæœçš„å¥½åï¼Œæ—¢è¦è€ƒè™‘å‡†ç¡®ç‡ï¼Œåˆè¦è€ƒè™‘å¬å›ç‡ï¼Œç»¼åˆè€ƒè™‘è¿™ä¸¤é¡¹å¾—å‡ºçš„ç»“æœï¼Œå°±æ˜¯ **F1** **åˆ†æ•°**ï¼ˆF1 Scoreï¼‰ã€‚F1åˆ†æ•°ï¼Œæ˜¯å‡†ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°ï¼Œä¹Ÿå°±æ˜¯ F1 Score = 2/ (1/Precision + 1/Recall)ã€‚å½“å‡†ç¡®ç‡å’Œå¬å›ç‡éƒ½æ˜¯100%çš„æ—¶å€™ï¼ŒF1åˆ†æ•°ä¹Ÿæ˜¯1ã€‚å¦‚æœå‡†ç¡®ç‡æ˜¯100%ï¼Œå¬å›ç‡æ˜¯80%ï¼Œé‚£ä¹ˆç®—ä¸‹æ¥F1åˆ†æ•°å°±æ˜¯0.88ã€‚F1åˆ†æ•°ä¹Ÿæ˜¯è¶Šé«˜è¶Šå¥½ã€‚
4. **æ”¯æŒçš„æ ·æœ¬é‡**ï¼Œæ˜¯æŒ‡æ•°æ®é‡Œé¢ï¼Œå®é™…æ˜¯è¿™ä¸ªåˆ†ç±»çš„æ•°æ®æ¡æ•°æœ‰å¤šå°‘ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ•°æ®æ¡æ•°è¶Šå¤šï¼Œè¿™ä¸ªåˆ†ç±»çš„è®­ç»ƒå°±ä¼šè¶Šå‡†ç¡®ã€‚

åˆ†ç±»æŠ¥å‘Šé‡Œä¸€ä¸ªç±»ç›®å ä¸€è¡Œï¼Œæ¯ä¸€è¡Œéƒ½åŒ…å«å¯¹åº”çš„è¿™å››ä¸ªæŒ‡æ ‡ï¼Œè€Œæœ€ä¸‹é¢è¿˜æœ‰ä¸‰è¡Œæ•°æ®ã€‚è¿™ä¸‰è¡Œæ•°æ®ï¼Œæ˜¯æ•´ä¸ªæ‹¿æ¥æµ‹è¯•çš„æ•°æ®é›†ï¼Œæ‰€ä»¥å¯¹åº”çš„æ”¯æŒæ ·æœ¬é‡éƒ½æ˜¯1ä¸‡ä¸ªã€‚

ç¬¬ä¸€è¡Œçš„accuracyï¼Œåªæœ‰ä¸€ä¸ªæŒ‡æ ‡ï¼Œè™½ç„¶å®ƒåœ¨F1 Scoreè¿™ä¸ªåˆ—é‡Œï¼Œä½†æ˜¯å¹¶ä¸æ˜¯F1åˆ†æ•°çš„æ„æ€ã€‚è€Œæ˜¯è¯´ï¼Œæ¨¡å‹æ€»å…±åˆ¤æ–­å¯¹çš„åˆ†ç±»/æ¨¡å‹æµ‹è¯•çš„æ ·æœ¬æ•°ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹çš„æ•´ä½“å‡†ç¡®ç‡ã€‚

ç¬¬äºŒè¡Œçš„macro averageï¼Œä¸­æ–‡åå«åšå®å¹³å‡ï¼Œå®å¹³å‡çš„ä¸‰ä¸ªæŒ‡æ ‡ï¼Œå°±æ˜¯æŠŠä¸Šé¢æ¯ä¸€ä¸ªåˆ†ç±»ç®—å‡ºæ¥çš„æŒ‡æ ‡åŠ åœ¨ä¸€èµ·å¹³å‡ä¸€ä¸‹ã€‚å®ƒä¸»è¦æ˜¯åœ¨æ•°æ®åˆ†ç±»ä¸å¤ªå¹³è¡¡çš„æ—¶å€™ï¼Œå¸®åŠ©æˆ‘ä»¬è¡¡é‡æ¨¡å‹æ•ˆæœæ€ä¹ˆæ ·ã€‚

æ¯”å¦‚ï¼Œæˆ‘ä»¬åšæƒ…æ„Ÿåˆ†æï¼Œå¯èƒ½90%éƒ½æ˜¯æ­£é¢æƒ…æ„Ÿï¼Œ10%æ˜¯è´Ÿé¢æƒ…æ„Ÿã€‚è¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬é¢„æµ‹æ­£é¢æƒ…æ„Ÿæ•ˆæœå¾ˆå¥½ï¼Œæ¯”å¦‚æœ‰90%çš„å‡†ç¡®ç‡ï¼Œä½†æ˜¯è´Ÿé¢æƒ…æ„Ÿé¢„æµ‹å¾ˆå·®ï¼Œåªæœ‰50%çš„å‡†ç¡®ç‡ã€‚å¦‚æœçœ‹æ•´ä½“æ•°æ®ï¼Œå…¶å®å‡†ç¡®ç‡è¿˜æ˜¯éå¸¸é«˜çš„ï¼Œæ¯•ç«Ÿè´Ÿé¢æƒ…æ„Ÿçš„ä¾‹å­å¾ˆå°‘ã€‚

ä½†æ˜¯æˆ‘ä»¬çš„ç›®æ ‡å¯èƒ½å°±æ˜¯æ‰¾åˆ°æœ‰è´Ÿé¢æƒ…ç»ªçš„å®¢æˆ·å’Œä»–ä»¬æ²Ÿé€šã€èµ”å¿ã€‚é‚£ä¹ˆæ•´ä½“å‡†ç¡®ç‡å¯¹æˆ‘ä»¬å°±æ²¡æœ‰ä»€ä¹ˆç”¨äº†ã€‚è€Œå®å¹³å‡ï¼Œä¼šæŠŠæ•´ä½“çš„å‡†ç¡®ç‡å˜æˆ (90%+50%)/2 = 70%ã€‚è¿™å°±ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é¢„æµ‹ç»“æœäº†ï¼Œæˆ‘ä»¬éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚å®å¹³å‡å¯¹äºæ•°æ®æ ·æœ¬ä¸å¤ªå¹³è¡¡ï¼Œæœ‰äº›ç±»ç›®æ ·æœ¬ç‰¹åˆ«å°‘ï¼Œæœ‰äº›ç‰¹åˆ«å¤šçš„åœºæ™¯ç‰¹åˆ«æœ‰ç”¨ã€‚

ç¬¬ä¸‰è¡Œçš„weighted averageï¼Œå°±æ˜¯åŠ æƒå¹³å‡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æŠŠæ¯ä¸€ä¸ªæŒ‡æ ‡ï¼ŒæŒ‰ç…§åˆ†ç±»é‡Œé¢æ”¯æŒçš„æ ·æœ¬é‡åŠ æƒï¼Œç®—å‡ºæ¥çš„ä¸€ä¸ªå€¼ã€‚æ— è®ºæ˜¯ Precisionã€Recall è¿˜æ˜¯ F1 Scoreéƒ½è¦è¿™ä¹ˆæŒ‰ç…§å„ä¸ªåˆ†ç±»åŠ æƒå¹³å‡ä¸€ä¸‹ã€‚

## å°ç»“

![](https://static001.geekbang.org/resource/image/94/c3/947d63ba603abfc137b904b7db1cf3c3.jpg?wh=2872x1940)

å¥½äº†ï¼Œä»Šå¤©çš„è¿™ä¸€è®²åˆ°è¿™é‡Œå°±ç»“æŸäº†ï¼Œæœ€åæˆ‘ä»¬æ¥å›é¡¾ä¸€ä¸‹ã€‚è¿™ä¸€è®²æˆ‘ä»¬å­¦ä¼šäº†ä¸¤ä»¶äº‹æƒ…ã€‚

ç¬¬ä¸€ä»¶ï¼Œæ˜¯æ€ä¹ˆåˆ©ç”¨OpenAIçš„APIæ¥è·å–æ–‡æœ¬çš„Embeddingã€‚è™½ç„¶æ¥å£ä¸å¤æ‚ï¼Œä½†æ˜¯æˆ‘ä»¬ä¹Ÿè¦è€ƒè™‘æ¨¡å‹èƒ½å¤Ÿæ¥å—çš„æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼ŒAPIæœ¬èº«çš„é™é€Ÿï¼Œä»¥åŠç½‘ç»œå»¶æ—¶å¸¦æ¥çš„é—®é¢˜ã€‚

æˆ‘ä»¬åˆ†åˆ«ç»™å‡ºäº†è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨Tiktokenè®¡ç®—æ ·æœ¬çš„Tokenæ•°é‡ï¼Œå¹¶è¿›è¡Œè¿‡æ»¤ï¼›åœ¨é‡åˆ°é™é€Ÿé—®é¢˜æ—¶é€šè¿‡backoffè¿›è¡ŒæŒ‡æ•°çº§åˆ«çš„æ—¶é—´ç­‰å¾…ï¼›é€šè¿‡ä¸€æ¬¡æ€§æ‰¹é‡è¯·æ±‚ä¸€æ‰¹æ•°æ®ï¼Œæœ€å¤§åŒ–æˆ‘ä»¬çš„ååé‡æ¥è§£å†³é—®é¢˜ï¼›å¯¹äºè¿”å›çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡parquetè¿™æ ·åºåˆ—åŒ–çš„æ–¹å¼ä¿å­˜æ•°æ®ï¼Œæ¥å‡å°‘æ•°æ®å°ºå¯¸ã€‚

ç¬¬äºŒä»¶ï¼Œæ˜¯å¦‚ä½•ç›´æ¥åˆ©ç”¨æ‹¿åˆ°çš„Embeddingï¼Œç®€å•è°ƒç”¨ä¸€ä¸‹scikit-learnï¼Œé€šè¿‡æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œè¿›è¡Œæ›´å‡†ç¡®çš„åˆ†ç±»ã€‚æˆ‘ä»¬æœ€ç»ˆæŠŠEmbeddingæ”¾åˆ°ä¸€ä¸ªç®€å•çš„é€»è¾‘å›å½’æ¨¡å‹é‡Œï¼Œå°±å–å¾—äº†å¾ˆä¸é”™çš„åˆ†ç±»æ•ˆæœã€‚ä½ å­¦ä¼šäº†å—ï¼Ÿ

## è¯¾åç»ƒä¹ 

è¿™ä¸€è®²é‡Œæˆ‘ä»¬å­¦ä¼šäº†åˆ©ç”¨OpenAIæ¥è·å–æ–‡æœ¬çš„Embeddingï¼Œç„¶åé€šè¿‡ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹å¼æ¥è¿›è¡Œè®­ç»ƒï¼Œå¹¶è¯„ä¼°è®­ç»ƒçš„ç»“æœã€‚

æˆ‘ä»¬ä¹‹å‰ç”¨è¿‡Amazon1000æ¡é£Ÿç‰©è¯„è®ºçš„æƒ…æ„Ÿåˆ†ææ•°æ®ï¼Œåœ¨é‚£ä¸ªæ•°æ®é›†é‡Œï¼Œæˆ‘ä»¬å…¶å®å·²ç»ä½¿ç”¨è¿‡è·å–åˆ°å¹¶ä¿å­˜ä¸‹æ¥çš„Embeddingæ•°æ®äº†ã€‚é‚£ä¹ˆï¼Œä½ èƒ½ä¸èƒ½è¯•ç€åœ¨å®Œæ•´çš„æ•°æ®é›†ä¸Šï¼Œè®­ç»ƒä¸€ä¸ªèƒ½æŠŠä»1åˆ†åˆ°5åˆ†çš„æ¯ä¸€ä¸ªçº§åˆ«éƒ½åŒºåˆ†å‡ºæ¥çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œçœ‹çœ‹æ•ˆæœæ€ä¹ˆæ ·ï¼Ÿ

æ•´ä¸ªåŸå§‹æ•°æ®é›†çš„ä¸‹è½½é“¾æ¥æˆ‘æ”¾åœ¨[è¿™é‡Œ](https://www.kaggle.com/snap/amazon-fine-food-reviews)äº†ï¼Œæ¬¢è¿ä½ æŠŠä½ æµ‹è¯•å‡ºæ¥çš„ç»“æœåˆ†äº«å‡ºæ¥ï¼Œçœ‹çœ‹å’Œå…¶ä»–äººæ¯”èµ·æ¥æ€ä¹ˆæ ·ã€‚å¦å¤–å¦‚æœä½ è§‰å¾—æœ‰æ”¶è·çš„è¯ï¼Œä¹Ÿæ¬¢è¿ä½ æŠŠè¿™èŠ‚è¯¾åˆ†äº«å‡ºå»ï¼Œè®©æ›´å¤šçš„äººäº†è§£ç”¨Embeddingç»™æ–‡æœ¬åˆ†ç±»çš„æ–¹æ³•ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>éº¦è€€é”‹</span> ğŸ‘ï¼ˆ32ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>åº”è¯¥è¿™ä¹ˆå»ç†è§£embeddingçš„ä½¿ç”¨ã€‚ä»¥å‰æˆ‘ä»¬åšæœºå™¨å­¦ä¹ çš„æ—¶å€™ï¼ˆæˆ–è€…ç›¸å¯¹äºæ·±åº¦å­¦ä¹ ä¹‹å‰çš„â€œæµ…å±‚å­¦ä¹ â€ï¼‰ï¼Œä¸ç®¡æ˜¯æœ‰ç›‘ç£è¿˜æ˜¯æ— ç›‘ç£ï¼Œä¸€èˆ¬æˆ‘ä»¬éœ€è¦åšfeature engineeringï¼Œä¹Ÿå°±æ˜¯éœ€è¦data scientistï¼Œæ ¹æ®ä¸šåŠ¡ã€ä¸“å®¶é¢†åŸŸï¼Œå¯¹æ•°æ®æå–æœ‰ç”¨çš„featureï¼›è€Œåœ¨NLPé¢†åŸŸï¼Œé‚£ä¹ˆå°±æ˜¯é€šè¿‡word2vecç­‰å„ç§æ–¹å¼æ¥æå–featureã€‚é€šè¿‡openaiçš„embeddingæ¥å£ï¼Œå®é™…ä¸Šå°±æ˜¯openaiå¸®æˆ‘ä»¬åšäº†feature engineeringè¿™ä¸€æ­¥ï¼Œå°†æ–‡æœ¬æ˜ å°„åˆ°ä¸€ä¸ªåˆé€‚çš„vector spaceï¼Œå¾—åˆ°çš„embeddingå…¶å®å°±æ˜¯æ–‡æœ¬çš„featureï¼Œæ‰€ä»¥å¯ä»¥åŸºäºè¿™ä¸ªembeddingå³feature Xæ¥åšä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ </p>2023-05-05</li><br/><li><span>æµ©ä»”æ˜¯ç¨‹åºå‘˜</span> ğŸ‘ï¼ˆ18ï¼‰ ğŸ’¬ï¼ˆ9ï¼‰<p>è€å¸ˆï¼Œä½ å¥½ï¼æ—¢ç„¶éƒ½è°ƒç”¨open aiçš„æ¥å£ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥è®©chatgptç›´æ¥è¿”å›åˆ†ç±»ç»“æœå‘¢</p>2023-03-28</li><br/><li><span>è‰¯è¾°ç¾æ™¯</span> ğŸ‘ï¼ˆ10ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>â€œè°ƒç”¨äº† Tiktoken è¿™ä¸ªåº“ï¼Œä½¿ç”¨äº† cl100k_base è¿™ç§ç¼–ç æ–¹å¼ï¼Œè¿™ç§ç¼–ç æ–¹å¼å’Œ text-embedding-ada-002 æ¨¡å‹æ˜¯ä¸€è‡´çš„ã€‚å¦‚æœé€‰é”™äº†ç¼–ç æ–¹å¼ï¼Œä½ è®¡ç®—å‡ºæ¥çš„ Token æ•°é‡å¯èƒ½å’Œ OpenAI çš„ä¸ä¸€æ ·ã€‚â€ è€å¸ˆï¼Œ é—®ä½ ä¸€ä¸ªå­¦ä¹ ä¸Šçš„é—®é¢˜ï¼Œ åƒè¿™ç§API æ–‡æ¡£é‡Œè¿™ä¹ˆç»†èŠ‚çš„ä¸œè¥¿ï¼Œä½ æ˜¯å¦‚ä½•è·å–è¿™äº›ä¿¡æ¯çš„</p>2023-04-07</li><br/><li><span>Toni</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>åœ¨æ‰§è¡Œè¿™æ®µä»£ç æ—¶ä¼šæŠ¥ParserWarning
df = pd.read_csv(&#39;data&#47;toutiao_cat&#47;toutiao_cat_data.txt&#39;, sep=&#39;_!_&#39;, names=[&#39;id&#39;, &#39;code&#39;, &#39;category&#39;, &#39;title&#39;, &#39;keywords&#39;])

ParserWarning: Falling back to the &#39;python&#39; engine because the &#39;c&#39; engine does not support regex separators (separators &gt; 1 char and different from &#39;\s+&#39; are interpreted as regex); you can avoid this warning by specifying engine=&#39;python&#39;.

åŸå› æ˜¯ &#39;c&#39; å¼•æ“ä¸æ”¯æŒåˆ†éš”ç¬¦sep=&#39;_!_&#39;è¡¨è¾¾å¼ï¼Œå¯æŒ‡å®š engine=&#39;python&#39; æ¥é¿å…æ­¤è­¦å‘Šã€‚
ä»£ç å¦‚ä¸‹
df = pd.read_csv(&#39;data&#47;toutiao_cat&#47;toutiao_cat_data.txt&#39;, engine= &#39;python&#39;ï¼Œ sep=&#39;_!_&#39;, names=[&#39;id&#39;, &#39;code&#39;, &#39;category&#39;, &#39;title&#39;, &#39;keywords&#39;])</p>2023-03-30</li><br/><li><span>æ€¡ä»”</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ4ï¼‰<p>githubä¸­çš„æ‰¾ä¸åˆ°é€”ä¸­åˆ—ä¸¾çš„æ•°æ®æ–‡ä»¶</p>2023-03-28</li><br/><li><span>lala</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç°åœ¨chatGPTæ˜¯é€šç”¨çš„é—®ç­”åº”ç”¨ã€‚è¯·é—®å¦‚ä½•æ ¹æ®ChatGPTç»“åˆå…¬å¸ä¸šåŠ¡çš„çŸ¥è¯†åº“å’Œå•†å“ä¿¡æ¯ï¼Œå¦‚ä½•æ‰“é€ æœåŠ¡å®¢æˆ·å’Œå†…éƒ¨é¡¾é—®çš„æœºå™¨äººï¼Ÿæ¯”å¦‚æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œæ¨èå¯¹åº”äº§å“ç»™ç”¨æˆ·ã€‚æ ¹æ®å†…éƒ¨çŸ¥è¯†åº“ï¼Œå›ç­”å†…éƒ¨åŒäº‹å…³äºäº§å“ä½¿ç”¨ï¼Œè¿è¥ä¿¡æ¯ç›¸å…³çš„é—®é¢˜å‘¢ã€‚è°¢è°¢</p>2023-04-03</li><br/><li><span>Toni</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è®­ç»ƒä¸€ä¸ªèƒ½æŠŠä» 1 åˆ†åˆ° 5 åˆ†çš„æ¯ä¸€ä¸ªçº§åˆ«éƒ½åŒºåˆ†å‡ºæ¥çš„æœºå™¨å­¦ä¹ æ¨¡å‹.
ä½¿ç”¨ä¸‹è½½çš„æ•°æ® fine_food_reviews_with_embeddings_1k.csv

éšæœºæ£®æ—æ¨¡å‹ï¼Œè¶…å‚æ•° n_estimators=100 ä¸‹è·‘çš„ç»“æœ

              precision    recall  f1-score   support

           1       0.86      0.30      0.44        20
           2       1.00      0.38      0.55         8
           3       1.00      0.18      0.31        11
           4       1.00      0.26      0.41        27
           5       0.74      1.00      0.85       134

       accuracy                               0.76       200
    macro avg       0.92      0.42      0.51       200
weighted avg       0.81      0.76      0.71       200

æç«¯éšæœºæ£®æ—æ¨¡å‹ ExtraTreesClassifier() ä¸‹è·‘çš„ç»“æœ

              precision    recall  f1-score   support

           1       1.00      0.20      0.33        20
           2       1.00      0.38      0.55         8
           3       1.00      0.18      0.31        11
           4       1.00      0.26      0.41        27
           5       0.73      1.00      0.84       134

      accuracy                                0.75       200
    macro avg       0.95      0.40      0.49       200
weighted avg       0.82      0.75      0.69       200
</p>2023-03-31</li><br/><li><span>Geek_513b7c</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è€å¸ˆï¼Œä½ èƒ½è§£é‡Šä¸€ä¸‹åœ¨aiä¸­å‘é‡æ˜¯ä»€ä¹ˆæ„æ€å—ï¼Ÿè¿™å‡ èŠ‚çœ‹çš„äº‘é‡Œé›¾é‡Œçš„</p>2023-03-28</li><br/><li><span>å››æœˆ.  ğŸ•Š</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¯·é—®ä¸€ä¸‹è€å¸ˆï¼Œæ—¢ç„¶å·²ç»æŒ‰ç…§batch_sizeåˆ’åˆ†å‡ºæ¥äº†prompt_batchesï¼Œä¸ºä»€ä¹ˆåœ¨get_embeddings_with_backoffå‡½æ•°ä¸­è¿˜è¦åˆ’åˆ†ä¸€æ¬¡batchå‘¢ï¼Ÿæ˜¯ä¸æ˜¯é‡å¤äº†ï¼Ÿ</p>2023-04-05</li><br/><li><span>ç‹å¹³</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆè¯·é—®å¦‚æœæŠŠæ—¶åºæ•°æ®ï¼Œæ¯”å¦‚ç”¨æˆ·è¡Œä¸ºåºåˆ—ä½œä¸ºè¾“å…¥ç»™openAI çš„ embedding, æ ¹æ®è¡Œä¸ºåˆ¤æ–­ç”¨æˆ·æƒ…æ„Ÿçš„æ•ˆæœä¼šå¥½å—ï¼Ÿ</p>2023-04-03</li><br/><li><span>é«˜æ·</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆæ‚¨å¥½ï¼Œæƒ³çŸ¥é“å¤§æ¦‚å¤šä¹…éœ€è¦å»åŒæ­¥ä¸€æ¬¡embeddingæ¨¡å‹ï¼Œå¤§æ¨¡å‹è‡ªå·±æ€ä¹ˆæ›´æ–°è¿­ä»£å‘¢ï¼Ÿ</p>2023-05-13</li><br/><li><span>æ¥šç¿”style</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>èŠ±è´¹å¤šå°‘tokenæ€ä¹ˆç®—å‘¢ 
ä¸€ä¸ªé—®é¢˜ä¸€ä¸ªtokenå—</p>2023-05-08</li><br/><li><span>R9Go</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>https:&#47;&#47;github.com&#47;xuwenhao&#47;geektime-ai-course&#47;blob&#47;main&#47;data&#47;20_newsgroup_with_embedding.parquet
è¿™ä¸ªæ•°æ®æ˜¯ä¸æ˜¯ä¸å…¨ï¼Ÿ</p>2023-05-02</li><br/><li><span>Geek_512735</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œè¯·åˆ©ç”¨ChatGPT embeddingçš„ä¼˜åŠ¿åœ¨ä»€ä¹ˆåœ°æ–¹ï¼Ÿ</p>2023-04-17</li><br/><li><span>Bank</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>åœ¨è¯»æ•°æ®çš„æ—¶å€™æŠ¥äº†è¿™ä¸ªé”™ï¼Œè¯·é—®å¦‚ä½•è§£å†³å‘¢ï¼Ÿå†…å­˜åº”è¯¥æ˜¯è¶³å¤Ÿçš„
---------------------------------------------------------------------------
ArrowMemoryError                          Traceback (most recent call last)
Cell In[6], line 6
      3 from sklearn.model_selection import train_test_split
      4 from sklearn.metrics import classification_report, accuracy_score
----&gt; 6 training_data = pd.read_parquet(&quot;D:&#47;work&#47;data&#47;toutiao_cat_data_all_with_embeddings.parquet&quot;)
      7 training_data.head()
      9 df =  training_data.sample(50000, random_state=42)

ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
-&gt; 2517 table = self._dataset.to_table(
   2518     columns=columns, filter=self._filter_expression,
   2519     use_threads=use_threads
   2520 )
   2522 # if use_pandas_metadata, restore the pandas metadata (which gets
   2523 # lost if doing a specific `columns` selection in to_table)
   2524 if use_pandas_metadata:

File D:\anaconda3\lib\site-packages\pyarrow\_dataset.pyx:332, in pyarrow._dataset.Dataset.to_table()

File D:\anaconda3\lib\site-packages\pyarrow\_dataset.pyx:2661, in pyarrow._dataset.Scanner.to_table()

File D:\anaconda3\lib\site-packages\pyarrow\error.pxi:144, in pyarrow.lib.pyarrow_internal_check_status()

File D:\anaconda3\lib\site-packages\pyarrow\error.pxi:117, in pyarrow.lib.check_status()

ArrowMemoryError: malloc of size 134217728 failed</p>2023-04-08</li><br/>
</ul>