ä½ å¥½ï¼Œæˆ‘æ˜¯æ–¹è¿œã€‚

æ¬¢è¿æ¥è·Ÿæˆ‘ä¸€èµ·å­¦ä¹ æƒ…æ„Ÿåˆ†æï¼Œä»Šå¤©æˆ‘ä»¬è¦è®²çš„å°±æ˜¯æœºå™¨å­¦ä¹ é‡Œçš„æ–‡æœ¬æƒ…æ„Ÿåˆ†æã€‚æ–‡æœ¬æƒ…æ„Ÿåˆ†æåˆå«åšè§‚ç‚¹æå–ã€ä¸»é¢˜åˆ†æã€å€¾å‘æ€§åˆ†æç­‰ã€‚å…‰è¯´æ¦‚å¿µï¼Œä½ å¯èƒ½ä¼šè§‰å¾—æœ‰äº›æŠ½è±¡ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹ä¸€ä¸ªç”Ÿæ´»ä¸­çš„åº”ç”¨ï¼Œä½ ä¸€çœ‹å°±èƒ½æ˜ç™½äº†ã€‚

æ¯”æ–¹è¯´æˆ‘ä»¬åœ¨è´­ç‰©ç½‘ç«™ä¸Šé€‰è´­ä¸€æ¬¾å•†å“æ—¶ï¼Œé¦–å…ˆä¼šç¿»é˜…ä¸€ä¸‹å•†å“è¯„ä»·ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä¸­å·®è¯„ã€‚è¿™äº›è¯„è®ºä¿¡æ¯è¡¨è¾¾äº†äººä»¬çš„å„ç§æƒ…æ„Ÿè‰²å½©å’Œæƒ…æ„Ÿå€¾å‘æ€§ï¼Œå¦‚å–œã€æ€’ã€å“€ã€ä¹å’Œæ‰¹è¯„ã€èµæ‰¬ç­‰ã€‚åƒè¿™æ ·æ ¹æ®è¯„ä»·æ–‡æœ¬ï¼Œç”±è®¡ç®—æœºè‡ªåŠ¨åŒºåˆ†è¯„ä»·å±äºå¥½è¯„ã€ä¸­è¯„æˆ–è€…è¯´å·®è¯„ï¼ŒèƒŒåç”¨åˆ°çš„æŠ€æœ¯å°±æ˜¯æƒ…æ„Ÿåˆ†æã€‚

å¦‚æœä½ è¿›ä¸€æ­¥è§‚å¯Ÿï¼Œè¿˜ä¼šå‘ç°ï¼Œåœ¨å¥½è¯„å·®è¯„çš„ä¸Šæ–¹è¿˜æœ‰ä¸€äº›æ ‡ç­¾ï¼Œæ¯”å¦‚â€œå£°éŸ³å¤§å°åˆé€‚â€ã€â€œè¿æ¥é€Ÿåº¦å¿«â€ã€â€œå”®åæ€åº¦å¾ˆå¥½â€ç­‰ã€‚è¿™äº›æ ‡ç­¾å…¶å®ä¹Ÿæ˜¯è®¡ç®—æœºæ ¹æ®æ–‡æœ¬ï¼Œè‡ªåŠ¨æå–çš„ä¸»é¢˜æˆ–è€…è§‚ç‚¹ã€‚

![](https://static001.geekbang.org/resource/image/ef/6f/ef69caa72565c50d98b63e20f499ea6f.jpg?wh=2572x2473)

æƒ…æ„Ÿåˆ†æçš„å¿«é€Ÿå‘å±•å¾—ç›Šäºç¤¾äº¤åª’ä½“çš„å…´èµ·ï¼Œè‡ª2000å¹´åˆä»¥æ¥ï¼Œæƒ…æ„Ÿåˆ†æå·²ç»æˆé•¿ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­æœ€æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸä¹‹ä¸€ï¼Œå®ƒä¹Ÿè¢«å¹¿æ³›åº”ç”¨åœ¨ä¸ªæ€§åŒ–æ¨èã€å•†ä¸šå†³ç­–ã€èˆ†æƒ…ç›‘æ§ç­‰æ–¹é¢ã€‚

ä»Šå¤©è¿™èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°†å®Œæˆä¸€ä¸ªæƒ…æ„Ÿåˆ†æé¡¹ç›®ï¼Œä¸€èµ·æ¥å¯¹å½±è¯„æ–‡æœ¬åšåˆ†æã€‚

## æ•°æ®å‡†å¤‡

ç°åœ¨æˆ‘ä»¬æ‰‹ä¸­æœ‰ä¸€æ‰¹å½±è¯„æ•°æ®ï¼ˆIMDBæ•°æ®é›†ï¼‰ï¼Œå½±è¯„è¢«åˆ†ä¸ºä¸¤ç±»ï¼šæ­£é¢è¯„ä»·ä¸è´Ÿé¢è¯„ä»·ã€‚æˆ‘ä»¬éœ€è¦è®­ç»ƒä¸€ä¸ªæƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œå¯¹å½±è¯„æ–‡æœ¬è¿›è¡Œåˆ†ç±»ã€‚

è¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»é—®é¢˜ï¼Œç ”ç©¶å¯¹è±¡æ˜¯ç”µå½±è¯„è®ºç±»çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ–‡æœ¬è¿›è¡ŒäºŒåˆ†ç±»ã€‚ä¸‹é¢æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹è®­ç»ƒæ•°æ®ã€‚

IMDBï¼ˆInternet Movie Databaseï¼‰æ˜¯ä¸€ä¸ªæ¥è‡ªäº’è”ç½‘ç”µå½±æ•°æ®åº“ï¼Œå…¶ä¸­åŒ…å«äº†50000æ¡ä¸¥é‡ä¸¤æåˆ†åŒ–çš„ç”µå½±è¯„è®ºã€‚æ•°æ®é›†è¢«åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­å„æœ‰25000æ¡è¯„è®ºï¼Œå¹¶ä¸”è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½åŒ…å«50%çš„æ­£é¢è¯„è®ºå’Œ50%çš„æ¶ˆæè¯„è®ºã€‚

### å¦‚ä½•ç”¨Torchtextè¯»å–æ•°æ®é›†

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨Torchtextå·¥å…·åŒ…æ¥è¯»å–æ•°æ®é›†ã€‚

Torchtextæ˜¯ä¸€ä¸ªåŒ…å«**å¸¸ç”¨çš„æ–‡æœ¬å¤„ç†å·¥å…·**å’Œ**å¸¸è§è‡ªç„¶è¯­è¨€æ•°æ®é›†**çš„å·¥å…·åŒ…ã€‚æˆ‘ä»¬å¯ä»¥ç±»æ¯”ä¹‹å‰å­¦ä¹ è¿‡çš„TorchvisionåŒ…æ¥ç†è§£å®ƒï¼Œåªä¸è¿‡ï¼ŒTorchvisionåŒ…æ˜¯ç”¨æ¥å¤„ç†å›¾åƒçš„ï¼Œè€ŒTorchtextåˆ™æ˜¯ç”¨æ¥å¤„ç†æ–‡æœ¬çš„ã€‚

å®‰è£…TorchtextåŒæ ·å¾ˆç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨pipè¿›è¡Œå®‰è£…ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

```plain
pip install torchtext
```

Torchtextä¸­åŒ…å«äº†ä¸Šé¢æˆ‘ä»¬è¦ä½¿ç”¨çš„IMDBæ•°æ®é›†ï¼Œå¹¶ä¸”è¿˜æœ‰è¯»å–è¯­æ–™åº“ã€è¯è½¬è¯å‘é‡ã€è¯è½¬ä¸‹æ ‡ã€å»ºç«‹ç›¸åº”è¿­ä»£å™¨ç­‰åŠŸèƒ½ï¼Œå¯ä»¥æ»¡è¶³æˆ‘ä»¬å¯¹æ–‡æœ¬çš„å¤„ç†éœ€æ±‚ã€‚

æ›´ä¸ºæ–¹ä¾¿çš„æ˜¯ï¼ŒTorchtextå·²ç»æŠŠä¸€äº›å¸¸è§å¯¹æ–‡æœ¬å¤„ç†çš„æ•°æ®é›†å›Šæ‹¬åœ¨äº†`torchtext.datasets`ä¸­ï¼Œä¸Torchvisionç±»ä¼¼ï¼Œä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½ã€è§£å‹å¹¶è§£ææ•°æ®ã€‚

ä»¥IMDBä¸ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨åé¢çš„ä»£ç æ¥è¯»å–æ•°æ®é›†ï¼š

```python
# è¯»å–IMDBæ•°æ®é›†
import torchtext
train_iter = torchtext.datasets.IMDB(root='./data', split='train')
next(train_iter)
```

torchtext.datasets.IMDBå‡½æ•°æœ‰ä¸¤ä¸ªå‚æ•°ï¼Œå…¶ä¸­ï¼š

- rootï¼šæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨äºæŒ‡å®šä½ æƒ³è¦è¯»å–ç›®æ ‡æ•°æ®é›†çš„ä½ç½®ï¼Œå¦‚æœæ•°æ®é›†ä¸å­˜åœ¨ï¼Œåˆ™ä¼šè‡ªåŠ¨ä¸‹è½½ï¼›
- splitï¼šæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–è€…å…ƒç»„ï¼Œè¡¨ç¤ºè¿”å›çš„æ•°æ®é›†ç±»å‹ï¼Œæ˜¯è®­ç»ƒé›†ã€æµ‹è¯•é›†æˆ–éªŒè¯é›†ï¼Œé»˜è®¤æ˜¯Â (â€˜trainâ€™, â€˜testâ€™)ã€‚  
  torchtext.datasets.IMDBå‡½æ•°çš„è¿”å›å€¼æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œè¿™é‡Œæˆ‘ä»¬è¯»å–äº†IMDBæ•°æ®é›†ä¸­çš„è®­ç»ƒé›†ï¼Œå…±25000æ¡æ•°æ®ï¼Œå­˜å…¥äº†å˜é‡train\_iterä¸­ã€‚

ç¨‹åºè¿è¡Œçš„ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåˆ©ç”¨next()å‡½æ•°ï¼Œè¯»å–å‡ºè¿­ä»£å™¨train\_iterä¸­çš„ä¸€æ¡æ•°æ®ï¼Œæ¯ä¸€è¡Œæ˜¯æƒ…ç»ªåˆ†ç±»ä»¥åŠåé¢çš„è¯„è®ºæ–‡æœ¬ã€‚â€œnegâ€è¡¨ç¤ºè´Ÿé¢è¯„ä»·ï¼Œâ€œposâ€è¡¨ç¤ºæ­£é¢è¯„ä»·ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/e4/e6/e4625437cafc8bb29851fb57a9b3e8e6.png?wh=1920x616)

### æ•°æ®å¤„ç†pipelines

è¯»å–å‡ºäº†æ•°æ®é›†ä¸­çš„è¯„è®ºæ–‡æœ¬å’Œæƒ…ç»ªåˆ†ç±»ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å°†æ–‡æœ¬å’Œåˆ†ç±»æ ‡ç­¾å¤„ç†æˆå‘é‡ï¼Œæ‰èƒ½è¢«è®¡ç®—æœºè¯»å–ã€‚å¤„ç†æ–‡æœ¬çš„ä¸€èˆ¬è¿‡ç¨‹æ˜¯å…ˆåˆ†è¯ï¼Œç„¶åæ ¹æ®è¯æ±‡è¡¨å°†è¯è¯­è½¬æ¢ä¸ºidã€‚

Torchtextä¸ºæˆ‘ä»¬æä¾›äº†åŸºæœ¬çš„æ–‡æœ¬å¤„ç†å·¥å…·ï¼ŒåŒ…æ‹¬åˆ†è¯å™¨â€œtokenizerâ€å’Œè¯æ±‡è¡¨â€œvocabâ€ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢ä¸¤ä¸ªå‡½æ•°æ¥åˆ›å»ºåˆ†è¯å™¨å’Œè¯æ±‡è¡¨ã€‚

get\_tokenizerå‡½æ•°çš„ä½œç”¨æ˜¯åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨ã€‚å°†æ–‡æœ¬å–‚ç»™ç›¸åº”çš„åˆ†è¯å™¨ï¼Œåˆ†è¯å™¨å°±å¯ä»¥æ ¹æ®ä¸åŒåˆ†è¯å‡½æ•°çš„è§„åˆ™å®Œæˆåˆ†è¯ã€‚ä¾‹å¦‚è‹±æ–‡çš„åˆ†è¯å™¨ï¼Œå°±æ˜¯ç®€å•æŒ‰ç…§ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œåˆ†è¯ã€‚

build\_vocab\_from\_iteratorå‡½æ•°å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä½¿ç”¨è®­ç»ƒæ•°æ®é›†çš„è¿­ä»£å™¨æ„å»ºè¯æ±‡è¡¨ï¼Œæ„å»ºå¥½è¯æ±‡è¡¨åï¼Œè¾“å…¥åˆ†è¯åçš„ç»“æœï¼Œå³å¯è¿”å›æ¯ä¸ªè¯è¯­çš„idã€‚

åˆ›å»ºåˆ†è¯å™¨å’Œæ„å»ºè¯æ±‡è¡¨çš„ä»£ç å¦‚ä¸‹ã€‚é¦–å…ˆæˆ‘ä»¬è¦å»ºç«‹ä¸€ä¸ªå¯ä»¥å¤„ç†è‹±æ–‡çš„åˆ†è¯å™¨tokenizerï¼Œç„¶åå†æ ¹æ®IMDBæ•°æ®é›†çš„è®­ç»ƒé›†è¿­ä»£å™¨train\_iterå»ºç«‹è¯æ±‡è¡¨vocabã€‚

```python
# åˆ›å»ºåˆ†è¯å™¨
tokenizer = torchtext.data.utils.get_tokenizer('basic_english')
print(tokenizer('here is the an example!'))
'''
è¾“å‡ºï¼š['here', 'is', 'the', 'an', 'example', '!']
'''

# æ„å»ºè¯æ±‡è¡¨
def yield_tokens(data_iter):
Â  Â  for _, text in data_iter:
Â  Â  Â  Â  yield tokenizer(text)

vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_iter), specials=["<pad>", "<unk>"])
vocab.set_default_index(vocab["<unk>"])

print(vocab(tokenizer('here is the an example <pad> <pad>')))
'''
è¾“å‡ºï¼š[131, 9, 40, 464, 0, 0]
'''
```

åœ¨æ„å»ºè¯æ±‡è¡¨çš„è¿‡ç¨‹ä¸­ï¼Œyield\_tokenså‡½æ•°çš„ä½œç”¨å°±æ˜¯ä¾æ¬¡å°†è®­ç»ƒæ•°æ®é›†ä¸­çš„æ¯ä¸€æ¡æ•°æ®éƒ½è¿›è¡Œåˆ†è¯å¤„ç†ã€‚å¦å¤–ï¼Œåœ¨æ„å»ºè¯æ±‡è¡¨æ—¶ï¼Œç”¨æˆ·è¿˜å¯ä»¥åˆ©ç”¨specialså‚æ•°è‡ªå®šä¹‰è¯è¡¨ã€‚

ä¸Šè¿°ä»£ç ä¸­æˆ‘ä»¬è‡ªå®šä¹‰äº†ä¸¤ä¸ªè¯è¯­ï¼šâ€œ&lt;pad&gt;â€å’Œâ€œ&lt;unk&gt;â€ï¼Œåˆ†åˆ«è¡¨ç¤ºå ä½ç¬¦å’Œæœªç™»å½•è¯ã€‚é¡¾åæ€ä¹‰ï¼Œæœªç™»å½•è¯æ˜¯æŒ‡æ²¡æœ‰è¢«æ”¶å½•åœ¨åˆ†è¯è¯è¡¨ä¸­çš„è¯ã€‚ç”±äºæ¯æ¡å½±è¯„æ–‡æœ¬çš„é•¿åº¦ä¸åŒï¼Œä¸èƒ½ç›´æ¥æ‰¹é‡åˆæˆçŸ©é˜µï¼Œå› æ­¤éœ€é€šè¿‡æˆªæ–­æˆ–å¡«è¡¥å ä½ç¬¦æ¥å›ºå®šé•¿åº¦ã€‚

ä¸ºäº†æ–¹ä¾¿åç»­è°ƒç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ†è¯å™¨å’Œè¯æ±‡è¡¨æ¥å»ºç«‹æ•°æ®å¤„ç†çš„pipelinesã€‚æ–‡æœ¬pipelineç”¨äºç»™å®šä¸€æ®µæ–‡æœ¬ï¼Œè¿”å›åˆ†è¯åçš„idã€‚æ ‡ç­¾pipelineç”¨äºå°†æƒ…ç»ªåˆ†ç±»è½¬åŒ–ä¸ºæ•°å­—ï¼Œå³â€œnegâ€è½¬åŒ–ä¸º0ï¼Œâ€œposâ€è½¬åŒ–ä¸º1ã€‚

å…·ä½“ä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚

```python
# æ•°æ®å¤„ç†pipelines
text_pipeline = lambda x: vocab(tokenizer(x))
label_pipeline = lambda x: 1 if x == 'pos' else 0

print(text_pipeline('here is the an example'))
'''
è¾“å‡ºï¼š[131, 9, 40, 464, 0, 0 , ... , 0]
'''
print(label_pipeline('neg'))
'''
è¾“å‡ºï¼š0
'''
```

é€šè¿‡ç¤ºä¾‹çš„è¾“å‡ºç»“æœï¼Œç›¸ä¿¡ä½ å¾ˆå®¹æ˜“å°±èƒ½ç†è§£æ–‡æœ¬pipelineå’Œæ ‡ç­¾pipelineçš„ç”¨æ³•äº†ã€‚

### ç”Ÿæˆè®­ç»ƒæ•°æ®

æœ‰äº†æ•°æ®å¤„ç†çš„pipelinesï¼Œæ¥ä¸‹æ¥å°±æ˜¯ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œä¹Ÿå°±æ˜¯ç”ŸæˆDataLoaderã€‚

è¿™é‡Œè¿˜æ¶‰åŠåˆ°ä¸€ä¸ªå˜é•¿æ•°æ®å¤„ç†çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨å°†æ–‡æœ¬pipelineæ‰€ç”Ÿæˆçš„idåˆ—è¡¨è½¬åŒ–ä¸ºæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«çš„tensoræ—¶ï¼Œç”±äºæ–‡æœ¬çš„å¥å­æ˜¯å˜é•¿çš„ï¼Œå› æ­¤ç”Ÿæˆçš„tensoré•¿åº¦ä¸ä¸€ï¼Œæ— æ³•ç»„æˆçŸ©é˜µã€‚

è¿™æ—¶ï¼Œæˆ‘ä»¬éœ€è¦é™å®šä¸€ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦ã€‚ä¾‹å¦‚å¥å­çš„æœ€å¤§é•¿åº¦ä¸º256ä¸ªå•è¯ï¼Œé‚£ä¹ˆè¶…è¿‡256ä¸ªå•è¯çš„å¥å­éœ€è¦åšæˆªæ–­å¤„ç†ï¼›ä¸è¶³256ä¸ªå•è¯çš„å¥å­ï¼Œéœ€è¦ç»Ÿä¸€è¡¥ä½ï¼Œè¿™é‡Œç”¨â€œ/â€æ¥å¡«è¡¥ã€‚

ä¸Šé¢æ‰€è¯´çš„è¿™äº›æ“ä½œï¼Œæˆ‘ä»¬éƒ½å¯ä»¥æ”¾åˆ°collate\_batchå‡½æ•°ä¸­æ¥å¤„ç†ã€‚

collate\_batchå‡½æ•°æœ‰ä»€ä¹ˆç”¨å‘¢ï¼Ÿå®ƒè´Ÿè´£åœ¨DataLoadæå–ä¸€ä¸ªbatchçš„æ ·æœ¬æ—¶ï¼Œå®Œæˆä¸€ç³»åˆ—é¢„å¤„ç†å·¥ä½œï¼šåŒ…æ‹¬ç”Ÿæˆæ–‡æœ¬çš„tensorã€ç”Ÿæˆæ ‡ç­¾çš„tensorã€ç”Ÿæˆå¥å­é•¿åº¦çš„tensorï¼Œä»¥åŠä¸Šé¢æ‰€è¯´çš„å¯¹æ–‡æœ¬è¿›è¡Œæˆªæ–­ã€è¡¥ä½æ“ä½œã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å°†collate\_batchå‡½æ•°é€šè¿‡å‚æ•°collate\_fnä¼ å…¥DataLoaderï¼Œå³å¯å®ç°å¯¹å˜é•¿æ•°æ®çš„å¤„ç†ã€‚

collate\_batchå‡½æ•°çš„å®šä¹‰ï¼Œä»¥åŠç”Ÿæˆè®­ç»ƒä¸éªŒè¯DataLoaderçš„ä»£ç å¦‚ä¸‹ã€‚

```python
# ç”Ÿæˆè®­ç»ƒæ•°æ®
import torch
import torchtext
from torch.utils.data import DataLoader
from torch.utils.data.dataset import random_split
from torchtext.data.functional import to_map_style_dataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def collate_batch(batch):
Â  Â  max_length = 256
Â  Â  pad = text_pipeline('<pad>')
Â  Â  label_list, text_list, length_list = [], [], []
Â  Â  for (_label, _text) in batch:
Â  Â  Â  Â  Â label_list.append(label_pipeline(_label))
Â  Â  Â  Â  Â processed_text = text_pipeline(_text)[:max_length]
Â  Â  Â  Â  Â length_list.append(len(processed_text))
Â  Â  Â  Â  Â text_list.append((processed_text+pad*max_length)[:max_length])
Â  Â  label_list = torch.tensor(label_list, dtype=torch.int64)
Â  Â  text_list = torch.tensor(text_list, dtype=torch.int64)
Â  Â  length_list = torch.tensor(length_list, dtype=torch.int64)
Â  Â  return label_list.to(device), text_list.to(device), length_list.to(device)

train_iter = torchtext.datasets.IMDB(root='./data', split='train')
train_dataset = to_map_style_dataset(train_iter)
num_train = int(len(train_dataset) * 0.95)
split_train_, split_valid_ = random_split(train_dataset,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â [num_train, len(train_dataset) - num_train])
train_dataloader = DataLoader(split_train_, batch_size=8, shuffle=True, collate_fn=collate_batch)
valid_dataloader = DataLoader(split_valid_, batch_size=8, shuffle=False, collate_fn=collate_batch)
```

æˆ‘ä»¬ä¸€èµ·æ¢³ç†ä¸€ä¸‹è¿™æ®µä»£ç çš„æµç¨‹ï¼Œä¸€å…±æ˜¯äº”ä¸ªæ­¥éª¤ã€‚

1.åˆ©ç”¨torchtextè¯»å–IMDBçš„è®­ç»ƒæ•°æ®é›†ï¼Œå¾—åˆ°è®­ç»ƒæ•°æ®è¿­ä»£å™¨ï¼›  
2.ä½¿ç”¨to\_map\_style\_datasetå‡½æ•°å°†è¿­ä»£å™¨è½¬åŒ–ä¸ºDatasetç±»å‹ï¼›  
3.ä½¿ç”¨random\_splitå‡½æ•°å¯¹Datasetè¿›è¡Œåˆ’åˆ†ï¼Œå…¶ä¸­95%ä½œä¸ºè®­ç»ƒé›†ï¼Œ5%ä½œä¸ºéªŒè¯é›†ï¼›  
4.ç”Ÿæˆè®­ç»ƒé›†çš„DataLoaderï¼›  
5.ç”ŸæˆéªŒè¯é›†çš„DataLoaderã€‚

åˆ°æ­¤ä¸ºæ­¢ï¼Œæ•°æ®éƒ¨åˆ†å·²ç»å…¨éƒ¨å‡†å¤‡å®Œæ¯•äº†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥è¿›è¡Œç½‘ç»œæ¨¡å‹çš„æ„å»ºã€‚

## æ¨¡å‹æ„å»º

ä¹‹å‰æˆ‘ä»¬å·²ç»å­¦è¿‡å·ç§¯ç¥ç»ç½‘ç»œçš„ç›¸å…³çŸ¥è¯†ã€‚å·ç§¯ç¥ç»ç½‘ç»œä½¿ç”¨å›ºå®šçš„å¤§å°çŸ©é˜µä½œä¸ºè¾“å…¥ï¼ˆä¾‹å¦‚ä¸€å¼ å›¾ç‰‡ï¼‰ï¼Œç„¶åè¾“å‡ºä¸€ä¸ªå›ºå®šå¤§å°çš„å‘é‡ï¼ˆä¾‹å¦‚ä¸åŒç±»åˆ«çš„æ¦‚ç‡ï¼‰ï¼Œå› æ­¤é€‚ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ç­‰ã€‚

ä½†æ˜¯é™¤äº†å›¾åƒä¹‹å¤–ï¼Œè¿˜æœ‰å¾ˆå¤šä¿¡æ¯ï¼Œå…¶å¤§å°æˆ–é•¿åº¦å¹¶ä¸æ˜¯å›ºå®šçš„ï¼Œä¾‹å¦‚éŸ³é¢‘ã€è§†é¢‘ã€æ–‡æœ¬ç­‰ã€‚æˆ‘ä»¬æƒ³è¦å¤„ç†è¿™äº›åºåˆ—ç›¸å…³çš„æ•°æ®ï¼Œå°±è¦ç”¨åˆ°æ—¶åºæ¨¡å‹ã€‚æ¯”å¦‚æˆ‘ä»¬ä»Šå¤©è¦å¤„ç†çš„æ–‡æœ¬æ•°æ®ï¼Œè¿™å°±æ¶‰åŠä¸€ç§å¸¸è§çš„æ—¶é—´åºåˆ—æ¨¡å‹ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networkï¼ŒRNNï¼‰ã€‚

ä¸è¿‡ç”±äºRNNè‡ªèº«çš„ç»“æ„é—®é¢˜ï¼Œåœ¨è¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸ã€‚**LSTMç½‘ç»œ**åœ¨RNNç»“æ„çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œé€šè¿‡ç²¾å¦™çš„é—¨æ§åˆ¶å°†çŸ­æ—¶è®°å¿†ä¸é•¿æ—¶è®°å¿†ç»“åˆèµ·æ¥ï¼Œ**ä¸€å®šç¨‹åº¦ä¸Šè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜**ã€‚

æˆ‘ä»¬ä½¿ç”¨LSTMç½‘ç»œæ¥è¿›è¡Œæƒ…ç»ªåˆ†ç±»çš„é¢„æµ‹ã€‚æ¨¡å‹çš„å®šä¹‰å¦‚ä¸‹ã€‚

```python
# å®šä¹‰æ¨¡å‹
class LSTM(torch.nn.Module):
Â  Â  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional,
Â  Â  Â  Â  Â  Â  Â  Â  Â dropout_rate, pad_index=0):
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)
Â  Â  Â  Â  self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dropout=dropout_rate, batch_first=True)
Â  Â  Â  Â  self.fc = torch.nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)
Â  Â  Â  Â  self.dropout = torch.nn.Dropout(dropout_rate)
Â  Â  Â  Â Â 
Â  Â  def forward(self, ids, length):
Â  Â  Â  Â  embedded = self.dropout(self.embedding(ids))
Â  Â  Â  Â  packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  enforce_sorted=False)
Â  Â  Â  Â  packed_output, (hidden, cell) = self.lstm(packed_embedded)
Â  Â  Â  Â  output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)
Â  Â  Â  Â  if self.lstm.bidirectional:
Â  Â  Â  Â  Â  Â  hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  hidden = self.dropout(hidden[-1])
Â  Â  Â  Â  prediction = self.fc(hidden)
Â  Â  Â  Â  return prediction
```

ç½‘ç»œæ¨¡å‹çš„å…·ä½“ç»“æ„ï¼Œé¦–å…ˆæ˜¯ä¸€ä¸ªEmbeddingå±‚ï¼Œç”¨æ¥æ¥æ”¶æ–‡æœ¬idçš„tensorï¼Œç„¶åæ˜¯LSTMå±‚ï¼Œæœ€åæ˜¯ä¸€ä¸ªå…¨è¿æ¥åˆ†ç±»å±‚ã€‚å…¶ä¸­ï¼Œbidirectionalä¸ºTrueï¼Œè¡¨ç¤ºç½‘ç»œä¸ºåŒå‘LSTMï¼Œbidirectionalä¸ºFalseï¼Œè¡¨ç¤ºç½‘ç»œä¸ºå•å‘LSTMã€‚

ç½‘ç»œæ¨¡å‹çš„ç»“æ„å›¾å¦‚ä¸‹æ‰€ç¤ºã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/f4/a8/f4013742ab70b0dc405948f07198cfa8.jpg?wh=619x404)

## æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°

å®šä¹‰å¥½ç½‘ç»œæ¨¡å‹çš„ç»“æ„ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒäº†ã€‚é¦–å…ˆæ˜¯å®ä¾‹åŒ–ç½‘ç»œæ¨¡å‹ï¼Œå‚æ•°ä»¥åŠå…·ä½“çš„ä»£ç å¦‚ä¸‹ã€‚

```python
# å®ä¾‹åŒ–æ¨¡å‹
vocab_size = len(vocab)
embedding_dim = 300
hidden_dim = 300
output_dim = 2
n_layers = 2
bidirectional = True
dropout_rate = 0.5

model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)
model = model.to(device)
```

ç”±äºæ•°æ®çš„æƒ…æ„Ÿææ€§å…±åˆ†ä¸ºä¸¤ç±»ï¼Œå› æ­¤è¿™é‡Œæˆ‘ä»¬è¦æŠŠoutput\_dimçš„å€¼è®¾ç½®ä¸º2ã€‚  
æ¥ä¸‹æ¥æ˜¯å®šä¹‰æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–æ–¹æ³•ï¼Œä»£ç å¦‚ä¸‹ã€‚åœ¨ä¹‹å‰çš„è¯¾ç¨‹é‡Œä¹Ÿå¤šæ¬¡è®²è¿‡äº†ï¼Œæ‰€ä»¥è¿™é‡Œä¸å†é‡å¤ã€‚

```python
# æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–æ–¹æ³•
lr = 5e-4
criterion = torch.nn.CrossEntropyLoss()
criterion = criterion.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=lr)
```

è®¡ç®—lossçš„ä»£ç å¦‚ä¸‹ã€‚

```python
import tqdm
import sys
import numpy as np

def train(dataloader, model, criterion, optimizer, device):
Â  Â  model.train()
Â  Â  epoch_losses = []
Â  Â  epoch_accs = []
Â  Â  for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):
Â  Â  Â  Â  (label, ids, length) = batch
Â  Â  Â  Â  label = label.to(device)
Â  Â  Â  Â  ids = ids.to(device)
Â  Â  Â  Â  length = length.to(device)
Â  Â  Â  Â  prediction = model(ids, length)
Â  Â  Â  Â  loss = criterion(prediction, label) # lossè®¡ç®—
Â  Â  Â  Â  accuracy = get_accuracy(prediction, label)
        # æ¢¯åº¦æ›´æ–°
Â  Â  Â  Â  optimizer.zero_grad()
Â  Â  Â  Â  loss.backward()
Â  Â  Â  Â  optimizer.step()
Â  Â  Â  Â  epoch_losses.append(loss.item())
Â  Â  Â  Â  epoch_accs.append(accuracy.item())
Â  Â  return epoch_losses, epoch_accs

def evaluate(dataloader, model, criterion, device):
Â  Â  model.eval()
Â  Â  epoch_losses = []
Â  Â  epoch_accs = []
Â  Â  with torch.no_grad():
Â  Â  Â  Â  for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):
Â  Â  Â  Â  Â  Â  (label, ids, length) = batch
Â  Â  Â  Â  Â  Â  label = label.to(device)
Â  Â  Â  Â  Â  Â  ids = ids.to(device)
Â  Â  Â  Â  Â  Â  length = length.to(device)
Â  Â  Â  Â  Â  Â  prediction = model(ids, length)
Â  Â  Â  Â  Â  Â  loss = criterion(prediction, label) # lossè®¡ç®—
Â  Â  Â  Â  Â  Â  accuracy = get_accuracy(prediction, label)
Â  Â  Â  Â  Â  Â  epoch_losses.append(loss.item())
Â  Â  Â  Â  Â  Â  epoch_accs.append(accuracy.item())
Â  Â  return epoch_losses, epoch_accs
```

å¯ä»¥çœ‹åˆ°ï¼Œè¿™é‡Œè®­ç»ƒè¿‡ç¨‹ä¸éªŒè¯è¿‡ç¨‹çš„lossè®¡ç®—ï¼Œåˆ†åˆ«å®šä¹‰åœ¨äº†trainå‡½æ•°å’Œevaluateå‡½æ•°ä¸­ã€‚ä¸»è¦åŒºåˆ«æ˜¯è®­ç»ƒè¿‡ç¨‹æœ‰æ¢¯åº¦çš„æ›´æ–°ï¼Œè€ŒéªŒè¯è¿‡ç¨‹ä¸­ä¸æ¶‰åŠæ¢¯åº¦çš„æ›´æ–°ï¼Œåªè®¡ç®—losså³å¯ã€‚  
æ¨¡å‹çš„è¯„ä¼°æˆ‘ä»¬ä½¿ç”¨ACCï¼Œä¹Ÿå°±æ˜¯å‡†ç¡®ç‡ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œè®¡ç®—ACCçš„ä»£ç å¦‚ä¸‹ã€‚

```python
def get_accuracy(prediction, label):
Â  Â  batch_size, _ = prediction.shape
Â  Â  predicted_classes = prediction.argmax(dim=-1)
Â  Â  correct_predictions = predicted_classes.eq(label).sum()
Â  Â  accuracy = correct_predictions / batch_size
Â  Â  return accuracy
```

æœ€åï¼Œè®­ç»ƒè¿‡ç¨‹çš„å…·ä½“ä»£ç å¦‚ä¸‹ã€‚åŒ…æ‹¬è®¡ç®—losså’ŒACCã€ä¿å­˜lossesåˆ—è¡¨å’Œä¿å­˜æœ€ä¼˜æ¨¡å‹ã€‚

```python
n_epochs = 10
best_valid_loss = float('inf')

train_losses = []
train_accs = []
valid_losses = []
valid_accs = []

for epoch in range(n_epochs):
Â  Â  train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)
Â  Â  valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)
Â  Â  train_losses.extend(train_loss)
Â  Â  train_accs.extend(train_acc)
Â  Â  valid_losses.extend(valid_loss)
Â  Â  valid_accs.extend(valid_acc)Â 
Â  Â  epoch_train_loss = np.mean(train_loss)
Â  Â  epoch_train_acc = np.mean(train_acc)
Â  Â  epoch_valid_loss = np.mean(valid_loss)
Â  Â  epoch_valid_acc = np.mean(valid_acc)Â  Â Â 
Â  Â  if epoch_valid_loss < best_valid_loss:
Â  Â  Â  Â  best_valid_loss = epoch_valid_loss
Â  Â  Â  Â  torch.save(model.state_dict(), 'lstm.pt') Â Â 
Â  Â  print(f'epoch: {epoch+1}')
Â  Â  print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')
Â  Â  print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')
```

æˆ‘ä»¬è¿˜å¯ä»¥åˆ©ç”¨ä¿å­˜ä¸‹æ¥train\_lossesåˆ—è¡¨ï¼Œç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„lossæ›²çº¿ï¼Œæˆ–ä½¿ç”¨[ç¬¬15è¯¾](https://time.geekbang.org/column/article/444252)è®²è¿‡çš„å¯è§†åŒ–å·¥å…·æ¥ç›‘æ§è®­ç»ƒè¿‡ç¨‹ã€‚  
è‡³æ­¤ï¼Œä¸€ä¸ªå®Œæ•´çš„æƒ…æ„Ÿåˆ†æé¡¹ç›®å·²ç»å®Œæˆäº†ã€‚ä»æ•°æ®è¯»å–åˆ°æ¨¡å‹æ„å»ºä¸è®­ç»ƒçš„æ–¹æ–¹é¢é¢ï¼Œæˆ‘éƒ½æ‰‹æŠŠæ‰‹æ•™ç»™äº†ä½ ï¼Œå¸Œæœ›ä½ èƒ½ä»¥æ­¤ä¸ºæ¨¡æ¿ï¼Œç‹¬ç«‹è§£å†³è‡ªå·±çš„é—®é¢˜ã€‚

## å°ç»“

æ­å–œä½ ï¼Œå®Œæˆäº†ä»Šå¤©çš„å­¦ä¹ ä»»åŠ¡ã€‚ä»Šå¤©æˆ‘ä»¬ä¸€èµ·å®Œæˆäº†ä¸€ä¸ªæƒ…æ„Ÿåˆ†æé¡¹ç›®çš„å®è·µï¼Œç›¸å½“äºæ˜¯å¯¹è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„ä¸€ä¸ªåˆæ¢ã€‚æˆ‘å¸¦ä½ å›é¡¾ä¸€ä¸‹ä»Šå¤©å­¦ä¹ çš„è¦ç‚¹ã€‚

åœ¨æ•°æ®å‡†å¤‡é˜¶æ®µï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨PyTorchæä¾›çš„æ–‡æœ¬å¤„ç†å·¥å…·åŒ…Torchtextã€‚æƒ³è¦æŒæ¡Torchtextä¹Ÿä¸éš¾ï¼Œæˆ‘ä»¬å¯ä»¥ç±»æ¯”ä¹‹å‰è¯¦ç»†ä»‹ç»è¿‡çš„Torchvisionï¼Œä¸æ‡‚çš„åœ°æ–¹å†å¯¹åº”å»[æŸ¥é˜…æ–‡æ¡£](https://pytorch.org/text/stable/index.html)ï¼Œç›¸ä¿¡ä½ ä¸€å®šå¯ä»¥åšåˆ°ä¸¾ä¸€åä¸‰ã€‚

**æ¨¡å‹æ„å»ºæ—¶ï¼Œè¦æ ¹æ®å…·ä½“çš„é—®é¢˜é€‰æ‹©é€‚åˆçš„ç¥ç»ç½‘ç»œã€‚å·ç§¯ç¥ç»ç½‘ç»œå¸¸è¢«ç”¨äºå¤„ç†å›¾åƒä½œä¸ºè¾“å…¥çš„é¢„æµ‹é—®é¢˜ï¼›å¾ªç¯ç¥ç»ç½‘ç»œå¸¸è¢«ç”¨äºå¤„ç†å˜é•¿çš„ã€åºåˆ—ç›¸å…³çš„æ•°æ®ã€‚è€ŒLSTMç›¸è¾ƒäºRNNï¼Œèƒ½æ›´å¥½åœ°è§£å†³æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜**ã€‚

åœ¨åç»­çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šè®²è§£ä¸¤å¤§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼šæ–‡æœ¬åˆ†ç±»å’Œæ‘˜è¦ç”Ÿæˆï¼Œå®ƒä»¬åˆ†åˆ«åŒ…æ‹¬äº†åˆ¤åˆ«æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹ï¼Œç›¸ä¿¡é‚£æ—¶ä½ ä¸€å®šä¼šåœ¨æ–‡æœ¬å¤„ç†æ–¹é¢æœ‰æ›´æ·±å±‚æ¬¡çš„ç†è§£ã€‚

## æ¯è¯¾ä¸€ç»ƒ

åˆ©ç”¨ä»Šå¤©è®­ç»ƒçš„æ¨¡å‹ï¼Œç¼–å†™ä¸€ä¸ªå‡½æ•°predict\_sentimentï¼Œå®ç°è¾“å…¥ä¸€å¥è¯ï¼Œè¾“å‡ºè¿™å¥è¯çš„æƒ…ç»ªç±»åˆ«ä¸æ¦‚ç‡ã€‚

ä¾‹å¦‚ï¼š

```python
text = "This film is terrible!"
predict_sentiment(text, model, tokenizer, vocab, device)
'''
è¾“å‡ºï¼š('neg', 0.8874172568321228)
'''
```

æ¬¢è¿ä½ åœ¨ç•™è¨€åŒºè·Ÿæˆ‘äº¤æµäº’åŠ¨ï¼Œä¹Ÿæ¨èä½ æŠŠä»Šå¤©å­¦åˆ°çš„å†…å®¹åˆ†äº«ç»™æ›´å¤šæœ‹å‹ï¼Œè·Ÿä»–ä¸€èµ·å­¦ä¹ è¿›æ­¥ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>æé›„</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>å¦‚æœæ˜¯ä½¿ç”¨torchtext==0.8.1ä»¥ä¸‹çš„ç‰ˆæœ¬å»ºè®®çœ‹å®˜ç½‘æ–‡æ¡£ï¼š
https:&#47;&#47;pytorch.org&#47;text&#47;0.8.1&#47;datasets.html</p>2021-12-31</li><br/><li><span>å¿«ä¹å°å¤œæ›²</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length.to(&#39;cpu&#39;), batch_first=True,  enforce_sorted=False)
è¿™é‡Œlengthå¿…é¡»è½¬ä¸ºæˆcpuï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚</p>2022-01-02</li><br/><li><span>æ—äºç¿”</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>LSTMæ¨¡å‹å®šä¹‰ä¸­:
 if self.lstm.bidirectional:
            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))
è¿™é‡Œä¸å¤ªç†è§£ï¼Œæœ€åè¿æ¥çš„ä¸æ˜¯æœ€åä¸€å±‚hiddençš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥å’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥å˜›ï¼Ÿå¦å¤–dimä¸ºä»€ä¹ˆä¼šæ˜¯-1ã€‚</p>2021-12-17</li><br/><li><span>hallo128</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>torchtextå¯ä»¥æ”¯æŒä¸­æ–‡åˆ†è¯å—</p>2022-06-30</li><br/><li><span>Geek_a82ba7</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ4ï¼‰<p>import torchtext
train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)
next(train_iter)
&#39;MapperIterDataPipe&#39; object is not an iterator
ä¸ºä»€ä¹ˆè¯´è¿™ä¸ªä¸æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼ŒæŸ¥äº†å¾ˆå¤šèµ„æ–™éƒ½æ²¡æœ‰è§£å†³ï¼Œè€å¸ˆèƒ½å›ç­”ä¸€ä¸‹å—ï¼Ÿ
c</p>2022-06-21</li><br/><li><span>Saraié’éœ</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æ–¹è€å¸ˆï¼Œæˆ‘è¯•äº†ä»£ç ï¼Œè¾“å‡ºæ˜¾ç¤ºï¼š
&quot;\nè¾“å‡ºï¼š[&#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;an&#39;, &#39;example&#39;, &#39;!&#39;]\n&quot;
æ˜¯æ–­è¡Œå‡ºç°é—®é¢˜äº†å—ï¼Ÿ
è°¢è°¢ï¼</p>2022-06-01</li><br/><li><span>Ringcoo</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆå¥½ï¼Œè¯·é—®
packed_output, (hidden, cell) = self.lstm(packed_embedded)       
 output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)
ä¸ºä»€ä¹ˆä»lstmå±‚å‡ºæ¥ä»¥å ä¼šæœ‰(hidden, cell) ï¼Œæˆ‘ä¸å¤ªç†è§£ï¼Œä»¥åŠä¸ºä»€ä¹ˆlstmå±‚å‡ºæ¥ä»¥åè¿˜æœ‰åœ¨è¿›RNNå±‚ï¼Œåº•ä¸‹çš„æµç¨‹å›¾ä¸Šæ˜æ˜æ˜¯lstmå±‚å‡ºæ¥ä»¥åè¿›å…¥äº†fcå…¨è¿æ¥å±‚ã€‚éº»çƒ¦æ‚¨è®²è§£ä¸€ä¸‹</p>2022-05-01</li><br/><li><span>èµµå¿ƒç¿</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¯·é—®ä½¿ç”¨çš„torchtextæ˜¯å“ªä¸ªç‰ˆæœ¬çš„å‘¢ï¼Ÿ
</p>2022-03-14</li><br/><li><span>æé›„</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¯·é—®è¿™æ˜¯é‚£ä¸ªç‰ˆæœ¬çš„torchtext</p>2021-12-31</li><br/><li><span>å´åä¸€</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>def predict_sentiment(text, model, tokenizer, vocab, device):
    max_length = 256
    pad = text_pipeline(&#39;&lt;pad&gt;&#39;)
    processed_text = text_pipeline(text)[:max_length]
    ids, length = [],[]
    ids.append((processed_text+pad*max_length)[:max_length])
    length.append(len(processed_text))
    ids_t,length_t = torch.tensor(ids, dtype = torch.int64), torch.tensor(length, dtype = torch.int64)
    print(ids_t,length_t)
    ids_t.to(device)
    length_t.to(device)
    pred = model(ids_t,length_t)
    return pred
vocab_size = len(vocab)
embedding_dim = 300
hidden_dim = 300
output_dim = 2
n_layers = 2
bidirectional = True
dropout_rate = 0.5
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)
model = model.to(device)
model.load_state_dict(torch.load(&quot;.&#47;lstm.pt&quot;))
model.eval()
text = &quot;This film is terrible!&quot;
pred = predict_sentiment(text, model, tokenizer, vocab, device)
&#39;&#39;&#39;
è¾“å‡ºï¼š(&#39;neg&#39;, 0.8874172568321228)
&#39;&#39;&#39;
tipsï¼šä¸€å®šè¦å®‰è£…torchtext ç¨å¾®æ–°ç‚¹çš„ç‰ˆæœ¬ï¼Œ0.6.0 IDMBæ•°æ®setè·‘æŠ¥é”™ï¼Œä¼šä¸€ç›´è¦æ±‚ç”¨tensorflow v1 ç‰ˆæœ¬å…¼å®¹</p>2022-06-12</li><br/><li><span>æé›„</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>import torch
import torchtext
torchtext.__version__
dir(torchtext)
from torchtext import data
dir(data)

# è¯»å–IMDBæ•°æ®é›†


# è¯»å–IMDBæ•°æ®é›†

TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)
LABEL = data.Field(sequential=False)

# make splits for data åˆ’åˆ†æ•°æ®
train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL)

# build the vocabulary
from torchtext.vocab import GloVe
TEXT.build_vocab(train, vectors=GloVe(name=&#39;6B&#39;, dim=300))
LABEL.build_vocab(train)

# make iterator for splits
train_iter, test_iter = data.BucketIterator.splits(
    (train, test), batch_size=3, device=0)</p>2021-12-31</li><br/><li><span>ifelse</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>å­¦ä¹ æ‰“å¡</p>2023-12-12</li><br/><li><span>John(æ˜“ç­‹)</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>æœ€åä¸€å—ä»£ç è·‘å‡ºé”™
ç‰ˆæœ¬ torchtext
print(torchtext.__version__)  # 0.13.1
å‡ºé”™çš„ä»£ç 
for epoch in range(n_epochs):
    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)

é”™è¯¯log
training...:   0%|          | 0&#47;2969 [00:00&lt;?, ?it&#47;s]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-23-3dd500bd7dd1&gt; in &lt;module&gt;
      8 
      9 for epoch in range(n_epochs):
---&gt; 10     train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)

      7     epoch_losses = []
      8     epoch_accs = []
----&gt; 9     for batch in tqdm.tqdm(dataloader, desc=&#39;training...&#39;, file=sys.stdout):
     10         (label, ids, length) = batch

~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;tqdm&#47;std.py in __iter__(self)
   1127 
   1128         try:
-&gt; 1129             for obj in iterable:

~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;dataloader.py in __next__(self)
    680                 self._reset()  # type: ignore[call-arg]
--&gt; 681             data = self._next_data()
    682             self._num_yielded += 1

~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;dataloader.py in _next_data(self)
    720         index = self._next_index()  # may raise StopIteration
--&gt; 721         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    722         if self._pin_memory:
~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;_utils&#47;fetch.py in fetch(self, possibly_batched_index)
     51             data = self.dataset[possibly_batched_index]
---&gt; 52         return self.collate_fn(data)

&lt;ipython-input-12-6fc8a353bed8&gt; in collate_batch(batch)
     10         length_list.append(len(processed_text))
---&gt; 11         text_list.append((processed_text + max_length)[:max_length])

TypeError: can only concatenate list (not &quot;int&quot;) to list</p>2022-09-13</li><br/><li><span>John(æ˜“ç­‹)</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>torchtext.datasets.IMDB æ”¹ä¸ºå¦‚ä¸‹å¯ä»¥æ­£ç¡®è¿è¡Œ

# pip install torchtext
# pip install torchdata
import torchtext
# è¯»å–IMDBæ•°æ®é›†
train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)
train_iter = iter(train_iter)
next(train_iter)</p>2022-09-10</li><br/><li><span>hallo128</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>torchtextè¯´æ˜æ–‡æ¡£ï¼šhttps:&#47;&#47;pytorch.org&#47;text&#47;stable&#47;index.html</p>2022-06-30</li><br/>
</ul>