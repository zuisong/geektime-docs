ä½ å¥½ï¼Œæˆ‘æ˜¯æ–¹è¿œã€‚

æ¬¢è¿æ¥è·Ÿæˆ‘ä¸€èµ·å­¦ä¹ æƒ…æ„Ÿåˆ†æï¼Œä»Šå¤©æˆ‘ä»¬è¦è®²çš„å°±æ˜¯æœºå™¨å­¦ä¹ é‡Œçš„æ–‡æœ¬æƒ…æ„Ÿåˆ†æã€‚æ–‡æœ¬æƒ…æ„Ÿåˆ†æåˆå«åšè§‚ç‚¹æå–ã€ä¸»é¢˜åˆ†æã€å€¾å‘æ€§åˆ†æç­‰ã€‚å…‰è¯´æ¦‚å¿µï¼Œä½ å¯èƒ½ä¼šè§‰å¾—æœ‰äº›æŠ½è±¡ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹ä¸€ä¸ªç”Ÿæ´»ä¸­çš„åº”ç”¨ï¼Œä½ ä¸€çœ‹å°±èƒ½æ˜ç™½äº†ã€‚

æ¯”æ–¹è¯´æˆ‘ä»¬åœ¨è´­ç‰©ç½‘ç«™ä¸Šé€‰è´­ä¸€æ¬¾å•†å“æ—¶ï¼Œé¦–å…ˆä¼šç¿»é˜…ä¸€ä¸‹å•†å“è¯„ä»·ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä¸­å·®è¯„ã€‚è¿™äº›è¯„è®ºä¿¡æ¯è¡¨è¾¾äº†äººä»¬çš„å„ç§æƒ…æ„Ÿè‰²å½©å’Œæƒ…æ„Ÿå€¾å‘æ€§ï¼Œå¦‚å–œã€æ€’ã€å“€ã€ä¹å’Œæ‰¹è¯„ã€èµæ‰¬ç­‰ã€‚åƒè¿™æ ·æ ¹æ®è¯„ä»·æ–‡æœ¬ï¼Œç”±è®¡ç®—æœºè‡ªåŠ¨åŒºåˆ†è¯„ä»·å±äºå¥½è¯„ã€ä¸­è¯„æˆ–è€…è¯´å·®è¯„ï¼ŒèƒŒåç”¨åˆ°çš„æŠ€æœ¯å°±æ˜¯æƒ…æ„Ÿåˆ†æã€‚

å¦‚æœä½ è¿›ä¸€æ­¥è§‚å¯Ÿï¼Œè¿˜ä¼šå‘ç°ï¼Œåœ¨å¥½è¯„å·®è¯„çš„ä¸Šæ–¹è¿˜æœ‰ä¸€äº›æ ‡ç­¾ï¼Œæ¯”å¦‚â€œå£°éŸ³å¤§å°åˆé€‚â€ã€â€œè¿æ¥é€Ÿåº¦å¿«â€ã€â€œå”®åæ€åº¦å¾ˆå¥½â€ç­‰ã€‚è¿™äº›æ ‡ç­¾å…¶å®ä¹Ÿæ˜¯è®¡ç®—æœºæ ¹æ®æ–‡æœ¬ï¼Œè‡ªåŠ¨æå–çš„ä¸»é¢˜æˆ–è€…è§‚ç‚¹ã€‚

![](https://static001.geekbang.org/resource/image/ef/6f/ef69caa72565c50d98b63e20f499ea6f.jpg?wh=2572x2473)

æƒ…æ„Ÿåˆ†æçš„å¿«é€Ÿå‘å±•å¾—ç›Šäºç¤¾äº¤åª’ä½“çš„å…´èµ·ï¼Œè‡ª2000å¹´åˆä»¥æ¥ï¼Œæƒ…æ„Ÿåˆ†æå·²ç»æˆé•¿ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­æœ€æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸä¹‹ä¸€ï¼Œå®ƒä¹Ÿè¢«å¹¿æ³›åº”ç”¨åœ¨ä¸ªæ€§åŒ–æ¨èã€å•†ä¸šå†³ç­–ã€èˆ†æƒ…ç›‘æ§ç­‰æ–¹é¢ã€‚

ä»Šå¤©è¿™èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°†å®Œæˆä¸€ä¸ªæƒ…æ„Ÿåˆ†æé¡¹ç›®ï¼Œä¸€èµ·æ¥å¯¹å½±è¯„æ–‡æœ¬åšåˆ†æã€‚

## æ•°æ®å‡†å¤‡

ç°åœ¨æˆ‘ä»¬æ‰‹ä¸­æœ‰ä¸€æ‰¹å½±è¯„æ•°æ®ï¼ˆIMDBæ•°æ®é›†ï¼‰ï¼Œå½±è¯„è¢«åˆ†ä¸ºä¸¤ç±»ï¼šæ­£é¢è¯„ä»·ä¸è´Ÿé¢è¯„ä»·ã€‚æˆ‘ä»¬éœ€è¦è®­ç»ƒä¸€ä¸ªæƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œå¯¹å½±è¯„æ–‡æœ¬è¿›è¡Œåˆ†ç±»ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ21ï¼‰</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg" width="30px"><span>æé›„</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>å¦‚æœæ˜¯ä½¿ç”¨torchtext==0.8.1ä»¥ä¸‹çš„ç‰ˆæœ¬å»ºè®®çœ‹å®˜ç½‘æ–‡æ¡£ï¼š
https:&#47;&#47;pytorch.org&#47;text&#47;0.8.1&#47;datasets.html</div>2021-12-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/73/0b/0f918fa2.jpg" width="30px"><span>å¿«ä¹å°å¤œæ›²</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length.to(&#39;cpu&#39;), batch_first=True,  enforce_sorted=False)
è¿™é‡Œlengthå¿…é¡»è½¬ä¸ºæˆcpuï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚</div>2022-01-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/e1/48/3a981e55.jpg" width="30px"><span>æ—äºç¿”</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>LSTMæ¨¡å‹å®šä¹‰ä¸­:
 if self.lstm.bidirectional:
            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))
è¿™é‡Œä¸å¤ªç†è§£ï¼Œæœ€åè¿æ¥çš„ä¸æ˜¯æœ€åä¸€å±‚hiddençš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥å’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥å˜›ï¼Ÿå¦å¤–dimä¸ºä»€ä¹ˆä¼šæ˜¯-1ã€‚</div>2021-12-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg" width="30px"><span>hallo128</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>torchtextå¯ä»¥æ”¯æŒä¸­æ–‡åˆ†è¯å—</div>2022-06-30</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqOMHZ2zMTQty2ToyS9Hc6lZQ1ibzwEnW2psEhyvZ1vNdh9kgermDycCJtp6xX725u1fpsH8CFe6vg/132" width="30px"><span>Geek_a82ba7</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ4ï¼‰<div>import torchtext
train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)
next(train_iter)
&#39;MapperIterDataPipe&#39; object is not an iterator
ä¸ºä»€ä¹ˆè¯´è¿™ä¸ªä¸æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼ŒæŸ¥äº†å¾ˆå¤šèµ„æ–™éƒ½æ²¡æœ‰è§£å†³ï¼Œè€å¸ˆèƒ½å›ç­”ä¸€ä¸‹å—ï¼Ÿ
c</div>2022-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/30/46/19f203cc.jpg" width="30px"><span>Saraié’éœ</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æ–¹è€å¸ˆï¼Œæˆ‘è¯•äº†ä»£ç ï¼Œè¾“å‡ºæ˜¾ç¤ºï¼š
&quot;\nè¾“å‡ºï¼š[&#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;an&#39;, &#39;example&#39;, &#39;!&#39;]\n&quot;
æ˜¯æ–­è¡Œå‡ºç°é—®é¢˜äº†å—ï¼Ÿ
è°¢è°¢ï¼</div>2022-06-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2d/76/60/4be280d7.jpg" width="30px"><span>Ringcoo</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆå¥½ï¼Œè¯·é—®
packed_output, (hidden, cell) = self.lstm(packed_embedded)       
 output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)
ä¸ºä»€ä¹ˆä»lstmå±‚å‡ºæ¥ä»¥å ä¼šæœ‰(hidden, cell) ï¼Œæˆ‘ä¸å¤ªç†è§£ï¼Œä»¥åŠä¸ºä»€ä¹ˆlstmå±‚å‡ºæ¥ä»¥åè¿˜æœ‰åœ¨è¿›RNNå±‚ï¼Œåº•ä¸‹çš„æµç¨‹å›¾ä¸Šæ˜æ˜æ˜¯lstmå±‚å‡ºæ¥ä»¥åè¿›å…¥äº†fcå…¨è¿æ¥å±‚ã€‚éº»çƒ¦æ‚¨è®²è§£ä¸€ä¸‹</div>2022-05-01</li><br/><li><img src="" width="30px"><span>èµµå¿ƒç¿</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è¯·é—®ä½¿ç”¨çš„torchtextæ˜¯å“ªä¸ªç‰ˆæœ¬çš„å‘¢ï¼Ÿ
</div>2022-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg" width="30px"><span>æé›„</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è¯·é—®è¿™æ˜¯é‚£ä¸ªç‰ˆæœ¬çš„torchtext</div>2021-12-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/50/34/172342fd.jpg" width="30px"><span>å´åä¸€</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>def predict_sentiment(text, model, tokenizer, vocab, device):
    max_length = 256
    pad = text_pipeline(&#39;&lt;pad&gt;&#39;)
    processed_text = text_pipeline(text)[:max_length]
    ids, length = [],[]
    ids.append((processed_text+pad*max_length)[:max_length])
    length.append(len(processed_text))
    ids_t,length_t = torch.tensor(ids, dtype = torch.int64), torch.tensor(length, dtype = torch.int64)
    print(ids_t,length_t)
    ids_t.to(device)
    length_t.to(device)
    pred = model(ids_t,length_t)
    return pred
vocab_size = len(vocab)
embedding_dim = 300
hidden_dim = 300
output_dim = 2
n_layers = 2
bidirectional = True
dropout_rate = 0.5
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)
model = model.to(device)
model.load_state_dict(torch.load(&quot;.&#47;lstm.pt&quot;))
model.eval()
text = &quot;This film is terrible!&quot;
pred = predict_sentiment(text, model, tokenizer, vocab, device)
&#39;&#39;&#39;
è¾“å‡ºï¼š(&#39;neg&#39;, 0.8874172568321228)
&#39;&#39;&#39;
tipsï¼šä¸€å®šè¦å®‰è£…torchtext ç¨å¾®æ–°ç‚¹çš„ç‰ˆæœ¬ï¼Œ0.6.0 IDMBæ•°æ®setè·‘æŠ¥é”™ï¼Œä¼šä¸€ç›´è¦æ±‚ç”¨tensorflow v1 ç‰ˆæœ¬å…¼å®¹</div>2022-06-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg" width="30px"><span>æé›„</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>import torch
import torchtext
torchtext.__version__
dir(torchtext)
from torchtext import data
dir(data)

# è¯»å–IMDBæ•°æ®é›†


# è¯»å–IMDBæ•°æ®é›†

TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)
LABEL = data.Field(sequential=False)

# make splits for data åˆ’åˆ†æ•°æ®
train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL)

# build the vocabulary
from torchtext.vocab import GloVe
TEXT.build_vocab(train, vectors=GloVe(name=&#39;6B&#39;, dim=300))
LABEL.build_vocab(train)

# make iterator for splits
train_iter, test_iter = data.BucketIterator.splits(
    (train, test), batch_size=3, device=0)</div>2021-12-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>å­¦ä¹ æ‰“å¡</div>2023-12-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg" width="30px"><span>John(æ˜“ç­‹)</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>æœ€åä¸€å—ä»£ç è·‘å‡ºé”™
ç‰ˆæœ¬ torchtext
print(torchtext.__version__)  # 0.13.1
å‡ºé”™çš„ä»£ç 
for epoch in range(n_epochs):
    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)

é”™è¯¯log
training...:   0%|          | 0&#47;2969 [00:00&lt;?, ?it&#47;s]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-23-3dd500bd7dd1&gt; in &lt;module&gt;
      8 
      9 for epoch in range(n_epochs):
---&gt; 10     train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)

      7     epoch_losses = []
      8     epoch_accs = []
----&gt; 9     for batch in tqdm.tqdm(dataloader, desc=&#39;training...&#39;, file=sys.stdout):
     10         (label, ids, length) = batch

~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;tqdm&#47;std.py in __iter__(self)
   1127 
   1128         try:
-&gt; 1129             for obj in iterable:

~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;dataloader.py in __next__(self)
    680                 self._reset()  # type: ignore[call-arg]
--&gt; 681             data = self._next_data()
    682             self._num_yielded += 1

~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;dataloader.py in _next_data(self)
    720         index = self._next_index()  # may raise StopIteration
--&gt; 721         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    722         if self._pin_memory:
~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;_utils&#47;fetch.py in fetch(self, possibly_batched_index)
     51             data = self.dataset[possibly_batched_index]
---&gt; 52         return self.collate_fn(data)

&lt;ipython-input-12-6fc8a353bed8&gt; in collate_batch(batch)
     10         length_list.append(len(processed_text))
---&gt; 11         text_list.append((processed_text + max_length)[:max_length])

TypeError: can only concatenate list (not &quot;int&quot;) to list</div>2022-09-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg" width="30px"><span>John(æ˜“ç­‹)</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>torchtext.datasets.IMDB æ”¹ä¸ºå¦‚ä¸‹å¯ä»¥æ­£ç¡®è¿è¡Œ

# pip install torchtext
# pip install torchdata
import torchtext
# è¯»å–IMDBæ•°æ®é›†
train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)
train_iter = iter(train_iter)
next(train_iter)</div>2022-09-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg" width="30px"><span>hallo128</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>torchtextè¯´æ˜æ–‡æ¡£ï¼šhttps:&#47;&#47;pytorch.org&#47;text&#47;stable&#47;index.html</div>2022-06-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg" width="30px"><span>äºšæ—</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>ç”¨è‡ªå·±çš„ç”µè„‘è®­ç»ƒä¸€ä¸ªç«¯åˆèŠ‚ï¼Œç»“æœå¦‚ä¸‹ï¼š
(&#39;neg&#39;, 0.9985383749008179)</div>2022-06-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg" width="30px"><span>äºšæ—</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>import torch
import torchtext

from LSTM import LSTM

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

def predict_sentiment(text, model, tokenizer, vocab, device):
    tokens = tokenizer(text)
    ids = [vocab[t] for t in tokens]
    length = torch.LongTensor([len(ids)])
    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)
    prediction = model(tensor, length).squeeze(dim=0)
    probability = torch.softmax(prediction, dim=-1)
    predicted_class = prediction.argmax(dim=-1).item()
    predicted_probability = probability[predicted_class].item()
    predicted_class_title = [&#39;neg&#39;, &#39;pos&#39;]
    return predicted_class_title[predicted_class], predicted_probability

if __name__ == &quot;__main__&quot;:
    text = &quot;This film is terrible!&quot;

    train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)
    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = torchtext.data.utils.get_tokenizer(&#39;basic_english&#39;)


    # æ„å»ºè¯æ±‡è¡¨
    def yield_tokens(data_iter):
        for _, text in data_iter:
            yield tokenizer(text)


    vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_iter), specials=[&quot;&lt;pad&gt;&quot;, &quot;&lt;unk&gt;&quot;])
    vocab.set_default_index(vocab[&quot;&lt;unk&gt;&quot;])

    # åŠ è½½æ¨¡å‹
    vocab_size = len(vocab)
    embedding_dim = 300
    hidden_dim = 300
    output_dim = 2
    n_layers = 2
    bidirectional = True
    dropout_rate = 0.5
    model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)
    model.load_state_dict(torch.load(&#39;.&#47;lstm.pt&#39;))
    model.to(device)
    model.eval()
    print(predict_sentiment(text, model, tokenizer, vocab, device))

å‚è€ƒï¼š
https:&#47;&#47;github.com&#47;bentrevett&#47;pytorch-sentiment-analysis&#47;blob&#47;master&#47;2_lstm.ipynb</div>2022-06-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg" width="30px"><span>äºšæ—</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>import torchtext
train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)
print(next(iter(train_iter)))
åœ¨PyCharmé‡Œé¢è¿™æ ·è·‘æ²¡é—®é¢˜ï¼ŒJupyterLabè¿™æ ·å°±è·‘ä¸åŠ¨äº†</div>2022-06-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg" width="30px"><span>æé›„</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>æˆ‘çš„ç‰ˆæœ¬å¤ªä½äº†
</div>2021-12-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg" width="30px"><span>æé›„</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>æˆ‘å®‰è£…äº†torchtext==0.6è·Ÿè€å¸ˆçš„ç‰ˆæœ¬ä¼°è®¡ä¸ä¸€è‡´
</div>2021-12-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/8c/1d/4641c012.jpg" width="30px"><span>reatiny</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>æˆ‘æ„Ÿè§‰æ¨¡å‹éƒ¨åˆ†æ•°æ®æµåŠ¨ä¸å¤ªæ˜ç™½ï¼Œå»ºè®®å¤šä¸€ç‚¹è§£é‡Š</div>2021-12-04</li><br/>
</ul>