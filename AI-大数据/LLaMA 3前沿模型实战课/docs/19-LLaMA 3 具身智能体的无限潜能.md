你好，我是 Tyler！

今天是我们这一季专栏的最后一节课！作为本季的收官之作，不妨让我们大胆预测一下未来的技术趋势。虽然说是预测，但这些趋势已经在当下逐步显现。在上一节课中，我们探讨了 LLaMA 3.2 Vision 多模态模型的特性及其实际应用，这背后揭示了一个重要趋势：大模型技术正在从语言单一锚点逐步扩展到多模态与空间智能，更进一步延伸到动作智能（Action Intelligence）。

### 为什么说“换锚”是大势所趋？

理解这一点，我们需要从当前大模型技术的发展瓶颈谈起。过去几年，训练大语言模型（LLM）的关键路径是通过不断扩展数据量和模型参数来提升性能。然而，如今这条路径正在遇到两个实际限制。

**首先，高质量语言数据的上限，** 训练大模型需要大量高质量的文本数据，如学术论文、书籍、新闻等。这些数据的“富矿”已经被大规模开发，进一步倍增数据量变得困难。

**此外，Token 增长的边际收益递减，** 即便能继续扩大数据规模，模型性能的提升也在趋缓，逐步呈现边际收益递减的趋势。这种情况让构建具身智能（Embodied Intelligence）或通用人工智能（AGI）的愿景显得更加遥远。

面对这个瓶颈，我们有两个可能的解决办法。

#### 方法一： **继续依靠合成数据**

这一策略的核心是通过生成器模型扩展训练数据，比如利用现有语言数据生成更多样的合成数据。这种方法可以延续当前的路径，但问题在于：

- 合成数据的质量和真实性可能不足，容易引入偏差。

- 对突破数据规模限制的帮助有限，只能暂时缓解瓶颈。


#### 方法二： **“换船”——转向多模态与空间智能**

相比依赖语言单模态，这种方法更具潜力。通过引入多模态数据，如图像、视频、音频、甚至 3D 空间数据，模型可以具备更加全面的感知能力。比如：

- **LLaMA 3.2 Vision** 已经结合视觉和语言输入，可以理解复杂的图文关系。

- 这类模型让智能系统在现实场景中的应用能力显著增强，比如在自动驾驶、AR/VR 和智能机器人等领域。


这种转向不仅扩展了模型的输入维度，还为大模型打开了全新的能力边界，能更好地构建对现实世界的全面理解。

总结来说， **“换锚”** 是大模型技术发展的必然趋势。从单一语言模型转向多模态和空间智能，不仅是应对数据瓶颈的必要选择，也是构建下一代智能系统的关键路径。

### 什么是 VLA 模型？

**VLA（Vision-Language-Action Model，即视觉-语言-动作模型）** 是当前技术发展的前沿领域。它的核心目标是结合视觉、语言与动作模态，打造具备综合感知、语言推理以及行动能力的智能体。

VLA 模型不仅需要感知和理解（Vision 和 Language），还需要能够基于多模态信息完成 **物理交互**（Action）。例如，现实场景理解和动态任务执行。

1. **现实场景理解**

VLA模型通过视觉捕捉场景信息，用语言进行描述或回答问题。例如，它可以在图像中识别物体、理解其语义关系并生成符合上下文的回应。这种能力使其能够处理多模态输入，在 **指令跟随任务** 等具身智能应用中表现出色。

2. **动态任务执行**

VLA模型具有执行复杂任务的能力，比如物体抓取、工具操作，甚至在虚拟或物理环境中完成一系列连续动作。它依赖于 **视觉编码器**（视觉感知）、 **语言编码器**（语言理解）和 **动作解码器**（任务执行）的协同工作，形成从感知到行动的完整链路。

3. **多模态数据融合**

通过整合视觉、语言和动作三大模态，VLA模型展现出比单一模态模型更强的场景推理和任务泛化能力。这种“端到端”多模态融合方式被认为是下一代智能系统的关键架构，特别是在需要跨模态协作的复杂场景中表现出色。

4. **自动驾驶领域的突破**

VLA模型在自动驾驶领域得到了突破性应用。例如，谷歌 Waymo 推出的多模态自动驾驶模型 **EMMA**，基于 VLA 技术实现了对动态交通场景的理解和实时决策。这标志着 VLA 模型的多模态能力正在改变智能驾驶技术的未来。

5. **机器人控制中的潜力**

早期的 VLA 模型已在机器人控制领域展现出巨大潜力。由谷歌 DeepMind 推出的机器人控制系统，利用 VLA 技术完成了如物体操作、路径规划等复杂任务，证明了其在自动化操作中的应用价值。

6. **3D-VLA模型的扩展**

**3D-VLA模型** 是 VLA 技术的高级版本，它通过整合三维空间信息，增强了模型在 **三维场景推理** 与 **动作规划** 中的能力。相比于传统的二维视觉语言模型，3D-VLA 能更好地理解物理世界，为机器人导航、虚拟现实（VR）和增强现实（AR）等领域提供了强大的技术支持。

VLA模型是多模态技术发展的重要里程碑。通过整合视觉、语言和动作模态，它突破了单模态模型的局限性，实现了从感知到执行的跨模态闭环能力。这种技术不仅扩展了模型的应用场景，还为智能体在复杂环境中的全面理解与交互提供了技术支撑。无论是自动驾驶、机器人控制，还是三维场景推理，VLA 模型正在推动智能系统向着更高效、更智能的方向演进，成为未来人工智能发展的重要基石。

### OpenVLA：开源的视觉-语言-动作模型

**OpenVLA** 是一项多模态人工智能领域的重要突破。它是一个开源的 **视觉-语言-动作模型（Vision-Language-Action Model, VLA）**，拥有 70 亿参数，结合了 **LLaMA 2** 的强大语言处理能力和先进的视觉编码器，实现了视觉数据与语言信息之间的高效交互。这种融合使得模型能够感知、理解和执行任务，为构建智能体提供了完整的功能链路。

![图片](https://static001.geekbang.org/resource/image/eb/fe/eb7f96fa56350dc5192a905f65eb0ffe.png?wh=1046x836)

### 主要特点

1. **参数规模与模型基础**：OpenVLA 基于 LLaMA 2 进行了微调，拥有 70 亿参数，是一个专为多模态任务设计的强大模型。它不仅继承了 LLaMA 2 的语言理解能力，还通过引入视觉编码器增强了视觉与语言模态的协同处理能力。

2. **视觉与语言的高效结合**：OpenVLA 在视觉数据和语言之间建立了深度联系。例如，它可以：
   1. 分析图像并生成相关的文本描述。

   2. 通过语言指令，识别图像中的目标并完成任务规划。

   3. 在复杂场景中通过多模态数据（如图像和文字）进行推理和决策。
3. **动作生成能力**：除了视觉和语言理解，OpenVLA 还具备动作规划和执行能力。这让它可以直接应用于机器人控制、虚拟环境操作等需要物理交互的场景。

4. **开源性与可扩展性**：OpenVLA 的开源特性使其脱颖而出。研究人员和开发人员可以轻松访问模型，构建自己的多模态解决方案。这种开放性加速了技术的传播和应用，使 OpenVLA 成为推动多模态人工智能发展的重要工具。


```bash
$ pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt

```

### 技术实现示例

以下是一个使用 OpenVLA 进行动作预测和执行的 Python 示例代码：

```plain
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
import torch

# 加载 OpenVLA 模型和处理器
processor = AutoProcessor.from_pretrained("openvla/openvla-7b", trust_remote_code=True)
vla = AutoModelForVision2Seq.from_pretrained(
    "openvla/openvla-7b",
    attn_implementation="flash_attention_2",  # [可选] 使用高效注意力机制
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True
).to("cuda:0")

# 输入图像与指令
image = get_from_camera()  # 获取实际图像
prompt = "In: What action should the robot take to {<INSTRUCTION>}?\nOut:"

# 执行预测
inputs = processor(prompt, image).to("cuda:0", dtype=torch.bfloat16)
action = vla.predict_action(**inputs, unnorm_key="bridge_orig", do_sample=False)

# 生成动作并执行
robot.act(action)

```

这个示例展示了 OpenVLA 如何将视觉信息与语言指令结合，并生成适合当前场景的动作决策。

OpenVLA 作为一个开源视觉-语言-动作模型，体现了人工智能领域多模态技术的最新进展。通过整合 LLaMA 2 的语言处理能力与先进的视觉编码器，OpenVLA 为多模态理解和任务执行提供了创新性的解决方案。它的开源性和强大的多模态能力，使其成为研究人员和开发者关注的焦点，也是推动智能体技术发展的关键平台。

### 总结

学到这里，本季的内容即将告一段落。在这个快速变化的时代，大模型技术正以惊人的速度向前发展。这一领域的进步不仅延续了我们在第一季中提到的几个关键趋势，还在持续深化，逐渐进入技术和应用的深水区。希望你能够保持关注，跟上这些前沿技术的步伐。

首先， **RAG（Retrieval-Augmented Generation）和AI搜索** 已经成为技术创新的重要方向。通过结合检索与生成能力，这些技术正在重新定义信息获取和处理的方式，使得复杂问题的解答和精准信息的提供更加高效，知识驱动的AI正在迈向更具智能化的阶段。

其次， **多步推理技术以及其他后推理技术** 正在逐步攻克超级复杂的问题。这些技术使得模型可以在信息不完全、问题条件多变的情况下，动态调整解题策略，展现出更接近人类思维的推理能力。这不仅推动了AI应用的广度和深度，还为解决跨学科、跨领域的问题提供了可能性。

最后， **多模态技术** 的进展令人瞩目。它已经不再局限于虚拟空间的应用，在本节课中，它进一步扩展到了物理空间，涵盖机器人控制、虚拟与现实交互等多个领域。这标志着AI技术正在从信息处理走向实际物理场景的全面赋能，将为工业自动化、智能家居、医疗等行业带来深远的影响。

这些趋势无疑都是令人振奋的，但同时也对技术开发者和研究人员提出了更高的要求。希望每一位关注和学习这些技术的朋友，都能抓住时代的机遇，深入理解这些前沿技术的本质，不断提升自己的能力，不掉队，不迷失。更多的心里话留在结束语中，我们在那接着聊。