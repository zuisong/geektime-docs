ä½ å¥½ï¼Œæˆ‘æ˜¯Tylerã€‚ä»Šå¤©æˆ‘ä»¬æ­£å¼å¼€å§‹å­¦ä¹ LLaMA 3çš„èƒ½åŠ›æ¨¡å‹ã€‚

è¿‡å»çš„ä¸€å¹´ä¸­ï¼Œå¤§æ¨¡å‹æŠ€æœ¯å¾—åˆ°äº†å¹¿æ³›è®¤å¯ï¼Œå…¨è¡Œä¸šå¯¹å¤§æ¨¡å‹çš„æŠ•å…¥ä¹Ÿåœ¨ä¸æ–­å¢åŠ ã€‚å¼€æºç¤¾åŒºæ¶Œç°äº†è®¸å¤šä¼˜ç§€çš„æ¨¡å‹å’Œæ¡†æ¶ï¼Œæ¨åŠ¨äº†å¤§æ¨¡å‹æŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚åœ¨è¿™ä¸€å¹´çš„æ—¶é—´é‡Œï¼ŒLLaMA ç³»åˆ—æ¨¡å‹ä¹Ÿç»å†äº†å¿«é€Ÿçš„å‘å±•ï¼Œä» LLaMA 2 åˆ° LLaMA 3ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ€§èƒ½å’Œåº”ç”¨ä¸Šçš„æ˜¾è‘—æå‡ã€‚

æœ¬å­£ä¸“æ ä¸­ï¼Œæˆ‘å°†é‡‡ç”¨â€œLearn by doingâ€çš„æ–¹æ³•ï¼Œé€šè¿‡ç®€æ´çš„ç¤ºä¾‹ï¼Œæ·±å…¥å‰–æå¤§æ¨¡å‹æŠ€æœ¯çš„æœ¬è´¨ã€‚æˆ‘ä»¬å°†æ¢è®¨LLaMA 3çš„èƒ½åŠ›æ¨¡å‹ï¼Œè¯¦ç»†è§£æå¤§æ¨¡å‹æŠ€æœ¯çš„å„ä¸ªæ–¹é¢ï¼Œå¹¶æ·±å…¥åˆ°ä½ åœ¨ä½¿ç”¨LLaMA 3è¿‡ç¨‹ä¸­ä¼šé‡åˆ°çš„å„ç§ç»†èŠ‚ã€‚

åœ¨ç¬¬ä¸€è®²ä¸­ï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»LLaMA 3æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”å¯¹è¯ç”Ÿæˆï¼Œå¹¶å±•ç¤ºå®ƒåœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚

## åŸºæœ¬æ“ä½œï¼šç”Ÿæˆå†…å®¹

é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹LLaMA 3çš„æ ¸å¿ƒèƒ½åŠ›ã€‚LLaMA 3ä¸»è¦ä¾èµ–äºNext Token Predictionï¼ˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼‰æœºåˆ¶ï¼Œé€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ¥ç”Ÿæˆè¿è´¯çš„å¯¹è¯ã€‚è¿™ç§æœºåˆ¶åŸºäºæµ·é‡æ–‡æœ¬æ•°æ®çš„è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰è¯­è¨€çš„æ¨¡å¼å’Œè§„å¾‹ï¼Œç”Ÿæˆç¬¦åˆä¸Šä¸‹æ–‡é€»è¾‘çš„æ–‡æœ¬å†…å®¹ã€‚

### Next Token Prediction

Next Token Predictionæ˜¯å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„åŸºç¡€ã€‚æ¨¡å‹å¤„ç†è¾“å…¥æ–‡æœ¬çš„æ­¥éª¤å¦‚ä¸‹ï¼š

1. **æ ‡è®°åŒ–ï¼š**é¦–å…ˆï¼Œå¤§æ¨¡å‹å°†è¾“å…¥æ–‡æœ¬åˆ†è§£æˆä¸€ç³»åˆ—çš„ tokenï¼ˆè¯æˆ–å­è¯ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¥å­â€œè¯·è§£é‡Šä¸€ä¸‹é»‘æ´çš„å½¢æˆâ€å¯èƒ½è¢«åˆ†è§£ä¸ºä»¥ä¸‹ tokenï¼š

```plain
["è¯·", "è§£é‡Š", "ä¸€", "ä¸‹", "é»‘", "æ´", "çš„", "å½¢", "æˆ"]
```

2. **æ–‡å­—è¡¨å¾ï¼š**æ¥ä¸‹æ¥ï¼Œå°†è¿™äº› token è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å€¼å½¢å¼ï¼ˆé€šå¸¸æ˜¯åµŒå…¥å‘é‡ï¼‰ã€‚
3. **æ¦‚ç‡é¢„æµ‹ï¼š**å¤§æ¨¡å‹ä¼šæ ¹æ®å½“å‰çš„è¾“å…¥åºåˆ—è®¡ç®—ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¿™äº›æ¦‚ç‡åˆ†å¸ƒè¡¨ç¤ºä¸‹ä¸€ä¸ªè¯çš„å¯èƒ½æ€§ï¼Œä¾‹å¦‚ï¼š

```plain
{"é»‘æ´": 0.1, "å½¢æˆ": 0.05, "æ˜¯": 0.2, "ç”±äº": 0.15, ...}
```

4. **ç”Ÿæˆæ–‡æœ¬ï¼š**æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©å…·ä½“çš„ä¸€ä¸ªè¯ä½œä¸ºä¸‹ä¸€ä¸ªè¯ã€‚é€‰æ‹©æ–¹å¼å¯ä»¥æ˜¯è´ªå©ªæœç´¢ï¼ˆé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯ï¼‰ã€éšæœºé‡‡æ ·ï¼ˆæ ¹æ®æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ï¼‰æˆ–å…¶ä»–æœç´¢ç­–ç•¥ï¼ˆæ¯”å¦‚Beam Searchï¼‰ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨è´ªå©ªæœç´¢é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯â€œæ˜¯â€ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°å·²ç”Ÿæˆçš„æ–‡æœ¬åºåˆ—ä¸­ï¼Œé‡å¤ä¸Šè¿°æ­¥éª¤ã€‚

```plain
[â€œè¯·â€, â€œè§£é‡Šâ€, â€œä¸€â€, â€œä¸‹â€, â€œé»‘â€, â€œæ´â€, â€œçš„â€, â€œå½¢â€, â€œæˆâ€, â€œæ˜¯â€]ã€‚
```

å› æ­¤ï¼ŒLLaMA 3æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹æ˜¯ä¸€ä¸ªå¾ªç¯ï¼šé€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œå°†å…¶åŠ å…¥åˆ°åºåˆ—ä¸­ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ¥ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ã€‚è¿™ç§å¾ªç¯é€ æˆäº†å¤§é‡çš„æ¨¡å‹æ¨ç†ç®—åŠ›å¼€é”€ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆOpenAIç­‰å…¬å¸åœ¨APIä½¿ç”¨ä¸­æ ¹æ®Tokenè®¡è´¹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå…·ä½“çš„ä»£ç ç¤ºä¾‹æ¥æ¼”ç¤ºLLaMA 3çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ–‡ç« å¼€å¤´ï¼š

> åœ¨ä¸€ä¸ªé˜³å…‰æ˜åªšçš„æ—©æ™¨ï¼ŒAliceå†³å®šå»æ£®æ—é‡Œæ¢é™©ã€‚å¥¹èµ°ç€èµ°ç€ï¼Œçªç„¶å‘ç°äº†ä¸€æ¡å°è·¯ã€‚

æˆ‘ä»¬ä½¿ç”¨LLaMA 3æ¥ç»­å†™è¿™æ®µè¯ï¼š

```python
import torch
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# ä¸‹è½½æ¨¡å‹
cache_dir = './llama_cache'
model_id = snapshot_download("LLM-Research/Meta-Llama-3-8B", cache_dir=cache_dir)
```

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
cache_dir = './llama_cache'
model_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
Â  Â  model_path,
Â  Â  torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
Â  Â  device_map="auto" if torch.cuda.is_available() else None
)

# ç¼–ç è¾“å…¥å¹¶å°†å…¶ç§»è‡³æ¨¡å‹è®¾å¤‡
input_text = "åœ¨ä¸€ä¸ªé˜³å…‰æ˜åªšçš„æ—©æ™¨ï¼ŒAliceå†³å®šå»æ£®æ—é‡Œæ¢é™©ã€‚å¥¹èµ°ç€èµ°ç€ï¼Œçªç„¶å‘ç°äº†ä¸€æ¡å°è·¯ã€‚"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

# ç”Ÿæˆå¹¶è§£ç æ–‡æœ¬
with torch.no_grad():
Â  Â  outputs = model.generate(**inputs)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

è¾“å‡ºç»“æœï¼š

> è¿™æ¡å°è·¯è¢«ä¸¤æ—èŒ‚å¯†çš„æ ‘æœ¨æ©æ˜ ç€ï¼Œä¼¼ä¹é€šå‘æ£®æ—æ·±å¤„ã€‚Aliceå†³å®šæ²¿ç€å°è·¯å‰è¿›ï¼Œæƒ³çœ‹çœ‹å°½å¤´æœ‰ä»€ä¹ˆæƒŠå–œç­‰ç€å¥¹ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒLLaMA 3 æˆåŠŸåœ°ç»­å†™äº†æ–‡ç« å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° LLaMA 3 æµç•…åœ°ç”Ÿæˆäº†å…·æœ‰åˆ›æ„æ€§çš„å†…å®¹ï¼Œè¿™ä¸ªç”Ÿæˆçš„è¿‡ç¨‹å°±æ˜¯åˆšåˆšæåˆ°çš„ Next Token PredictionÂ å¾ªç¯æ‰€å½¢æˆçš„ã€‚

### åœæ­¢æ¡ä»¶

ä½ å¯èƒ½ä¼šé—®ï¼ŒNext Token Prediction çš„æ–¹å¼å²‚ä¸æ˜¯ä¼šä¸€ç›´è¾“å‡ºå†…å®¹ä¸åœæ­¢ï¼Ÿå¾ˆå¥½çš„é—®é¢˜ï¼ç°åœ¨æˆ‘ä»¬æ¥èŠèŠLLaMA 3 **åœæ­¢è¾“å‡ºçš„æ¡ä»¶**ï¼ŒLLaMA 3 çš„è¾“å‡ºç”±ä»¥ä¸‹å‡ ä¸ªå› ç´ æ§åˆ¶ï¼š

- **æœ€å¤§é•¿åº¦ï¼ˆmax\_lengthï¼‰ï¼š**è¿™æ˜¯æœ€å¸¸ç”¨çš„æ§åˆ¶æ–¹å¼ä¹‹ä¸€ã€‚åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼Œä½ å¯ä»¥æŒ‡å®šä¸€ä¸ªæœ€å¤§é•¿åº¦å€¼ã€‚å½“æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦è¾¾åˆ°è¯¥å€¼æ—¶ï¼Œå®ƒå°±ä¼šåœæ­¢è¾“å‡ºã€‚

ä¾‹å¦‚ï¼Œå¦‚æœå°†æœ€å¤§é•¿åº¦è®¾ç½®ä¸º100ï¼Œé‚£ä¹ˆæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æœ€é•¿ä¸ä¼šè¶…è¿‡100ä¸ªè¯ã€‚

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

cache_dir = './llama_cache'
model_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
Â  Â  model_path,
Â  Â  torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
Â  Â  device_map="auto" if torch.cuda.is_available() else None
)

max_length = 50
input_text = "å†™ä¸€é¦–å…³äºçˆ±æƒ…çš„è¯—"
encoded_input = tokenizer(input_text, return_tensors="pt")
output = model.generate(encoded_input.input_ids, max_length=max_length)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

- **å…¶ä»–åœæ­¢æ¡ä»¶ï¼š**ä¸€äº›æ¨¡å‹è¿˜æ”¯æŒå…¶ä»–åœæ­¢æ¡ä»¶ã€‚ä¾‹å¦‚æ£€æµ‹åˆ°é‡å¤çš„æ–‡æœ¬ã€ä½è´¨é‡çš„æ–‡æœ¬ç­‰ã€‚

LLaMA 3 å¯ä»¥æ£€æµ‹åˆ°é‡å¤çš„æ–‡æœ¬å¹¶å°†å…¶è·³è¿‡ã€‚

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

cache_dir = './llama_cache'
model_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
Â  Â  model_path,
Â  Â  torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
Â  Â  device_map="auto" if torch.cuda.is_available() else None
)

repetition_penalty = 1.2
input_text = "å†™ä¸€é¦–å…³äºçˆ±æƒ…çš„è¯—ï¼š"
encoded_input = tokenizer(input_text, return_tensors="pt")
output = model.generate(encoded_input.input_ids, repetition_penalty=repetition_penalty)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

è¿™äº›åœæ­¢æ¡ä»¶ç¡®ä¿æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¸ä¼šæ— é™å»¶é•¿ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®ä¸åŒçš„å‚æ•°æ¥æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚

## æ— çŠ¶æ€åˆ°æœ‰çŠ¶æ€ï¼šå¯¹è¯èƒ½åŠ›

å¯¹è¯ç”Ÿæˆä¸ä»…éœ€è¦ç†è§£ä¸Šä¸‹æ–‡ï¼Œè¿˜å¾—ä¿æŒè¿è´¯æ€§ã€‚**å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè¦è§£å†³çš„é—®é¢˜ï¼Œå°±æ˜¯å¤§æ¨¡å‹æœåŠ¡â€œæ— çŠ¶æ€â€çš„é—®é¢˜ã€‚**ä¸ºäº†è®©â€œæ— çŠ¶æ€â€çš„LLaMA 3æ¨¡å‹å…·å¤‡å¯¹è¯èƒ½åŠ›ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…ˆå‰çš„â€œå†å²ä¼šè¯â€ä½œä¸ºå½“å‰è¾“å…¥çš„ä¸€éƒ¨åˆ†ã€‚è¿™æ ·å¯ä»¥ä¿æŒä¸Šä¸‹æ–‡çš„è¿è´¯æ€§ï¼Œä½¿æ¨¡å‹æˆä¸ºä¸€ä¸ªâ€œæœ‰çŠ¶æ€â€çš„æœåŠ¡ï¼Œä»è€Œå‡†ç¡®åœ°ç”Ÿæˆå“åº”ã€‚

ä»¥ä¸‹æ˜¯å®ç°è¿™ä¸€åŠŸèƒ½çš„ä»£ç ç¤ºä¾‹ï¼š

```python
import torch
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# ä¸‹è½½æ¨¡å‹
cache_dir = './llama_cache'
model_id = snapshot_download("LLM-Research/Meta-Llama-3-8B-Instruct", cache_dir=cache_dir)
```

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

cache_dir = './llama_cache'
model_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B-Instruct'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
Â  Â  model_path,
Â  Â  torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
Â  Â  device_map="auto" if torch.cuda.is_available() else None
)

# åˆå§‹åŒ–å¯¹è¯å†å²
dialogue_history = [
Â  Â  "Customer: Hi, I have an issue with my order.",
Â  Â  "Support: Sure, could you please provide your order number?",
Â  Â  "Customer: Sure, it's #12345.",
Â  Â  "Support: Thank you. Let me check the status for you.",
]

# åˆå¹¶å¯¹è¯å†å²ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²
dialogue_history_text = "\n".join(dialogue_history)

# æ·»åŠ ç”¨æˆ·è¾“å…¥ï¼Œæ¨¡æ‹Ÿå½“å‰å¯¹è¯
user_input = "Customer: Can you please expedite the delivery?"
input_text = dialogue_history_text + "\n" + user_input

# ç”Ÿæˆæ–‡æœ¬
input_ids = tokenizer.encode(input_text, return_tensors="pt")
outputs = model.generate(input_ids, max_length=100)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Generated Response:", generated_text)

```

è¿™æ®µä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LLaMA 3 æ¨¡å‹ç”Ÿæˆå¯¹è¯å“åº”ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰äº†æ¨¡å‹å’Œ tokenizerï¼Œå¹¶åˆå§‹åŒ–äº†ä¸€ä¸ªç®€å•çš„å¯¹è¯å†å²åˆ—è¡¨ã€‚ç„¶åï¼Œå°†å¯¹è¯å†å²è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶æ·»åŠ ç”¨æˆ·çš„å½“å‰è¾“å…¥ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ tokenizer å¯¹è¾“å…¥è¿›è¡Œç¼–ç ï¼Œç„¶åé€šè¿‡æ¨¡å‹ç”Ÿæˆå“åº”æ–‡æœ¬ã€‚æœ€åï¼Œè§£ç ç”Ÿæˆçš„æ–‡æœ¬å¹¶æ‰“å°å‡ºæ¥ã€‚

**è¿™é‡Œä¸ºä½ å¸¦æ¥è¿™èŠ‚è¯¾çš„ç¬¬ä¸€ä¸ªé‡ç‚¹**ï¼Œä½ å¯èƒ½ä¼šå‘ç°ï¼Œä¸Šé¢çš„ç¤ºä¾‹ä½¿ç”¨äº†åŸºç¡€ç‰ˆæ¨¡å‹ï¼Œè€Œä¸‹é¢çš„ç¤ºä¾‹ä½¿ç”¨äº† Instruct ç‰ˆæœ¬çš„æ¨¡å‹ã€‚è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿè¿™æ˜¯å› ä¸ºè¿™ä¸¤ä¸ªæ¨¡å‹çš„ç›®æ ‡ä¸ä¸€æ ·ï¼Œä¸ºäº†æœåŠ¡ä¸åŒçš„è§’è‰²ï¼Œæ¯ä¸ªç‰ˆæœ¬ LLaMA 3 æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®å’Œè®­ç»ƒæ–¹æ³•ä¸Šæœ‰æ‰€ä¸åŒï¼Œé’ˆå¯¹ç‰¹å®šå¯¹è±¡è¿›è¡Œäº†å¾®è°ƒï¼Œæ¯”å¦‚ï¼š

- äººç±»ç”¨æˆ·ï¼šé€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œå“åº”äººç±»æŒ‡ä»¤ã€‚è¿™ç§å¾®è°ƒä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´è‡ªç„¶çš„è¯­è¨€è¾“å…¥ï¼Œæä¾›æ›´å‡†ç¡®å’Œç›¸å…³çš„å›ç­”ã€‚
- æ£€ç´¢ç³»ç»Ÿï¼šç»“åˆæ£€ç´¢ç³»ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ã€‚é€šè¿‡å®æ—¶æ£€ç´¢æœ€æ–°çš„å¤–éƒ¨æ•°æ®ï¼Œæ¨¡å‹å¯ä»¥æä¾›æ›´åŠ å‡†ç¡®å’Œæ—¶æ•ˆæ€§å¼ºçš„å›ç­”ã€‚
- æ™ºèƒ½ä½“ï¼ˆå¤šæ­¥æ¨ç†ï¼‰ï¼šæ¨¡å‹ä¹‹é—´çš„åä½œä¸äº¤äº’ï¼Œé€šè¿‡äº’ç›¸å¾®è°ƒæå‡æ•´ä½“æ™ºèƒ½æ°´å¹³ã€‚ä¸åŒæ¨¡å‹å¯ä»¥ç›¸äº’è¡¥å……ï¼Œå…±åŒå®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

æ­¤å¤–ï¼Œé€‰æ‹©ä¸åŒç‰ˆæœ¬çš„ LLaMA 3 æ¨¡å‹è¿˜éœ€è¦è€ƒè™‘å¤šä¸ªç»´åº¦ã€‚

- åœºæ™¯æ•°æ®ï¼šæ ¹æ®ä¸åŒåº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œå¦‚è¯­è¨€ã€è¡Œä¸šã€æ–‡åŒ–ç­‰ã€‚ä¸åŒé¢†åŸŸçš„æ•°æ®ç‰¹ç‚¹å’Œéœ€æ±‚ä¸åŒï¼Œéœ€è¦é’ˆå¯¹æ€§åœ°è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚
- è¾“å…¥é•¿åº¦ï¼šä¸åŒç‰ˆæœ¬çš„æ¨¡å‹åœ¨è¾“å…¥é•¿åº¦ä¸Šæœ‰æ‰€å·®å¼‚ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®å…·ä½“åº”ç”¨éœ€æ±‚é€‰æ‹©é€‚åˆçš„æ¨¡å‹ç‰ˆæœ¬ã€‚ä¸€äº›åº”ç”¨åœºæ™¯å¯èƒ½éœ€è¦å¤„ç†è¾ƒé•¿çš„è¾“å…¥æ–‡æœ¬ï¼Œå› æ­¤é€‰æ‹©æ”¯æŒè¾ƒé•¿è¾“å…¥çš„æ¨¡å‹ç‰ˆæœ¬æ˜¯å¿…è¦çš„ã€‚
- å‚æ•°æ•ˆç‡ï¼šé€šè¿‡å¾®è°ƒé™ä½æ¨¡å‹çš„å‚æ•°è¦æ±‚ï¼Œä¾‹å¦‚ä½¿ç”¨æ›´å°çš„æ¨¡å‹æ¶æ„æˆ–ä¼˜åŒ–ç®—æ³•ã€‚åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ï¼Œå¯ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ä»¥æ»¡è¶³éœ€æ±‚ã€‚é€šè¿‡åˆç†çš„å‚æ•°é…ç½®ï¼Œå¯ä»¥åœ¨æ€§èƒ½å’Œèµ„æºæ¶ˆè€—ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚
- é‡åŒ–ç¨‹åº¦ï¼šä¸åŒé‡åŒ–ç¨‹åº¦å¸¦æ¥ä¸åŒçš„æ€§èƒ½æå‡å’Œæ•ˆæœä¸‹é™ï¼Œéœ€è¦åœ¨æ€§èƒ½å’Œæ•ˆæœä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚é‡åŒ–å¯ä»¥å‡å°‘æ¨¡å‹çš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œä½†ä¹Ÿå¯èƒ½å½±å“ç”Ÿæˆæ•ˆæœï¼Œå› æ­¤éœ€è¦æ ¹æ®å…·ä½“åº”ç”¨æƒ…å†µè¿›è¡Œæƒè¡¡ã€‚

è¿™äº›è€ƒè™‘å› ç´ åœ¨é€‰æ‹©å’Œä½¿ç”¨ LLaMA 3 æ¨¡å‹æ—¶è‡³å…³é‡è¦ã€‚éšç€åé¢è¯¾ç¨‹çš„æ·±å…¥ï¼Œæˆ‘ä»¬å°†åœ¨ç¤ºä¾‹ä¸­ä¸æ–­å±•å¼€è¿™äº›å†…å®¹ã€‚

åœ¨è§£å†³äº†å¤§æ¨¡å‹çš„â€œæ— çŠ¶æ€â€œé—®é¢˜ä¹‹åï¼Œæˆ‘ä»¬å†æ¥çœ‹å¦ä¸€ä¸ªå¤§æ¨¡å‹çš„å±€é™æ€§ï¼Œé‚£å°±æ˜¯**è®­ç»ƒæ•°æ®æ—¶æ•ˆæ€§çš„é—®é¢˜ã€‚**

## å°é—­åˆ°å¼€æ”¾ï¼šæ£€ç´¢å¢å¼º

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ LLaMA 3 è®­ç»ƒåäº§ç”Ÿçš„æœ€æ–°äº‹å®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ç»“åˆæ£€ç´¢ç³»ç»Ÿï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è·å–æœ€æ–°æˆ–ç‰¹å®šé¢†åŸŸçš„äº‹å®ä¿¡æ¯ã€‚

**ä¸ºäº†è§£å†³ LLaMA 3 æ— æ³•æä¾›æœ€æ–°çš„äº‹å®ä¿¡æ¯çš„é—®é¢˜**ï¼Œæˆ‘ä»¬éœ€è¦ç”¨åˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ã€‚ç®€å•æ¥è¯´ï¼ŒRAG å°±æ˜¯ç»“åˆå¤–éƒ¨çŸ¥è¯†åº“æˆ– API è¿›è¡Œå®æ—¶æ£€ç´¢ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„å†…å®¹é€šè¿‡æç¤ºè¯­è¡¥å……åˆ°æ¨¡å‹çš„è¾“å…¥ä¸­ã€‚

è¿™ç§æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹è¯è¿‡ç¨‹ä¸­è°ƒç”¨å¤–éƒ¨çŸ¥è¯†åº“ï¼Œå¢å¼ºå›ç­”çš„å‡†ç¡®æ€§ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¸¸è§çš„ç¤ºä¾‹å®ç°æ­¥éª¤ï¼š

1. å®šä¹‰æ£€ç´¢æœºåˆ¶ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€ä¸ªé€‚å½“çš„å¤–éƒ¨çŸ¥è¯†åº“æˆ– APIï¼Œç”¨äºå®æ—¶æ£€ç´¢æœ€æ–°çš„äº‹å®ä¿¡æ¯ã€‚å¸¸è§çš„é€‰æ‹©åŒ…æ‹¬æœç´¢å¼•æ“ APIã€æ–°é—» APIã€ä¸“é—¨çš„è¡Œä¸šçŸ¥è¯†åº“ç­‰ã€‚
2. é›†æˆæ£€ç´¢ç³»ç»Ÿï¼šå°†å¤–éƒ¨æ£€ç´¢ç³»ç»Ÿé›†æˆåˆ° LLaMA 3 çš„ç”Ÿæˆæµç¨‹ä¸­ã€‚å½“æ¨¡å‹ç”Ÿæˆåˆæ­¥å“åº”åï¼Œè°ƒç”¨æ£€ç´¢ç³»ç»Ÿè·å–æœ€æ–°çš„ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯æ•´åˆåˆ°æœ€ç»ˆçš„å“åº”ä¸­ã€‚
3. æ›´æ–°ç”Ÿæˆå†…å®¹ï¼šåˆ©ç”¨æ£€ç´¢åˆ°çš„æœ€æ–°ä¿¡æ¯ï¼Œæ›´æ–°å’Œå®Œå–„æ¨¡å‹çš„åˆæ­¥ç”Ÿæˆå†…å®¹ï¼Œç¡®ä¿å›ç­”çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚

ç›¸åº”çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä»£ç ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é›†æˆå¤–éƒ¨çŸ¥è¯†åº“è¿›è¡Œå®æ—¶æ£€ç´¢å’Œä¿¡æ¯æ›´æ–°ã€‚

1. å®šä¹‰æ£€ç´¢æœºåˆ¶ï¼šåœ¨ retrieve\_information å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æœç´¢å¼•æ“APIæ ¹æ®ç»™å®šçš„æŸ¥è¯¢å…³é”®è¯æ£€ç´¢æœ€æ–°çš„ä¿¡æ¯ã€‚
2. é›†æˆæ£€ç´¢ç³»ç»Ÿï¼šåœ¨ generate\_response å‡½æ•°ä¸­ï¼Œé¦–å…ˆä½¿ç”¨ LLaMA 3 ç”Ÿæˆåˆæ­¥å“åº”ï¼Œç„¶åæå–ç”Ÿæˆæ–‡æœ¬ä¸­çš„å…³é”®è¯ï¼Œé€šè¿‡ retrieve\_information å‡½æ•°è·å–ç›¸å…³çš„æœ€æ–°ä¿¡æ¯ã€‚
3. æ›´æ–°ç”Ÿæˆå†…å®¹ï¼šå°†æ£€ç´¢åˆ°çš„ä¿¡æ¯æ•´åˆåˆ°æœ€ç»ˆçš„å“åº”ä¸­ï¼Œç”Ÿæˆæ›´ä¸ºå‡†ç¡®å’Œæ—¶æ•ˆæ€§çš„å›ç­”ã€‚

```python
import requests
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from googleapiclient.discovery import build

cache_dir = './llama_cache'
model_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B-Instruct'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
Â  Â  model_path,
Â  Â  torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
Â  Â  device_map="auto" if torch.cuda.is_available() else None
)

# Google Custom Search APIé…ç½®
API_KEY = 'YOUR_GOOGLE_API_KEY'
SEARCH_ENGINE_ID = 'YOUR_SEARCH_ENGINE_ID'

# æ£€ç´¢ç›¸å…³æ–‡æ¡£çš„å‡½æ•°
def retrieve_documents(query):
Â  Â  try:
Â  Â  Â  Â  service = build("customsearch", "v1", developerKey=API_KEY)
Â  Â  Â  Â  res = service.cse().list(q=query, cx=SEARCH_ENGINE_ID).execute()
Â  Â  Â  Â  results = res.get('items', [])
Â  Â  Â  Â  documents = [item["snippet"] for item in results]
Â  Â  Â  Â  return documents
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"Error retrieving documents: {e}")
Â  Â  Â  Â  return []

# ç”Ÿæˆç­”æ¡ˆçš„å‡½æ•°
def generate_answer(query, documents):
Â  Â  # é™åˆ¶æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡
Â  Â  documents = documents[:3]
Â  Â  context = "\n\n".join(documents) + "\n\nQuestion: " + query + "\nAnswer:"
Â  Â  # ç¼–ç è¾“å…¥
Â  Â  inputs = tokenizer(context, return_tensors="pt", truncation=True, max_length=2048).to(model.device)
Â  Â  # ç”Ÿæˆç­”æ¡ˆ
Â  Â  outputs = model.generate(**inputs, max_length=512)
Â  Â  # è§£ç ç­”æ¡ˆ
Â  Â  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
Â  Â  return answer

# ä¸»å‡½æ•°
def main():
Â  Â  query = "What is the capital of France?"
Â  Â  documents = retrieve_documents(query)
Â  Â  if documents:
Â  Â  Â  Â  answer = generate_answer(query, documents)
Â  Â  Â  Â  print("Question:", query)
Â  Â  Â  Â  print("Answer:", answer)
Â  Â  else:
Â  Â  Â  Â  print("No documents retrieved.")

if __name__ == "__main__":
Â  Â  main()

```

> Question: What is the capital of France?  
> Answer: The capital of France is Paris. It is known for its art, fashion, and culture, and is home to famous landmarks such as the Eiffel Tower and the Louvre Museum.

é€šè¿‡ç»“åˆå¤–éƒ¨çŸ¥è¯†åº“æˆ– API è¿›è¡Œå®æ—¶æ£€ç´¢å’Œä¿¡æ¯æ›´æ–°ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆè§£å†³ LLaMA 3 åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ— æ³•è·å–æœ€æ–°äº‹å®çš„é—®é¢˜ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å›ç­”çš„å‡†ç¡®æ€§ï¼Œè¿˜ç¡®ä¿äº†å†…å®¹çš„æ—¶æ•ˆæ€§ã€‚åœ¨åç»­è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥æ¢è®¨å¦‚ä½•ä½¿ç”¨å‘é‡æ•°æ®å’Œæ··åˆæ£€ç´¢åŠé‡æ’æŠ€æœ¯ï¼Œæ¥æ„å»ºä¸€ä¸ªå®šåˆ¶åŒ–çš„ RAG å¼•æ“ã€‚

åœ¨å¯¹è¯ç”Ÿæˆçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¤šæ™ºèƒ½ä½“æ¶æ„ã€‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¯ä»¥åˆ†æ‹…å¤æ‚ä»»åŠ¡ï¼Œé€šè¿‡ä¸åŒçš„æ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œæä¾›æ›´ä¸“ä¸šã€æ›´é«˜æ•ˆçš„æœåŠ¡ã€‚

## å•ä½“æœåŠ¡åˆ°å¾®æœåŠ¡ï¼šå¤šæ™ºèƒ½ä½“

åœ¨å•æ­¥æ¨ç†æ–¹é¢ï¼ŒLLaMA 3 å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼Œä¾‹å¦‚é•¿å¯¹è¯å’ŒèŒè´£æ··ä¹±å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œå¤šæ™ºèƒ½ä½“æ¶æ„æä¾›äº†ä¸€äº›ä¼˜åŒ–ç­–ç•¥ã€‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰é€šè¿‡å°†å¤šä¸ªæ™ºèƒ½ä½“ç»„ç»‡èµ·æ¥ååŒå·¥ä½œï¼Œå®ç°å¤æ‚ä»»åŠ¡çš„è§£å†³ã€‚

æ¯ä¸ªæ™ºèƒ½ä½“éƒ½æœ‰ç‰¹å®šçš„è§’è‰²å’ŒåŠŸèƒ½ï¼Œé€šè¿‡ç›¸äº’ä¹‹é—´çš„é€šä¿¡å’Œåä½œæ¥è¾¾æˆå…±åŒç›®æ ‡ã€‚åœ¨LLaMA 3çš„åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ä»¥ä¸‹ä¼˜ç‚¹ï¼š

- **æ‹†åˆ†å¯¹è¯**ï¼šå°†å•ä¸€å¯¹è¯æ‹†åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„å¯¹è¯æ®µï¼Œä»¥å‡å°‘å¯¹è¯é•¿åº¦å¯¹æ¨¡å‹æ¨ç†æ€§èƒ½çš„å½±å“ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤„ç†ä¸€ä¸ªé•¿ç¯‡å¯¹è¯æ—¶ï¼Œå¯ä»¥å°†å¯¹è¯åˆ†æˆè‹¥å¹²æ®µï¼Œæ¯æ®µé›†ä¸­è®¨è®ºä¸€ä¸ªå…·ä½“é—®é¢˜ã€‚
- **æ˜ç¡®èŒè´£**ï¼šå½“å•ä¸ªæ¨¡å‹éœ€è¦å¤„ç†è¿‡å¤šé—®é¢˜æ—¶ï¼Œå¯ä»¥é€šè¿‡æ‹†åˆ†è§’è‰²æ¥æ˜ç¡®æ¯ä¸ªæ¨¡å‹çš„èŒè´£ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªå¤æ‚çš„å¯¹è¯ç³»ç»Ÿä¸­ï¼Œå¯ä»¥è®¾ç½®ä¸åŒçš„å­æ¨¡å‹åˆ†åˆ«å¤„ç†ç”¨æˆ·æ„å›¾è¯†åˆ«ã€å¯¹è¯ç®¡ç†å’Œåº”ç­”ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
- **è§’è‰²æ‹†åˆ†**ï¼šé€šè¿‡è§’è‰²æ‹†åˆ†ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡å‹å¤„ç†çš„å†…å®¹å•ä¸€è€Œè¿è´¯ï¼Œä»è€Œæå‡æ•´ä½“å¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ªä¸“é—¨å¤„ç†æŠ€æœ¯é—®é¢˜çš„æ¨¡å‹å’Œä¸€ä¸ªä¸“é—¨å¤„ç†æ—¥å¸¸å¯¹è¯çš„æ¨¡å‹ï¼Œåˆ†åˆ«è´Ÿè´£ä¸åŒç±»å‹çš„å¯¹è¯ã€‚

ä¸ºäº†æ¨¡æ‹Ÿä¸€ä¸ªè®¡ç®—æœºç§‘å­¦å®¶ï¼ˆAgent Aï¼‰å’Œä¸€ä¸ªæ³•å¾‹ä¸“å®¶ï¼ˆAgent Bï¼‰åä½œè§£å†³äººå·¥æ™ºèƒ½åˆè§„æ ‡å‡†åˆ¶å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¦‚ä¸‹å¤æ‚çš„ä»»åŠ¡å’Œåˆ†å·¥ï¼š

1. **Agent Aï¼ˆè®¡ç®—æœºç§‘å­¦å®¶ï¼‰ï¼š**è´Ÿè´£æå‡ºæŠ€æœ¯å®æ–½çš„å»ºè®®å’ŒæŠ€æœ¯éš¾é¢˜çš„è§£å†³ï¼Œç†Ÿæ‚‰äººå·¥æ™ºèƒ½æŠ€æœ¯å’Œæ•°æ®éšç§ã€‚

```python
# agent_a.py
from common import create_app

app = create_app("system: ä½ æ˜¯ä¸€ä¸ªç†Ÿæ‚‰äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è®¡ç®—æœºç§‘å­¦å®¶ã€‚")

if __name__ == '__main__':
Â  Â  app.run(port=5000)
```

2. **Agent Bï¼ˆæ³•å¾‹ä¸“å®¶ï¼‰ï¼š**è´Ÿè´£è¯„ä¼°åˆè§„æ€§ã€æå‡ºæ³•å¾‹å»ºè®®å’Œæ³•è§„æ¡†æ¶ï¼Œç²¾é€šæ³•å¾‹æ³•è§„å’Œæ•°æ®éšç§ä¿æŠ¤ã€‚

```python
# agent_b.py
from common import create_app

app = create_app("system: ä½ æ˜¯ä¸€ä¸ªç†Ÿæ‚‰æ³•å¾‹æ³•è§„çš„æ³•å¾‹ä¸“å®¶ã€‚")

if __name__ == '__main__':
Â  Â  app.run(port=5001)
```

3. **Agent Cï¼ˆæ ‡å‡†åŒ–ä¸“å®¶ï¼‰ï¼š**è´Ÿè´£åè°ƒå’Œæ•´åˆå»ºè®®ï¼Œç”Ÿæˆå®Œæ•´çš„æŠ€æœ¯åˆè§„æ–¹æ¡ˆã€‚ç¡®ä¿æœ€ç»ˆçš„åˆè§„æ–¹æ¡ˆå®Œæ•´å’Œå¯å®æ–½ã€‚

```python
# agent_c.py
from flask import Flask, request, jsonify
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM

from langchain.agents import initialize_agent, Tool, AgentType
from langchain.llms import HuggingFacePipeline
import requests
import torch

app = Flask(__name__)

cache_dir = './llama_cache'
model_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B-Instruct'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
Â  Â  model_path,
Â  Â  torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
Â  Â  device_map="auto" if torch.cuda.is_available() else None,
Â  Â  pad_token_id=tokenizer.eos_token_id
)

def call_expert(url, task_requirement):
Â  Â  response = requests.post(url, json={"intent": task_requirement}, timeout=5)
Â  Â  response.raise_for_status()
Â  Â  return response.json().get("response", "Error: No response from expert")

ai_expert = lambda task: call_expert("http://localhost:5000/chat", task)
law_expert = lambda task: call_expert("http://localhost:5001/chat", task)

ai = Tool.from_function(func=ai_expert, name="ai_expert", description="å½“ä½ éœ€è¦äººå·¥æ™ºèƒ½ä¸“å®¶çŸ¥è¯†æ—¶ä½¿ç”¨è¿™ä¸ªå·¥å…·ï¼Œè¾“å…¥ä¸ºå…·ä½“é—®é¢˜ï¼Œè¿”å›ä¸ºé—®é¢˜ç­”æ¡ˆ")
law = Tool.from_function(func=law_expert, name="law_expert", description="å½“ä½ éœ€è¦æ³•å¾‹åˆè§„ä¸“å®¶çŸ¥è¯†æ—¶ä½¿ç”¨è¿™ä¸ªå·¥å…·ï¼Œè¾“å…¥ä¸ºå…·ä½“é—®é¢˜ï¼Œè¿”å›ä¸ºé—®é¢˜ç­”æ¡ˆ")

tools = [ai, law]

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
llm = HuggingFacePipeline(pipeline=pipe)

agent = initialize_agent(tools,
Â  Â  Â  Â  llm,
Â  Â  Â  Â  agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
Â  Â  Â  Â  verbose=True,
Â  Â  Â  Â  max_iterations = 5,
Â  Â  Â  Â  handle_parsing_errors = True)

@app.route('/integrate', methods=['POST'])
def integrate():
Â  Â  data = request.get_json()
Â  Â  task = data.get('task', '')
Â  Â  res = agent.run(task)
Â  Â  return jsonify({'response': res})

if __name__ == '__main__':
Â  Â  app.run(port=5002)
```

å¯ä»¥çœ‹å‡ºï¼Œåœ¨å¤šæ™ºèƒ½ä½“åˆä½œçš„æ—¶å€™ï¼Œ**å„ä¸ªæ™ºèƒ½ä½“åœ¨å½¼æ­¤çš„çœ¼ä¸­éƒ½æ˜¯å·¥å…·**ã€‚ä¸‹é¢ï¼Œé€šè¿‡ curl å‘½ä»¤å‘ [http://localhost:5002/integrate](http://localhost:5002/integrate) å‘é€ POST è¯·æ±‚ï¼Œä»»åŠ¡æè¿°ä¸ºåˆ¶å®šäººå·¥æ™ºèƒ½åˆè§„æ ‡å‡†ï¼Œé¿å…äººå·¥æ™ºèƒ½ä¼¤å®³äººç±»ã€‚

è¿”å›ç»“æœä¸ºæ ‡å‡†åŒ–ä¸“å®¶ï¼ˆAgent Cï¼‰é€šè¿‡åè°ƒè®¡ç®—æœºç§‘å­¦å®¶ï¼ˆAgent Aï¼‰å’Œæ³•å¾‹ä¸“å®¶ï¼ˆAgent Bï¼‰çš„å“åº”ç”Ÿæˆçš„ç»¼åˆç­”æ¡ˆã€‚

```bash
$ curl -X POST http://localhost:5002/integrate \
Â  Â  Â  Â -H "Content-Type: application/json" \
Â  Â  Â  Â -d '{"task": "åˆ¶å®šäººå·¥æ™ºèƒ½åˆè§„æ ‡å‡†ï¼Œé¿å…äººå·¥æ™ºèƒ½ä¼¤å®³äººç±»"}'
```

åœ¨è¿™ä¸ªè®¾è®¡ç¤ºä¾‹ä¸­ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å’Œæ³•å¾‹ä¸“å®¶åœ¨åˆ¶å®šäººå·¥æ™ºèƒ½åˆè§„æ ‡å‡†æ—¶è¿›è¡Œäº†åˆ†å·¥å’Œåä½œã€‚Agent A æå‡ºæŠ€æœ¯å®æ–½çš„å»ºè®®å’Œè§£å†³æŠ€æœ¯éš¾é¢˜ï¼Œè€Œ Agent B è´Ÿè´£è¯„ä¼°åˆè§„æ€§ã€æå‡ºæ³•å¾‹å»ºè®®å’Œæ³•è§„æ¡†æ¶ã€‚è¿™ç§åˆ†å·¥æ¨¡å¼èƒ½æœ‰æ•ˆç»“åˆæŠ€æœ¯å®æ–½å’Œæ³•å¾‹æ³•è§„ï¼Œä»¥ç¡®ä¿äººå·¥æ™ºèƒ½æŠ€æœ¯çš„åˆæ³•åˆè§„æ€§ã€‚

æœ€åï¼Œæˆ‘ä»¬å±•å¼€ä»‹ç» common.pyï¼Œè¿™ä¸ªå‡½æ•°æ¥æ”¶æ¨¡å‹ç›®å½•å’Œç§å­è®°å¿†ï¼ˆseed memoryï¼‰ä½œä¸ºå‚æ•°ï¼Œåˆ›å»ºå¹¶è¿”å›ä¸€ä¸ª python æœåŠ¡åº”ç”¨ã€‚

è¿™é‡Œæœ‰ä¸€ä¸ªé‡ç‚¹ï¼Œé‚£å°±æ˜¯æˆ‘ä»¬è¦ç»™æ¯ä¸ª Agent ä¸€ä¸ªç§å­è®°å¿†ï¼Œä¹Ÿå°±æ˜¯å®ƒçš„å…¨å±€äººè®¾ï¼Œè¿™æ˜¯æ™ºèƒ½ä½“åº”ç”¨ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ï¼Œå› ä¸º**å¦‚æœæ²¡æœ‰ä¸€ä¸ªåšå›ºçš„äººè®¾ï¼Œæ™ºèƒ½ä½“åœ¨é•¿æœŸå·¥ä½œè¿‡ç¨‹ä¸­ä¸€å®šä¼šå‡ºç°åç¦»**ï¼Œåœ¨åé¢è¯¾ç¨‹ä¸­æˆ‘å°†è¿›ä¸€æ­¥è§£é‡Šè¿™å¥è¯çš„å«ä¹‰ã€‚

```python
# common.py
from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def create_app(seed_memory):
Â  Â  app = Flask(__name__)

Â  Â  cache_dir = './llama_cache'
Â  Â  model_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B-Instruct'
Â  Â  tokenizer = AutoTokenizer.from_pretrained(model_path)
Â  Â  model = AutoModelForCausalLM.from_pretrained(
Â  Â  Â  Â  model_path,
Â  Â  Â  Â  torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
Â  Â  Â  Â  device_map="auto" if torch.cuda.is_available() else None,
Â  Â  Â  Â  pad_token_id=tokenizer.eos_token_id
Â  Â  )

Â  Â  # å®šä¹‰èŠå¤©æ¥å£
Â  Â  @app.route('/chat', methods=['POST'])
Â  Â  def chat():
Â  Â  Â  Â  data = request.get_json()
Â  Â  Â  Â  intent = data.get('intent', '')

Â  Â  Â  Â  # æ„é€ æç¤ºè¯
Â  Â  Â  Â  prompt = f"{seed_memory}\nè¯·å›ç­”ä»¥ä¸‹é—®é¢˜:{intent}"
Â  Â  Â  Â  input_ids = tokenizer.encode(prompt, return_tensors="pt").to("cuda")
Â  Â  Â  Â  outputs = model.generate(input_ids, max_length=150)
Â  Â  Â  Â  response = tokenizer.decode(outputs[0], skip_special_tokens=True)

Â  Â  Â  Â  return jsonify({'response': response})

Â  Â  return app
```

å½“ç„¶æœ¬èŠ‚è¯¾çš„ä¾‹å­éƒ½æ˜¯ä¸ºäº†è®©ä½ å¯ä»¥å¿«é€Ÿäº§ç”Ÿæ„Ÿæ€§è®¤è¯†ï¼Œè¿™ç§å®ç°æ–¹å¼é€‚ç”¨äºç®€å•çš„ç¤ºä¾‹å’Œå°è§„æ¨¡çš„åº”ç”¨ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åˆ™éœ€è¦è¿›ä¸€æ­¥åœ°å­¦ä¹ æ›´å¤æ‚çš„æ–¹æ¡ˆã€‚

## å°ç»“

å¥½äº†ï¼Œåˆ°è¿™é‡Œæˆ‘ä»¬æ¥åšä¸ªæ€»ç»“å§ã€‚åœ¨è¿™èŠ‚è¯¾ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† LLaMA 3 èƒ½åŠ›çš„å‡ ä¸ªå±‚é¢ã€‚

é¦–å…ˆæ˜¯ NTPï¼ˆNext Token Predictionï¼‰çš„åŸºç¡€èƒ½åŠ›ã€‚æˆ‘ä»¬è¯¦ç»†è§£æäº†å…¶åŸç†å’Œå®ç°æ–¹å¼ï¼ŒNTPé€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ¥ç”Ÿæˆè¿è´¯çš„å¯¹è¯ï¼Œæ˜¯å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ ¸å¿ƒæœºåˆ¶ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†è¾“å…¥æ–‡æœ¬åˆ†è§£ä¸ºtokenï¼Œå°†å…¶è½¬æ¢ä¸ºåµŒå…¥å½¢å¼ï¼Œå¹¶é€šè¿‡å¤§è¯­è¨€æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒæ¥ç”Ÿæˆæ–‡æœ¬ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº† LLaMA 3 å’Œ LLaMA 3-Instruct ä¸¤ä¸ªç‰ˆæœ¬ï¼Œå¹¶æåŠäº†LLaMA 3 çš„å…¶ä»–å˜ä½“ã€‚

æ¥ç€æ˜¯ä»æ— çŠ¶æ€åˆ°æœ‰çŠ¶æ€çš„å¯¹è¯æœåŠ¡è½¬å˜ã€‚æˆ‘ä»¬è®²è§£äº†å¦‚ä½•ä¿ç•™å¯¹è¯å†å²ä¿¡æ¯ï¼Œå°†æ— çŠ¶æ€çš„LLMè½¬åŒ–ä¸ºæœ‰çŠ¶æ€çš„å¯¹è¯æœåŠ¡ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å¯¹è¯ä¿æŒä¸Šä¸‹æ–‡çš„è¿è´¯æ€§ã€‚

ç„¶åæ˜¯ä»å°é—­åˆ°å¼€æ”¾çš„å®æ—¶ä¿¡æ¯æœåŠ¡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„ç›®æ ‡ã€‚æˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•å°†å°é—­çš„ LLM è½¬å˜ä¸ºèƒ½å¤Ÿè·å–å®æ—¶ä¿¡æ¯çš„å¼€æ”¾æœåŠ¡ã€‚é€šè¿‡æ•´åˆå¤–éƒ¨æ£€ç´¢ç³»ç»Ÿï¼ˆä¾‹å¦‚æˆ‘ä»¬ä½¿ç”¨çš„æœç´¢å¼•æ“ APIï¼‰ï¼Œæ¨¡å‹å¯ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è·å–æœ€æ–°çš„äº‹å®ä¿¡æ¯ï¼Œä»¥ä¿è¯ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚

æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å¤šæ™ºèƒ½ä½“çš„åä½œã€‚é€šè¿‡å¤šæ™ºèƒ½ä½“çš„é…åˆï¼Œæˆ‘ä»¬å¯ä»¥è§£å†³é•¿å¯¹è¯å¸¦æ¥çš„æ¨ç†æ€§èƒ½å’Œæ•ˆæœé—®é¢˜ï¼Œé¿å…èŒè´£æ··ä¹±ã€‚æ˜ç¡®åˆ†å·¥åï¼Œå„æ™ºèƒ½ä½“å¯ä»¥å¤„ç†ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œä»è€Œæå‡æ•´ä½“å¯¹è¯ç³»ç»Ÿçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

é€šè¿‡è¿™äº›è®²è§£ï¼Œå¸Œæœ›ä½ èƒ½æ›´æ·±å…¥åœ°ç†è§£ LLaMA 3 çš„å„é¡¹èƒ½åŠ›å’Œåº”ç”¨åœºæ™¯ï¼Œå¿«é€Ÿåœ°æŒæ¡å¤§æ¨¡å‹æŠ€æœ¯çš„å®é™…æ“ä½œæ–¹æ³•ã€‚

è¯¾ç¨‹ä»£ç åœ°å€ï¼š[https://github.com/tylerelyt/LLaMa-in-Action](https://github.com/tylerelyt/LLaMa-in-Action)

## è¯¾åæ€è€ƒ

1. ä½¿ç”¨ LLaMA 3 çš„ä¸åŒç‰ˆæœ¬å¯¹æ¯”å®éªŒï¼Œåœ¨è¯„è®ºåŒºç»™å‡ºä½ çš„å®éªŒå¯¹æ¯”æ•ˆæœã€‚
2. ä½¿ç”¨ Ollama å’Œ HuggingFace LLaMA 3 è¿›è¡Œå†…å®¹ç”Ÿæˆï¼Œå¯¹æ¯”æ€§èƒ½è¡¨ç°ã€‚

æœŸå¾…ä½ åœ¨ç•™è¨€åŒºå’Œæˆ‘äº¤æµäº’åŠ¨ã€‚å¦‚æœä»Šå¤©çš„è¯¾ç¨‹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ä½ æŠŠå®ƒè½¬å‘å‡ºå»ã€‚æˆ‘ä»¬ä¸‹èŠ‚è¯¾è§ï¼

## å®éªŒç¯å¢ƒ

æˆ‘ä»¬å°†ä½¿ç”¨Ollamaè¿›è¡Œå®éªŒç¯å¢ƒçš„æ­å»ºã€‚ä»¥ä¸‹æ˜¯å…·ä½“æ­¥éª¤ï¼š

ä½ éœ€è¦åœ¨ä»¥ä¸‹ç¯å¢ƒä¸­è¿›è¡Œæ“ä½œï¼š

- macOS bash
- Linux bash
- Windows WSL bash

é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¯åŠ¨LLaMA 3æ¨¡å‹ï¼š

```bash
$ ollama run llama3:text
```

ä½ ä¼šçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹è¾“å‡ºï¼š

```python
pulling manifest
pulling cebceffdc781... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 4.7 GB
pulling 4fa551d4f938... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Â  12 KB
pulling 0dbc577651fb... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Â  337 B
verifying sha256 digest
writing manifest
removing any unused layers
success
>>> Send a message (/? for help)
```

å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æµ‹è¯•Ollamaæ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```python
$ curl -X POST http://localhost:11434/api/generate -d '{
Â  "model": "llama3",
Â  "prompt":"Why is the sky blue?"
Â }'
```

æ¥ä¸‹æ¥å¯åŠ¨open-webuiï¼š

```python
$ docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

ä½ ä¼šçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹è¾“å‡ºï¼š

```python
Unable to find image 'ghcr.io/open-webui/open-webui:main' locally
main: Pulling from open-webui/open-webui
2cc3ae149d28: Pull complete
dc57dfa1396c: Pull complete
b275de30f399: Pull complete
0ea58f563222: Pull complete
251072225b40: Pull complete
130662f3df11: Pull complete
4f4fb700ef54: Pull complete
de53a1836181: Pull complete
d28e6308a168: Pull complete
e2c345686679: Pull complete
4a9cac9db244: Pull complete
8f76aa437192: Pull complete
5e8d46269631: Pull complete
83e1a8b855bf: Pull complete
179448cc6367: Pull complete
a3c72f49a0d3: Pull complete
Digest: sha256:cecf06773cc0621dbe83c25fdeaf9c9bae33799cd7df14790a9b8ccf61b91764
Status: Downloaded newer image for ghcr.io/open-webui/open-webui:main
cbff08075458b6342eef83c36343ec04500fe899281e0f74260aa4ed64bbe374
```

ä¹‹åè®¿é—® [http://localhost:3000](http://localhost:3000)ï¼Œæ³¨å†Œä¸€ä¸ªè´¦å·ï¼Œå°±å¯ä»¥çœ‹åˆ°ä»¥ä¸‹ç•Œé¢ä½¿ç”¨å•¦ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/3f/ce/3f345bdbb8c5d0eddef102c700ccd6ce.png?wh=1400x730)
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>å…µæˆˆ</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>Typlerè€å¸ˆçš„è¯¾ç¨‹ä»£ç åœ°å€ï¼šhttps:&#47;&#47;github.com&#47;tylerelyt&#47;llamaã€‚ä¸è¿‡å½“å‰å·¥ç¨‹ç›®å½•ä¸‹ç¼ºå°‘requirements.txtï¼Œå¯èƒ½ä¼šè®©è®¸å¤šè·‘ä»£ç çš„åŒå­¦é‡åˆ°åŒ…ä¾èµ–ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæˆ‘æäº†ä¸€ä¸ªissueï¼š
https:&#47;&#47;github.com&#47;tylerelyt&#47;llama&#47;issues&#47;1
å¸Œæœ›Typerè€å¸ˆæœ‰ç©ºè¡¥å……ä¸‹ :ï¼‰</p>2024-10-18</li><br/><li><span>å…µæˆˆ</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>Tylerè€å¸ˆæä¾›ç¤ºä¾‹å¾ˆä¸é”™ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼Œæœ€å¥½åŠ ä¸Šmax_lengthå‚æ•°ï¼Œå¦åˆ™å¯èƒ½ä¼šä¸€ç›´è¿è¡Œåœä¸ä¸‹æ¥ï¼Œå¦‚ä¸‹ï¼š
with torch.no_grad():
    outputs = model.generate(**inputs, max_length=100)</p>2024-10-17</li><br/><li><span>edward</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>è¯·é—®è€å¸ˆ å®éªŒç¯å¢ƒéœ€è¦ä»€ä¹ˆæ ·çš„æœºå™¨é…ç½®ï¼Ÿ</p>2024-10-16</li><br/><li><span>keep move</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç¤ºä¾‹ä»£ç å¦‚ä½•èƒ½è¿è¡Œèµ·æ¥ï¼Œèƒ½å¦å†™ä¸ªç®€è¦çš„æ­¥éª¤å‘¢</p>2024-10-30</li><br/><li><span>J Sun</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æ¨¡å‹æ€ä¹ˆä¸‹è½½
</p>2024-10-18</li><br/><li><span>Sï¼</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>m2ç³»åˆ—çš„mac proè¿è¡Œdemoä¸€ç›´æ²¡æœ‰è¾“å‡ºç»“æœï¼Œconsoleè¾“å‡ºå¦‚ä¸‹ï¼š
import sys; print(&#39;Python %s on %s&#39; % (sys.version, sys.platform))
&#47;usr&#47;local&#47;bin&#47;python3.10 -X pycache_prefix=&#47;Users&#47;salesonlee&#47;Library&#47;Caches&#47;JetBrains&#47;PyCharm2024.1&#47;cpython-cache &#47;Applications&#47;PyCharm.app&#47;Contents&#47;plugins&#47;python&#47;helpers&#47;pydev&#47;pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 57101 --file &#47;Users&#47;salesonlee&#47;IT&#47;dev_codes&#47;PythonProjects&#47;tylerelyt-llama&#47;chapter1&#47;lesson1&#47;example1.py 
Connected to pydev debugger (build 241.17890.14)
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4&#47;4 [01:00&lt;00:00, 15.10s&#47;it]
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)


è¯·é—®æ˜¯å› ä¸ºæ²¡æœ‰ä½¿ç”¨gpuä¹ˆï¼Œæ˜¯ç¡¬ä»¶é…ç½®çš„åŸå› ï¼Ÿè¿˜æ˜¯å…¶å®ƒåŸå› ï¼Ÿ </p>2024-11-11</li><br/><li><span>Sï¼</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>import sys; print(&#39;Python %s on %s&#39; % (sys.version, sys.platform))
&#47;usr&#47;local&#47;bin&#47;python3.10 -X pycache_prefix=&#47;Users&#47;salesonlee&#47;Library&#47;Caches&#47;JetBrains&#47;PyCharm2024.1&#47;cpython-cache &#47;Applications&#47;PyCharm.app&#47;Contents&#47;plugins&#47;python&#47;helpers&#47;pydev&#47;pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 57101 --file &#47;Users&#47;salesonlee&#47;IT&#47;dev_codes&#47;PythonProjects&#47;tylerelyt-llama&#47;chapter1&#47;lesson1&#47;example1.py 
Connected to pydev debugger (build 241.17890.14)
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4&#47;4 [01:00&lt;00:00, 15.10s&#47;it]
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
</p>2024-11-11</li><br/><li><span>Sï¼</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>m2ç³»åˆ—mac pro è¿è¡Œdemoä¸€ç›´æ²¡æœ‰è¾“å‡ºç»“æœï¼Œconsoleè¾“å‡ºå¦‚ä¸‹ï¼š</p>2024-11-11</li><br/><li><span>Sï¼</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è¿è¡Œè¯¾ç¨‹demoéœ€è¦ä»€ä¹ˆæ ·çš„ç¢ç¡¬ä»¶èµ„æºï¼Ÿ</p>2024-11-10</li><br/><li><span>Sï¼</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>Mac proå¯ä»¥è¿è¡Œllama3ä¹ˆ</p>2024-11-10</li><br/><li><span>keep move</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>pythonå­¦ä¹ é‚£ä¸ªæ–¹å‘ pytorchå—</p>2024-10-23</li><br/><li><span>Orson</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p># agent_a.py
from common import create_app

app = create_app(&quot;system: ä½ æ˜¯ä¸€ä¸ªç†Ÿæ‚‰äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è®¡ç®—æœºç§‘å­¦å®¶ã€‚&quot;)

if __name__ == &#39;__main__&#39;:
    app.run(port=5000)
è¯·é—®ï¼Œåœ¨ç”¨AutoDLè¿™æ ·çš„äº‘æœåŠ¡çš„æ—¶å€™ï¼Œapp.run(port=5000ï¼‰éœ€è¦æ€ä¹ˆä¿®æ”¹ã€‚æˆ–è€…ï¼Œagent_aã€bã€cå’Œcommonæ–‡ä»¶éœ€è¦åšå“ªäº›ä¿®æ”¹ï¼Ÿ</p>2024-10-21</li><br/><li><span>Allen</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>ä½ å¥½ï¼Œè¯·é—®åœ¨è°ƒç”¨æ¨¡å‹æ—¶with torch.no_grad():ä¸ºå•¥åé¢å‡ æ®µé‡Œæ²¡æœ‰åŠ ï¼Ÿä¼šå½±å“æ€§èƒ½å—ï¼Ÿè°¢è°¢ã€‚</p>2024-10-20</li><br/><li><span>Dowen Liu</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>ä¸ç”¨python èƒ½ä¸èƒ½åšåŒæ ·çš„è°ƒç”¨å‘¢ï¼Ÿæ¯”å¦‚ç›´æ¥è°ƒç”¨æ¥å£ã€spring ai </p>2024-10-19</li><br/><li><span>J Sun</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æ¨¡å‹åœ¨å“ªé‡Œä¸‹è½½</p>2024-10-18</li><br/>
</ul>