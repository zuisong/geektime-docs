ä½ å¥½ï¼Œæˆ‘æ˜¯é»„é¸¿æ³¢ã€‚

ä¸ŠèŠ‚è¯¾æˆ‘ä»¬è®²äº†å…³äºYouTubeDNNçš„å¬å›æ¨¡å‹ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•ç”¨ä»£ç æ¥å®ç°å®ƒã€‚

æˆ‘ä»¬åœ¨åšYouTubeDNNçš„æ—¶å€™ï¼Œè¦æŠŠä»£ç åˆ†æˆä¸¤ä¸ªæ­¥éª¤ï¼Œç¬¬ä¸€ä¸ªæ­¥éª¤æ˜¯å¯¹æ•°æ®çš„æ¸…æ´—å’Œå¤„ç†ï¼Œç¬¬äºŒä¸ªæ­¥éª¤æ˜¯æ­å»ºæ¨¡å‹ç„¶åæŠŠæ•°æ®æ”¾è¿›å»è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚

## æ•°æ®çš„æ¸…æ´—å’Œå¤„ç†

å…ˆæ¥è®²æ•°æ®éƒ¨åˆ†ã€‚

æŒ‰ç…§YouTubeDNNè®ºæ–‡æ¥çœ‹ï¼Œè¾“å…¥çš„æ•°æ®æ˜¯ç”¨æˆ·çš„ä¿¡æ¯ã€è§†é¢‘çš„IDåºåˆ—ã€ç”¨æˆ·æœç´¢çš„ç‰¹å¾å’Œä¸€äº›åœ°ç†ä¿¡æ¯ç­‰å…¶ä»–ä¿¡æ¯ã€‚åˆ°äº†åŸºäºæ–‡ç« å†…å®¹çš„ä¿¡æ¯æµäº§å“ä¸­ï¼Œå°±å˜æˆäº†ç”¨æˆ·IDã€å¹´é¾„ã€æ€§åˆ«ã€åŸå¸‚ã€é˜…è¯»çš„æ—¶é—´æˆ³å†åŠ ä¸Šè§†é¢‘çš„IDã€‚æˆ‘ä»¬æŠŠè¿™äº›å†…å®¹å¯ä»¥ç»„åˆæˆYouTubeDNNéœ€è¦çš„å†…å®¹ï¼Œæœ€åå¤„ç†æˆéœ€è¦çš„Embeddingã€‚

ç”±äºå‰é¢æ²¡æœ‰å¤ªå¤šçš„ç”¨æˆ·æµè§ˆæ•°æ®ï¼Œæ‰€ä»¥æˆ‘å…ˆé€ äº†ä¸€æ‰¹æ•°æ®ï¼Œæ•°æ®é›†æˆ‘ä¼šæ”¾åˆ°GitHubä¸Šï¼ˆåç»­æ›´æ–°ï¼‰ï¼Œæ•°æ®çš„å½¢å¼å¦‚ä¸‹ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/ea/0a/eaeef45b0eb7e64c3f11c4a252f8120a.png?wh=1379x1424)

æ¥ä¸‹æ¥æˆ‘ä»¬å°±æŠŠè¿™æ‰¹æ•°æ®å¤„ç†æˆYouTubeDNNéœ€è¦çš„å½¢å¼ã€‚é¦–å…ˆåœ¨recommendation-classé¡¹ç›®ä¸­çš„utilsç›®å½•ä¸‹å»ºç«‹ä¸€ä¸ªpreprocess.pyæ–‡ä»¶ï¼Œä½œä¸ºå¤„ç†æ•°æ®çš„æ–‡ä»¶ã€‚

æˆ‘ä»¬è¦å¤„ç†è¿™ä¸€æ‰¹æ•°æ®ï¼Œéœ€è¦ä¸‹é¢äº”ä¸ªæ­¥éª¤ã€‚

1. åŠ è½½æ•°æ®é›†ã€‚
2. å¤„ç†æ•°æ®ç‰¹å¾ã€‚
3. ç‰¹å¾è½¬åŒ–ä¸ºæ¨¡å‹è¾“å…¥ã€‚
4. æ¨¡å‹çš„æ­å»ºå’Œè®­ç»ƒã€‚
5. æ¨¡å‹è¯„ä¼°ã€‚

åœ¨æ­£å¼å†™ä»£ç ä¹‹å‰ï¼Œéœ€è¦å®‰è£…å‡ ä¸ªåº“ï¼Œå¦‚ä¸‹ã€‚

```plain
deepctr
deepmatch
tensorflow==2.2
pandas
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨pip installåŠ ä¸Šåº“åæ¥å®‰è£…å®ƒä»¬ï¼Œä¹Ÿå¯ä»¥æŠŠå®ƒä»¬æ”¾åœ¨ä¸€ä¸ªå«requirements.txtçš„æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨pip install -rè¿›è¡Œå®‰è£…ã€‚

å®‰è£…å®Œæˆä¹‹åï¼Œæˆ‘ä»¬æ¥å†™preprocess.pyçš„ä»£ç ã€‚ä¸ºäº†èƒ½å¤Ÿè®©ä½ çœ‹å¾—æ›´æ˜ç™½ï¼Œæˆ‘åœ¨å‡½æ•°é‡ŒåŠ äº†ä¸€äº›æ³¨é‡Šï¼Œå…ˆä¸Šä»£ç ã€‚

```plain
from tqdm import tqdm
import numpy as np
import random
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
Â 
def gen_data_set(data, negsample=0):
Â Â Â Â data.sort_values("timestamp", inplace=True) Â #æ˜¯å¦ç”¨æ’åºåçš„æ•°æ®é›†æ›¿æ¢åŸæ¥çš„æ•°æ®ï¼Œè¿™é‡Œæ˜¯æ›¿æ¢
Â Â Â Â item_ids = data['item_id'].unique() Â Â Â #iteméœ€è¦è¿›è¡Œå»é‡
Â 
Â Â Â Â train_set = list()
Â Â Â Â test_set = list()
Â Â Â Â for reviewrID, hist in tqdm(data.groupby('user_id')): Â Â #è¯„ä»·è¿‡, Â å†å²è®°å½•
Â Â Â Â Â Â Â Â pos_list = hist['item_id'].tolist()
Â Â Â Â Â Â Â Â rating_list = hist['rating'].tolist()
Â 
Â Â Â Â Â Â Â Â if negsample > 0: Â Â Â #è´Ÿæ ·æœ¬
Â Â Â Â Â Â Â Â Â Â Â Â candidate_set = list(set(item_ids) - set(pos_list)) Â Â #å»æ‰ç”¨æˆ·çœ‹è¿‡çš„itemé¡¹ç›®
Â Â Â Â Â Â Â Â Â Â Â Â neg_list = np.random.choice(candidate_set, size=len(pos_list) * negsample, replace=True) Â #éšæœºé€‰æ‹©è´Ÿé‡‡æ ·æ ·æœ¬
Â Â Â Â Â Â Â Â for i in range(1, len(pos_list)):
Â Â Â Â Â Â Â Â Â Â Â Â if i != len(pos_list) - 1:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â train_set.append((reviewrID, hist[::-1], pos_list[i], 1, len(hist[:: -1]), rating_list[i])) Â #è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ’åˆ† Â [::-1]ä»åç©å‰æ•°
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â for negi in range(negsample):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â train_set.append((reviewrID, hist[::-1], neg_list[i * negsample + negi], 0, len(hist[::-1])))
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â test_set.append((reviewrID, hist[::-1], pos_list[i], 1, len(hist[::-1]), rating_list[i]))
Â 
Â Â Â Â random.shuffle(train_set) Â Â Â Â #æ‰“ä¹±æ•°æ®é›†
Â Â Â Â random.shuffle(test_set)
Â Â Â Â return train_set, test_set
Â 
def gen_model_input(train_set, user_profile, seq_max_len):
Â Â Â Â train_uid = np.array([line[0] for line in train_set])
Â Â Â Â train_seq = [line[1] for line in train_set]
Â Â Â Â train_iid = np.array([line[2] for line in train_set])
Â Â Â Â train_label = np.array([line[3] for line in train_set])
Â Â Â Â train_hist_len = np.array([line[4] for line in train_set])
Â 
Â Â Â Â """
Â Â Â Â pad_sequencesæ•°æ®é¢„å¤„ç†
Â Â Â Â sequencesï¼šæµ®ç‚¹æ•°æˆ–æ•´æ•°æ„æˆçš„ä¸¤å±‚åµŒå¥—åˆ—è¡¨
Â Â Â Â maxlenï¼šNoneæˆ–æ•´æ•°ï¼Œä¸ºåºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚å¤§äºæ­¤é•¿åº¦çš„åºåˆ—å°†è¢«æˆªçŸ­ï¼Œå°äºæ­¤é•¿åº¦çš„åºåˆ—å°†åœ¨åéƒ¨å¡«0.
Â Â Â Â dtypeï¼šè¿”å›çš„numpy arrayçš„æ•°æ®ç±»å‹
Â Â Â Â paddingï¼šâ€˜preâ€™æˆ–â€˜postâ€™ï¼Œç¡®å®šå½“éœ€è¦è¡¥0æ—¶ï¼Œåœ¨åºåˆ—çš„èµ·å§‹è¿˜æ˜¯ç»“å°¾è¡¥`
Â Â Â Â truncatingï¼šâ€˜preâ€™æˆ–â€˜postâ€™ï¼Œç¡®å®šå½“éœ€è¦æˆªæ–­åºåˆ—æ—¶ï¼Œä»èµ·å§‹è¿˜æ˜¯ç»“å°¾æˆªæ–­
Â Â Â Â valueï¼šæµ®ç‚¹æ•°ï¼Œæ­¤å€¼å°†åœ¨å¡«å……æ—¶ä»£æ›¿é»˜è®¤çš„å¡«å……å€¼0
Â Â Â Â """
Â Â Â Â train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)
Â Â Â Â train_model_input = {"user_id": train_uid, "item_id": train_iid, "hist_item_id": train_seq_pad,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "hist_len": train_hist_len}
Â Â Â Â for key in {"gender", "age", "city"}:
Â Â Â Â Â Â Â Â train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values Â Â #è®­ç»ƒæ¨¡å‹çš„å…³é”®å­—
Â 
	return train_model_input, train_label
```

è¿™æ®µä»£ç ä¸»è¦ç”¨äºç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä»¥åŠæ¨¡å‹çš„è¾“å…¥ã€‚å®ƒçœ‹èµ·æ¥æœ‰ç‚¹é•¿ï¼Œæˆ‘æ¥åˆ†åˆ«è§£é‡Šä¸€ä¸‹ã€‚

gen\_data\_set()å‡½æ•°æ¥å—ä¸€ä¸ªæ•°æ®é›†ï¼ˆdataï¼‰å’Œä¸€ä¸ªè´Ÿé‡‡æ ·ï¼ˆnegsampleï¼‰å‚æ•°ï¼Œè¿”å›ä¸€ä¸ªè®­ç»ƒé›†åˆ—è¡¨å’Œä¸€ä¸ªæµ‹è¯•é›†åˆ—è¡¨ã€‚è¯¥å‡½æ•°é¦–å…ˆå°†æ•°æ®é›†æ ¹æ®æ—¶é—´æˆ³æ’åºï¼Œç„¶åä»æ¯ä¸€ä¸ªç”¨æˆ·çš„å†å²è®°å½•ä¸­é€‰å–æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼Œå¹¶å°†å®ƒä»¬ä¿å­˜åˆ°è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ã€‚

gen\_model\_input()å‡½æ•°æ¥å—ä¸€ä¸ªè®­ç»ƒé›†åˆ—è¡¨ã€ç”¨æˆ·ç”»åƒä¿¡æ¯å’Œåºåˆ—æœ€å¤§é•¿åº¦å‚æ•°ï¼Œè¿”å›è®­ç»ƒæ¨¡å‹çš„è¾“å…¥å’Œæ ‡ç­¾ã€‚è¯¥å‡½æ•°å°†è®­ç»ƒé›†åˆ—è¡¨æ‹†åˆ†æˆtrain\_uidã€train\_seqã€train\_iidã€train\_labelå’Œtrain\_hist\_lenäº”éƒ¨åˆ†ã€‚

- train\_uidå’Œtrain\_iidä¸ºç”¨æˆ·IDå’Œç‰©å“IDã€‚
- train\_seqä¸ºå†å²äº¤äº’åºåˆ—ã€‚
- train\_labelä¸ºæ­£è´Ÿæ ·æœ¬æ ‡ç­¾ã€‚
- train\_hist\_lenä¸ºå†å²äº¤äº’åºåˆ—çš„é•¿åº¦ã€‚

æ­¤å¤–ï¼Œå®ƒå¯¹å†å²äº¤äº’åºåˆ—è¿›è¡Œäº†å¡«å……å¤„ç†ï¼ˆpad\_sequencesï¼‰ï¼Œå¹¶ä¸”å°†ç”¨æˆ·ç”»åƒä¿¡æ¯åŠ å…¥åˆ°è®­ç»ƒæ¨¡å‹çš„å…³é”®å­—ä¸­ã€‚æœ€ç»ˆï¼Œè¯¥å‡½æ•°è¿”å›è®­ç»ƒæ¨¡å‹çš„è¾“å…¥å’Œæ ‡ç­¾ã€‚

åœ¨gen\_data\_set()å‡½æ•°ä¸­ï¼Œé¦–å…ˆä½¿ç”¨data.sort\_values(â€œtimestampâ€, inplace=True)å‡½æ•°å°†æ•°æ®é›†æŒ‰ç…§æ—¶é—´æˆ³æ’åºï¼Œè¿™æ˜¯ä¸ºäº†ä¿è¯æ•°æ®æŒ‰ç…§æ—¶é—´é¡ºåºæ’åˆ—ï¼Œä¾¿äºåç»­å¤„ç†ã€‚æ¥ä¸‹æ¥ä½¿ç”¨data\[â€˜item\_idâ€™].unique()å‡½æ•°è·å–æ•°æ®é›†ä¸­æ‰€æœ‰ä¸é‡å¤çš„ç‰©å“IDã€‚å› ä¸ºåç»­éœ€è¦ç­›é€‰å‡ºç”¨æˆ·æœªæ›¾è´­ä¹°è¿‡çš„ç‰©å“ï¼Œè¦å…ˆè·å–æ•°æ®é›†ä¸­æ‰€æœ‰çš„ç‰©å“IDä»¥ä¾¿åç»­å¤„ç†ã€‚

æ¥ä¸‹æ¥ä½¿ç”¨groupby()å‡½æ•°å°†ç”¨æˆ·IDï¼ˆuser\_idï¼‰ç›¸åŒçš„æ•°æ®åˆ†ç»„ã€‚å¯¹äºæ¯ä¸€ç»„æ•°æ®ï¼Œå°†å…¶åˆ†æˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ã€‚å…¶ä¸­æ­£æ ·æœ¬ä¸ºç”¨æˆ·å·²ç»è´­ä¹°è¿‡çš„ç‰©å“ï¼Œè´Ÿæ ·æœ¬ä¸ºç”¨æˆ·æœªè´­ä¹°è¿‡çš„å…¶ä»–ç‰©å“ã€‚å¦‚æœnegsampleå‚æ•°å¤§äº0ï¼Œåˆ™éœ€è¦è¿›è¡Œè´Ÿé‡‡æ ·ã€‚éšæœºé€‰å–ä¸€äº›æœªæ›¾è´­ä¹°è¿‡çš„ç‰©å“ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œå¹¶å°†å®ƒä»¬ä¿å­˜åˆ°è®­ç»ƒé›†åˆ—è¡¨ä¸­ã€‚æœ€åï¼Œå°†æ­£è´Ÿæ ·æœ¬æ•°æ®ä»¥åŠå…¶ä»–ä¿¡æ¯ï¼ˆå¦‚å†å²äº¤äº’åºåˆ—ã€ç”¨æˆ·IDå’Œå†å²äº¤äº’åºåˆ—çš„é•¿åº¦ï¼‰ä¿å­˜åˆ°è®­ç»ƒé›†åˆ—è¡¨å’Œæµ‹è¯•é›†åˆ—è¡¨ä¸­ã€‚

åœ¨gen\_model\_input()å‡½æ•°ä¸­ï¼Œé¦–å…ˆå°†è®­ç»ƒé›†åˆ—è¡¨æ‹†åˆ†æˆ5ä¸ªåˆ—è¡¨ï¼Œåˆ†åˆ«ä¿å­˜ç”¨æˆ·IDã€ç‰©å“IDã€å†å²äº¤äº’åºåˆ—ã€æ­£è´Ÿæ ·æœ¬æ ‡ç­¾å’Œå†å²äº¤äº’åºåˆ—é•¿åº¦ã€‚ç„¶åä½¿ç”¨pad\_sequences()å‡½æ•°å¯¹å†å²äº¤äº’åºåˆ—è¿›è¡Œå¡«å……å¤„ç†ï¼Œå°†å…¶å˜æˆé•¿åº¦ç›¸åŒçš„åºåˆ—ã€‚æœ€åï¼Œå°†ç”¨æˆ·ç”»åƒä¿¡æ¯åŠ å…¥åˆ°è®­ç»ƒæ¨¡å‹çš„å…³é”®å­—ä¸­ï¼Œè¿”å›è®­ç»ƒæ¨¡å‹çš„è¾“å…¥å’Œæ ‡ç­¾ã€‚

## æ­å»ºæ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹

å½“æ•°æ®å¤„ç†å®Œæˆåï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥æ¥åšYouTubeDNNçš„æ¨¡å‹éƒ¨åˆ†äº†ï¼Œæˆ‘ä»¬åœ¨recallç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œåå­—å«YouTubeDNNï¼Œç„¶åç¼–å†™å¦‚ä¸‹ä»£ç ã€‚

```plain
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from models.recall.preprocess import gen_data_set, gen_model_input
from deepctr.feature_column import SparseFeat, VarLenSparseFeat
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.models import Model
import tensorflow as tf
from deepmatch.models import *
from deepmatch.utils import recall_N
from deepmatch.utils import sampledsoftmaxloss
import numpy as np
from tqdm import tqdm
	Â 
	Â 
	class YoutubeModel(object):
	Â Â Â Â def __init__(self, embedding_dim=32):
	Â Â Â Â Â Â Â Â self.SEQ_LEN = 50
	Â Â Â Â Â Â Â Â self.embedding_dim = embedding_dim
	Â Â Â Â Â Â Â Â self.user_feature_columns = None
	Â Â Â Â Â Â Â Â self.item_feature_columns = None
	Â 
	Â Â Â Â def training_set_construct(self):
	Â Â Â Â Â Â Â Â # åŠ è½½æ•°æ®
	Â Â Â Â Â Â Â Â data = pd.read_csv('../../data/read_history.csv')
	Â Â Â Â Â Â Â Â # è´Ÿé‡‡æ ·ä¸ªæ•°
	Â Â Â Â Â Â Â Â negsample = 0
	Â Â Â Â Â Â Â Â # ç‰¹å¾ç¼–ç 
	Â Â Â Â Â Â Â Â features = ["user_id", "item_id", "gender", "age", "city"]
	Â Â Â Â Â Â Â Â features_max_idx = {}
	Â Â Â Â Â Â Â Â for feature in features:
	Â Â Â Â Â Â Â Â Â Â Â Â lbe = LabelEncoder()
	Â Â Â Â Â Â Â Â Â Â Â Â data[feature] = lbe.fit_transform(data[feature]) + 1
	Â Â Â Â Â Â Â Â Â Â Â Â features_max_idx[feature] = data[feature].max() + 1
	Â 
	Â Â Â Â Â Â Â Â # æŠ½å–ç”¨æˆ·ã€ç‰©å“ç‰¹å¾
	Â Â Â Â Â Â Â Â user_info = data[["user_id", "gender", "age", "city"]].drop_duplicates('user_id') Â # å»é‡æ“ä½œ
	Â Â Â Â Â Â Â Â item_info = data[["item_id"]].drop_duplicates('item_id')
	Â Â Â Â Â Â Â Â user_info.set_index("user_id", inplace=True)
	Â 
	Â Â Â Â Â Â Â Â # æ„å»ºè¾“å…¥æ•°æ®
	Â Â Â Â Â Â Â Â train_set, test_set = gen_data_set(data, negsample)
	Â Â Â Â Â Â Â Â # è½¬åŒ–ä¸ºæ¨¡å‹çš„è¾“å…¥
	Â Â Â Â Â Â Â Â train_model_input, train_label = gen_model_input(train_set, user_info, self.SEQ_LEN)
	Â Â Â Â Â Â Â Â test_model_input, test_label = gen_model_input(test_set, user_info, self.SEQ_LEN)
	Â Â Â Â Â Â Â Â # ç”¨æˆ·ç«¯ç‰¹å¾è¾“å…¥
	Â Â Â Â Â Â Â Â self.user_feature_columns = [SparseFeat('user_id', features_max_idx['user_id'], 16),
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â SparseFeat('gender', features_max_idx['gender'], 16),
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â SparseFeat('age', features_max_idx['age'], 16),
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â SparseFeat('city', features_max_idx['city'], 16),
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â VarLenSparseFeat(SparseFeat('hist_item_id', features_max_idx['item_id'],
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â self.embedding_dim, embedding_name='item_id'),
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â self.SEQ_LEN, 'mean', 'hist_len')
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ]
	Â Â Â Â Â Â Â Â # ç‰©å“ç«¯çš„ç‰¹å¾è¾“å…¥
	Â Â Â Â Â Â Â Â self.item_feature_columns = [SparseFeat('item_id', features_max_idx['item_id'], self.embedding_dim)]
	Â 
	Â Â Â Â Â Â Â Â return train_model_input, train_label, test_model_input, test_label, train_set, test_set, user_info, item_info
	Â 
	Â Â Â Â def training_model(self, train_model_input, train_label):
	Â Â Â Â Â Â Â Â K.set_learning_phase(True)
	Â Â Â Â Â Â Â Â if tf.__version__ >= '2.0.0':
	Â Â Â Â Â Â Â Â Â Â Â Â tf.compat.v1.disable_eager_execution()
	Â Â Â Â Â Â Â Â # å®šä¹‰æ¨¡å‹
	Â Â Â Â Â Â Â Â model = YouTubeDNN(self.user_feature_columns, self.item_feature_columns, num_sampled=100,
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â user_dnn_hidden_units=(128, 64, self.embedding_dim))
	Â Â Â Â Â Â Â Â model.compile(optimizer="adam", loss=sampledsoftmaxloss)
	Â Â Â Â Â Â Â Â # ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®
	Â Â Â Â Â Â Â Â model.fit(train_model_input, train_label, batch_size=512, epochs=20, verbose=1, validation_split=0.0,)
	Â Â Â Â Â Â Â Â return model
	Â 
	Â Â Â Â def extract_embedding_layer(self, model, test_model_input, item_info):
	Â Â Â Â Â Â Â Â all_item_model_input = {"item_id": item_info['item_id'].values, }
	Â Â Â Â Â Â Â Â # è·å–ç”¨æˆ·ã€itemçš„embedding_layer
	Â Â Â Â Â Â Â Â user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)
	Â Â Â Â Â Â Â Â item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)
	Â 
	Â Â Â Â Â Â Â Â user_embs = user_embedding_model.predict(test_model_input, batch_size=2 ** 12)
	Â Â Â Â Â Â Â Â item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)
	Â Â Â Â Â Â Â Â print(user_embs.shape)
	Â Â Â Â Â Â Â Â print(item_embs.shape)
	Â Â Â Â Â Â Â Â return user_embs, item_embs
	Â 
	Â Â Â Â def eval(self, user_embs, item_embs, test_model_input, item_info, test_set):
	Â Â Â Â Â Â Â Â test_true_label = {line[0]: line[2] for line in test_set}
	Â Â Â Â Â Â Â Â index = faiss.IndexFlagIP(self.embedding_dim)
	Â Â Â Â Â Â Â Â index.add(item_embs)
	Â Â Â Â Â Â Â Â D, I = index.search(np.ascontiguousarray(user_embs), 50)
	Â Â Â Â Â Â Â Â s = []
	Â Â Â Â Â Â Â Â hit = 0
	Â 
	Â Â Â Â Â Â Â Â # ç»Ÿè®¡é¢„æµ‹ç»“æœ
	Â Â Â Â Â Â Â Â for i, uid in tqdm(enumerate(test_model_input['user_id'])):
	Â Â Â Â Â Â Â Â Â Â Â Â try:
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred = [item_info['item_id'].value[x] for x in I[i]]
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â recall_score = recall_N(test_true_label[uid], pred, N=50)
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â s.append(recall_score)
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if test_true_label[uid] in pred:
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â hit += 1
	Â Â Â Â Â Â Â Â Â Â Â Â except:
	Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â print(i)
	Â 
	Â Â Â Â Â Â Â Â # è®¡ç®—å¬å›ç‡å’Œå‘½ä¸­ç‡
	Â Â Â Â Â Â Â Â recall = np.mean(s)
	Â Â Â Â Â Â Â Â hit_rate = hit / len(test_model_input['user_id'])
	Â 
	Â Â Â Â Â Â Â Â return recall, hit_rate
	Â 
	Â Â Â Â def scheduler(self):
	Â Â Â Â Â Â Â Â # æ„å»ºè®­ç»ƒé›†ã€æµ‹è¯•é›†
	Â Â Â Â Â Â Â Â train_model_input, train_label, test_model_input, test_label, \
	Â Â Â Â Â Â Â Â train_set, test_set, user_info, item_info = self.training_set_construct()
	Â Â Â Â Â Â Â Â #
	Â Â Â Â Â Â Â Â self.training_model(train_model_input, train_label)
	Â 
	Â Â Â Â Â Â Â Â # è·å–ç”¨æˆ·ã€itemçš„layer
	Â Â Â Â Â Â Â Â # user_embs, item_embs = self.extract_embedding_layer(model, test_model_input, item_info)
	Â Â Â Â Â Â Â Â # # è¯„ä¼°æ¨¡å‹
	Â Â Â Â Â Â Â Â # recall, hit_rate = self.eval(user_embs, item_embs, test_model_input, item_info, test_set)
	Â Â Â Â Â Â Â Â # print(recall, hit_rate)
	Â 
	Â 
	if __name__ == '__main__':
	Â Â Â Â model = YoutubeModel()
	Â Â Â Â model.scheduler()
```

æˆ‘æ¥è¯¦ç»†åœ°è§£é‡Šä¸‹è¿™æ®µä»£ç ã€‚é¦–å…ˆæ ¹æ®å¯¼å…¥çš„æ¨¡å—ï¼Œå¯ä»¥çœ‹å‡ºè¿™æ®µä»£ç ä¸»è¦ä½¿ç”¨äº†ä¸‹é¢è¡¨æ ¼é‡Œçš„å‡ ä¸ªå·¥å…·å’Œåº“ã€‚

![](https://static001.geekbang.org/resource/image/86/22/868fa24203ced6e3e74208bf0c178c22.jpg?wh=2628x1934)

é¦–å…ˆæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„ä»£ç åŠ è½½æ•°æ®ã€‚

```plain
	data = pd.read_csv('../../data/read_history.csv')
```

è¿™è¡Œä»£ç ä½¿ç”¨Pandasåº“æ¥è¯»å–CSVæ ¼å¼çš„å†å²é˜…è¯»è®°å½•æ•°æ®æ–‡ä»¶ï¼Œå°†å…¶å­˜å‚¨åˆ°dataè¿™ä¸ªDataFrameå¯¹è±¡ä¸­ã€‚

ç„¶åæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œç‰¹å¾ç¼–ç ã€‚

```plain
features = ["user_id", "item_id", "gender", "age", "city"]
features_max_idx = {}
for feature in features:
    lbe = LabelEncoder()
    data[feature] = lbe.fit_transform(data[feature]) + 1
    features_max_idx[feature] = data[feature].max() + 1
```

è¿™æ®µä»£ç ä½¿ç”¨sklearn.preprocessing.LabelEncoderå¯¹åŸå§‹æ•°æ®çš„å‡ ä¸ªç‰¹å¾è¿›è¡Œç¼–ç ï¼Œå°†è¿ç»­æˆ–ç¦»æ•£çš„ç‰¹å¾è½¬åŒ–ä¸ºæ•´æ•°ç±»å‹ã€‚è¿™é‡Œç¼–ç çš„ç‰¹å¾åŒ…æ‹¬user\_idã€item\_idã€genderã€ageã€cityã€‚å°†ç‰¹å¾ç¼–ç åï¼Œå°†æœ€å¤§ç´¢å¼•å€¼ä¿å­˜åˆ°features\_max\_idxå­—å…¸ä¸­ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥æ„å»ºäº†æ•°æ®é›†ã€‚

```plain
train_set, test_set = gen_data_set(data, negsample)
```

è¿™è¡Œä»£ç ä½¿ç”¨gen\_data\_setå‡½æ•°å°†åŸå§‹æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ŒåŒæ—¶è¿›è¡Œè´Ÿé‡‡æ ·æ“ä½œã€‚è¯¥å‡½æ•°çš„è¾“å…¥å‚æ•°ä¸ºåŸå§‹æ•°æ®å’Œè´Ÿé‡‡æ ·ä¸ªæ•°ã€‚è¾“å‡ºç»“æœä¸ºç»è¿‡è´Ÿé‡‡æ ·åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

ç„¶åæˆ‘ä»¬å°±å¯ä»¥è°ƒç”¨ä¹‹å‰çš„gen\_model\_inputå‡½æ•°å°†è®­ç»ƒé›†å’Œæµ‹è¯•é›†è½¬åŒ–ä¸ºæ¨¡å‹çš„è¾“å…¥æ ¼å¼ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†/æµ‹è¯•é›†çš„ç”¨æˆ·IDã€å†å²ç‰©å“IDåºåˆ—ã€å†å²ç‰©å“IDåºåˆ—çš„é•¿åº¦å’Œå¾…é¢„æµ‹ç‰©å“IDã€‚è¿™äº›æ•°æ®ä¼šä½œä¸ºè®­ç»ƒæ¨¡å‹çš„è¾“å…¥ã€‚

```plain
train_model_input, train_label = gen_model_input(train_set, user_info, self.SEQ_LEN) 
test_model_input, test_label = gen_model_input(test_set, user_info, self.SEQ_LEN)
```

æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨deepctråº“ä¸­çš„SparseFeatå’ŒVarLenSparseFeatå‡½æ•°ï¼Œåˆ†åˆ«æ„å»ºäº†ç”¨æˆ·å’Œç‰©å“çš„ç‰¹å¾è¾“å…¥ã€‚å…¶ä¸­SparseFeatè¡¨ç¤ºç¦»æ•£ç‰¹å¾ï¼ŒVarLenSparseFeatè¡¨ç¤ºå˜é•¿ç‰¹å¾ã€‚å…·ä½“åœ°ï¼Œç”¨æˆ·ç‰¹å¾è¾“å…¥ç”±4ä¸ªç¦»æ•£ç‰¹å¾å’Œä¸€ä¸ªå˜é•¿ç‰¹å¾ï¼ˆå†å²ç‰©å“IDåºåˆ—ï¼‰ç»„æˆï¼Œç‰©å“ç‰¹å¾è¾“å…¥åªæœ‰ä¸€ä¸ªç¦»æ•£ç‰¹å¾ï¼ˆç‰©å“IDï¼‰ã€‚

```plain
# ç”¨æˆ·ç«¯ç‰¹å¾è¾“å…¥
self.user_feature_columns = [SparseFeat('user_id', features_max_idx['user_id'], 16),
                             SparseFeat('gender', features_max_idx['gender'], 16),
                             SparseFeat('age', features_max_idx['age'], 16),
                             SparseFeat('city', features_max_idx['city'], 16),
                             VarLenSparseFeat(SparseFeat('hist_item_id', features_max_idx['item_id'],
                                                         self.embedding_dim, embedding_name='item_id'),
                                              self.SEQ_LEN, 'mean', 'hist_len')
                             ]
# ç‰©å“ç«¯çš„ç‰¹å¾è¾“å…¥
self.item_feature_columns = [SparseFeat('item_id', features_max_idx['item_id'], self.embedding_dim)]
```

ç„¶åæˆ‘ä»¬ä½¿ç”¨deepmatchåº“æ„å»ºäº†å«æœ‰DNNçš„YouTubeæ¨èæ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„è¾“å…¥ç”±ä¸Šä¸€æ­¥å®šä¹‰çš„ç”¨æˆ·å’Œç‰©å“ç‰¹å¾è¾“å…¥ç»„æˆï¼Œå…¶ä¸­num\_sampledè¡¨ç¤ºåˆ†ç±»å™¨ä½¿ç”¨çš„é‡‡æ ·ç‚¹çš„æ•°ç›®ã€‚åœ¨æ¨¡å‹æ„å»ºå’Œç¼–è¯‘åï¼Œä½¿ç”¨fitå‡½æ•°è¿›è¡Œè®­ç»ƒã€‚

```plain
# å®šä¹‰æ¨¡å‹
model = YouTubeDNN(self.user_feature_columns, self.item_feature_columns, num_sampled=100,
                   user_dnn_hidden_units=(128, 64, self.embedding_dim))
model.compile(optimizer="adam", loss=sampledsoftmaxloss)
# ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®
model.fit(train_model_input, train_label, batch_size=512, epochs=20, verbose=1, validation_split=0.0,)
```

æœ€åï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æå–ç”¨æˆ·å’Œç‰©å“çš„Embedding Layerï¼Œä»¥ä¾¿åç»­è®¡ç®—å¬å›ç‡å’Œå‘½ä¸­ç‡ã€‚å…·ä½“åœ°ï¼Œä½¿ç”¨Modelå‡½æ•°å°†æ¨¡å‹çš„è¾“å…¥å’Œå®ƒçš„ç”¨æˆ·/ç‰©å“Embeddingå±‚å…³è”èµ·æ¥ï¼Œç„¶åè°ƒç”¨predictå‡½æ•°è®¡ç®—å¾—åˆ°é¢„æµ‹ç»“æœã€‚

```plain
user_embs = user_embedding_model.predict(test_model_input, batch_size=2 ** 12) 
item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)
```

å®é™…ä¸Šï¼Œåˆ°è¿™é‡Œæ•´ä¸ªæ•°æ®å¤„ç†å’Œè®­ç»ƒéƒ¨åˆ†çš„ä»£ç å°±å·²ç»ç»“æŸäº†ï¼Œæ¥ä¸‹æ¥ï¼Œå°±æ˜¯è¦åšå¬å›ç‡å’Œå‘½ä¸­ç‡çš„è®¡ç®—ã€‚åœ¨è¿™ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬åˆ©ç”¨Faissåº“è®¡ç®—ç”¨æˆ·å’Œç‰©å“Embedding Layerä¹‹é—´çš„è¿‘é‚»å…³ç³»ï¼Œå¹¶æ ¹æ®é¢„æµ‹çš„ç‰©å“åˆ—è¡¨è®¡ç®—å¬å›ç‡å’Œå‘½ä¸­ç‡ã€‚å…·ä½“æ¥è¯´å°±æ˜¯æ ¹æ®ç”¨æˆ·IDç´¢å¼•åˆ°å¯¹åº”çš„Embeddingå‘é‡ï¼Œç„¶ååœ¨ç‰©å“Embeddingå‘é‡é›†åˆä¸­æœç´¢è¿‘é‚»ï¼Œå¾—åˆ°é¢„æµ‹çš„ç‰©å“åˆ—è¡¨ã€‚æœ€åï¼Œæ ¹æ®é¢„æµ‹çš„ç‰©å“åˆ—è¡¨å’ŒçœŸå®çš„ç‰©å“IDï¼Œè®¡ç®—å¬å›ç‡å’Œå‘½ä¸­ç‡ã€‚

```plain
def eval(self, user_embs, item_embs, test_model_input, item_info, test_set):
Â Â Â test_true_label = {line[0]: line[2] for line in test_set}
Â Â Â index = faiss.IndexFlagIP(self.embedding_dim)
Â Â Â index.add(item_embs)
Â Â Â D, I = index.search(np.ascontiguousarray(user_embs), 50)
Â Â Â s = []
Â Â Â hit = 0

Â Â Â # ç»Ÿè®¡é¢„æµ‹ç»“æœ
Â Â Â for i, uid in tqdm(enumerate(test_model_input['user_id'])):
Â Â Â Â Â Â Â try:
Â Â Â Â Â Â Â Â Â Â Â pred = [item_info['item_id'].value[x] for x in I[i]]
Â Â Â Â Â Â Â Â Â Â Â recall_score = recall_N(test_true_label[uid], pred, N=50)
Â Â Â Â Â Â Â Â Â Â Â s.append(recall_score)
Â Â Â Â Â Â Â Â Â Â Â if test_true_label[uid] in pred:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â hit += 1
Â Â Â Â Â Â Â except:
Â Â Â Â Â Â Â Â Â Â Â print(i)

Â Â Â # è®¡ç®—å¬å›ç‡å’Œå‘½ä¸­ç‡
Â Â Â recall = np.mean(s)
Â Â Â hit_rate = hit / len(test_model_input['user_id'])

Â Â Â return recall, hit_rate
Â æ•´ä¸ªæµç¨‹å®é™…ä¸Šåˆ°è¿™é‡Œå°±ç»“æŸäº†ï¼Œé‚£ä¹ˆæœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªschedulerå‡½æ•°å°†å®ƒä»¬ä¸²èµ·æ¥ï¼š
def scheduler(self):
Â Â Â # æ„å»ºè®­ç»ƒé›†ã€æµ‹è¯•é›†
Â Â Â train_model_input, train_label, test_model_input, test_label, \
Â Â Â train_set, test_set, user_info, item_info = self.training_set_construct()
Â Â Â #
Â Â Â self.training_model(train_model_input, train_label)

Â Â Â # è·å–ç”¨æˆ·ã€itemçš„layer
Â Â Â # user_embs, item_embs = self.extract_embedding_layer(model, test_model_input, item_info)
Â Â Â # # è¯„ä¼°æ¨¡å‹
Â Â Â # recall, hit_rate = self.eval(user_embs, item_embs, test_model_input, item_info, test_set)
Â Â Â # print(recall, hit_rate)
Â 
```

è¿™é‡Œæœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼ŒFaissåº“ç›®å‰åœ¨Windowsä¸Šæ— æ³•ä½¿ç”¨ï¼Œå¿…é¡»åœ¨Linuxä¸Šæ‰è¡Œã€‚å› æ­¤ï¼Œåœ¨æœ€åçš„Scheduleé˜¶æ®µï¼Œæˆ‘å°†è¿™æ®µä»£ç è¿›è¡Œäº†æ³¨é‡Šã€‚

æ•´ä¸ªYouTubeDNNçš„å¬å›å±‚è®­ç»ƒå’Œé¢„æµ‹åˆ°è¿™é‡Œå°±ç»“æŸäº†ã€‚

## æ€»ç»“

åˆ°è¿™é‡Œï¼Œä»Šå¤©çš„è¯¾ç¨‹å°±è®²å®Œäº†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥å¯¹ä»Šå¤©çš„è¯¾ç¨‹åšä¸€ä¸ªç®€å•çš„æ€»ç»“ï¼Œå­¦å®Œæœ¬èŠ‚è¯¾ä½ åº”è¯¥çŸ¥é“ä¸‹é¢ä¸‰å¤§è¦ç‚¹ã€‚

1. åœ¨YouTubeDNNä¸­ï¼Œæ•°æ®å¤„ç†ä¼šç»è¿‡åŠ è½½æ•°æ®é›†ã€å¤„ç†æ•°æ®ç‰¹å¾ã€ç‰¹å¾è½¬åŒ–ä¸ºæ¨¡å‹è¾“å…¥ã€æ¨¡å‹çš„æ­å»ºå’Œè®­ç»ƒã€æ¨¡å‹è¯„ä¼°è¿™5ä¸ªéƒ¨åˆ†ã€‚
2. YouTubeDNNæ¨¡å‹é€šè¿‡å°†ç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—åµŒå…¥åˆ°ä½ç»´å‘é‡ç©ºé—´ä¸­ï¼Œæ¥å­¦ä¹ ç”¨æˆ·å’Œç‰©å“ä¹‹é—´çš„å…³ç³»ã€‚å®ƒçš„è¾“å…¥åŒ…æ‹¬ç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—ä»¥åŠç‰©å“IDï¼Œè¾“å‡ºåŒ…æ‹¬ç”¨æˆ·å’Œç‰©å“çš„åµŒå…¥å‘é‡ä»¥åŠå®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚
3. ç†Ÿæ‚‰ä½¿ç”¨Pythonæ¥æ­å»ºä¸€æ•´å¥—YouTubeDNNæ¨¡å‹ä»£ç ã€‚

## è¯¾åé¢˜

æœ¬èŠ‚è¯¾å­¦å®Œäº†ï¼Œæˆ‘æ¥ç»™ä½ ç•™ä¸¤é“è¯¾åé¢˜ã€‚

1. å®ç°æœ¬èŠ‚è¯¾çš„ä»£ç ã€‚
2. æ ¹æ®æˆ‘ä»¬å‰é¢çš„çŸ¥è¯†ï¼Œè‡ªåŠ¨ç”Ÿæˆæ•°æ®é›†ã€‚

æ¬¢è¿ä½ åœ¨ç•™è¨€åŒºä¸æˆ‘äº¤æµè®¨è®ºï¼Œå¦‚æœè¿™èŠ‚è¯¾å¯¹ä½ æœ‰å¸®åŠ©ï¼Œä¹Ÿæ¬¢è¿ä½ æ¨èç»™æœ‹å‹ä¸€èµ·å­¦ä¹ ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ6ï¼‰</strong></div><ul>
<li><span>é™å¿ƒ</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æœ‰ä¸¤ä¸ªé—®é¢˜æ²¡å¼„æ˜ç™½ï¼Œè¯·è€å¸ˆæŒ‡ç‚¹ï¼š
1ã€embedding_dimçš„å¤§å°æ˜¯å¦‚ä½•ç¡®å®šçš„ï¼Ÿ
2ã€ä»€ä¹ˆæ—¶å€™ç”¨ç¦»æ•£ç‰¹å¾ï¼Œä»€ä¹ˆæ—¶å€™ç”¨å˜é•¿ç‰¹å¾ï¼Ÿ</p>2023-09-11</li><br/><li><span>çˆ±æå®¢</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è€å¸ˆï¼Œåé¢ä¼šå‡ºä¸€ç¯‡è¯¾åç­”ç–‘çš„æ–‡ç« å—ï¼Ÿ</p>2023-06-06</li><br/><li><span>alexliu</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œfaiss.IndexFlagIPè¿™ä¸ªåº”è¯¥æ˜¯faiss.IndexFlatIPå§ï¼Ÿå¦å¤–index.add(n,x)æœ‰ä¸¤ä¸ªå‚æ•°ï¼Œä¸ºä»€ä¹ˆåœ¨ä»£ç é‡Œåªæœ‰item_embsä¸€ä¸ªå‚æ•°ï¼Ÿ
psï¼šaddæºç å¦‚ä¸‹ï¼š
    def add(self, n, x):
        r&quot;&quot;&quot; default add uses sa_encode&quot;&quot;&quot;
        return _swigfaiss.IndexFlatCodes_add(self, n, x)</p>2023-06-07</li><br/><li><span>peter</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>YoutubeDNNæ˜¯æ‹¿æ¥å°±èƒ½ç”¨å—ï¼Ÿç±»ä¼¼äºå·¥å…·è½¯ä»¶é‚£ç§ï¼Œä¸éœ€è¦å¼€å‘ã€‚</p>2023-06-06</li><br/><li><span>Emma</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è¯·é—®è€å¸ˆï¼ŒyoutubeDNNçš„æ’åºéƒ¨åˆ†çš„ä»£ç æœ‰å—</p>2024-09-29</li><br/><li><span>çˆ±æå®¢</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p># å®šä¹‰æ¨¡å‹
        model = YoutubeDNN(self.user_feature_columns, self.item_feature_columns, num_sampled=100,
                           user_dnn_hidden_units=(128, 64, self.embedding_dim))  

è¿™ä¸ªå‚æ•° num_sampled=100 åœ¨æ–°ç‰ˆçš„æ¨¡å‹APIé‡Œé¢æ˜¯æ²¡æœ‰çš„ï¼Œå¸Œæœ›è€å¸ˆè§£ç­”</p>2024-06-30</li><br/>
</ul>