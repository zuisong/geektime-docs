ä½ å¥½ï¼Œæˆ‘æ˜¯é»„é¸¿æ³¢ã€‚

ä¸ŠèŠ‚è¯¾ï¼Œæˆ‘ä»¬ä½¿ç”¨Scrapyæ¡†æ¶å·²ç»èƒ½å¤Ÿçˆ¬å–äº†æ–°æµªç½‘çš„æ–°é—»æ•°æ®ï¼Œå¹¶ä¸”ï¼Œæˆ‘ä»¬ä¹Ÿåšäº†ç›¸åº”çš„ç¿»é¡µçˆ¬å–åŠŸèƒ½ã€‚

è¿™èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°±åœ¨ä¸Šä¸€èŠ‚è¯¾çš„ç¨‹åºä¸­åšä¸€ä¸ªè¡¥å……ï¼ŒåŠ å…¥å‚æ•°ä¼ é€’å’Œæ•°æ®åº“å­˜å‚¨ç›¸å…³åŠŸèƒ½ï¼ˆä½¿ç”¨MongoDBæ•°æ®åº“è¿›è¡Œå­˜å‚¨ï¼‰ã€‚

## Pythonä¸­çš„pymongoåº“

å¦‚æœè¦æƒ³åœ¨Pythonä¸­æ“ä½œMongoDBæ•°æ®åº“ï¼Œé¦–å…ˆæˆ‘ä»¬è¦äº†è§£ä¸€ä¸‹pymongoè¿™ä¸ªåº“ã€‚

pymongoå‡†ç¡®æ¥è®²æ˜¯ä¸€ä¸ªè¿æ¥Pythonå’ŒMongoDBæ•°æ®åº“çš„ä¸€ä¸ªä¸­é—´çš„é©±åŠ¨ç¨‹åºï¼Œè¿™ä¸ªç¨‹åºå¯ä»¥ä½¿Pythonèƒ½å¤Ÿéå¸¸æ–¹ä¾¿åœ°ä½¿ç”¨å’Œæ“ä½œMongoDBæ•°æ®åº“ã€‚åœ¨Pythonä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨pip install pymongoçš„æ–¹å¼æ¥å®‰è£…ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±åœ¨æˆ‘ä»¬çš„cmdç¯å¢ƒä¸­æ¥å®‰è£…æˆ‘ä»¬çš„pymongoåº“ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åˆ‡æ¢åˆ°æˆ‘ä»¬çš„anacondaç¯å¢ƒä¸­ã€‚

```
activate scrapy_recommendation
```

å¦‚æœä½ æ˜¯Linuxæˆ–è€…Macç”¨æˆ·ï¼Œåˆ™éœ€è¦æŠŠå‘½ä»¤æ”¹æˆä¸‹é¢è¿™æ ·ã€‚

```
conda activate scrapy_recommendation
```

ç´§æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æ¥å®‰è£…pymongoã€‚

```
pip install pymongo
```

å®‰è£…å®Œæˆä¹‹åï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](https://static001.geekbang.org/resource/image/f7/30/f70dd8653c160486e21a8c0ab25d4430.png?wh=1443x417)

æ¥ç€ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°è¯•ç€åœ¨æˆ‘ä»¬çš„Pythonç¯å¢ƒä¸­ä½¿ç”¨å®ƒã€‚

æƒ³è¦åœ¨Pythonç¯å¢ƒä¸­ä½¿ç”¨pymongoåº“ï¼Œæˆ‘ä»¬éœ€è¦ç»è¿‡ä¸‹é¢å››ä¸ªæ­¥éª¤ã€‚

1. å¯¼å…¥pymongoåº“ã€‚
2. è¿æ¥MongoDBæ•°æ®åº“ã€‚
3. é€‰æ‹©æ•°æ®è¡¨ã€‚
4. å¯¹æ•°æ®è¡¨è¿›è¡Œå¢åˆ æ”¹æŸ¥çš„æ“ä½œã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬å°±ä»ä¸Šé¢å››ä¸ªæ­¥éª¤å…¥æ‰‹ï¼Œå†™ä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚

é¦–å…ˆæ˜¯å¯¼å…¥pymongoè¿™ä¸ªåº“ï¼Œåœ¨å¯¼å…¥è¿™ä¸ªåº“çš„æ—¶å€™ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨importè¿›è¡Œå¯¼å…¥å³å¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ­£å¼å¯¹MongoDBæ•°æ®åº“è¿›è¡Œè¿æ¥å’Œæ“ä½œã€‚

ä¸€èˆ¬æ¥è®²å¯¹äºä»»ä½•Pythonç¨‹åºï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦åšæˆå·¥ç¨‹åŒ–çš„å½¢å¼éƒ½ä¼šå…ˆå»ºç«‹ä¸€ä¸ªç±»ï¼Œç„¶åå†åœ¨ç±»ä¸­é‡å†™å®ƒçš„\_\_init\_\_å‡½æ•°ï¼Œå°†æˆ‘ä»¬éœ€è¦åˆå§‹åŒ–çš„å†…å®¹åœ¨è¿™é‡Œè¿›è¡Œåˆå§‹åŒ–ã€‚å¯¹äºæ•°æ®åº“æ¥è¯´ï¼Œä¸€èˆ¬æˆ‘ä»¬éœ€è¦åˆå§‹åŒ–çš„éƒ¨åˆ†å°±æ˜¯æ•°æ®åº“çš„è¿æ¥ã€é€‰æ‹©ï¼Œä»¥åŠå¯¹äºæ•°æ®è¡¨çš„é€‰æ‹©ã€‚

å¦‚æœæˆ‘ä»¬æƒ³è¦è¿æ¥æ•°æ®åº“å°±éœ€è¦ä¸€ç³»åˆ—çš„å‚æ•°ï¼ŒåŒ…æ‹¬æ•°æ®åº“æ‰€åœ¨çš„æœåŠ¡å™¨IPåœ°å€ã€ç«¯å£å·ã€ç”¨æˆ·åã€å¯†ç ä»¥åŠæ•°æ®åº“åã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒMongoDBå¯ä»¥è®¾ç½®ç”¨æˆ·åå¯†ç ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é»˜è®¤çš„ç”¨æˆ·åå¯†ç ã€‚å½“ä½¿ç”¨é»˜è®¤çš„ç”¨æˆ·åå¯†ç æ—¶ï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦è¾“å…¥å®ƒä»¬ï¼Œå› æ­¤è¿™é‡Œä¼šåˆ†æˆæœ‰ç”¨æˆ·åå¯†ç å’Œæ— ç”¨æˆ·åå¯†ç ä¸¤ç§æ–¹å¼ã€‚

å¦å¤–ï¼ŒMongoDBæ•°æ®åº“çš„è¿æ¥éœ€è¦éµå¾ªMongoDBçš„é€šè®¯åè®®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼Œæˆ‘ç»™ä½ ç”»äº†ä¸€ä¸ªè¡¨æ ¼ç®€è¦è§£æäº†è¿™è¡Œä»£ç ï¼Œä½ å¯ä»¥å¯¹ç…§ç€è¿›è¡Œå­¦ä¹ ã€‚

```
client = 'mongodb://' + user + ':' + pwd + '@' + host + ":" + str(port) + "/" + db
```

![](https://static001.geekbang.org/resource/image/cf/28/cfb528d0a2e851027946ca690775a428.jpg?wh=2160x1808)

åœ¨ä¸Šé¢çš„ä»£ç ä¸­å¯ä»¥æœ‰userå’Œpwdä¹Ÿå¯ä»¥æ²¡æœ‰ã€‚å› æ­¤ï¼Œæ•´ä¸ªMongoDBè¿æ¥çš„æ‹¼æ¥ç¨‹åºå°±å¯ä»¥å†™æˆä»¥ä¸‹å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°æˆ‘ä»¬éœ€è¦æ”¾åˆ°è¿æ¥æ•°æ®åº“çš„å‡½æ•°ä¸­ï¼Œä½œä¸ºæ•°æ®åº“è¿æ¥çš„ä¸€éƒ¨åˆ†ã€‚

```
def _splicing(host, port, user, pwd, db):
	client = 'mongodb://' + host + ":" + str(port) + "/"
	if user != '':
		client = 'mongodb://' + user + ':' + pwd + '@' + host + ":" + str(port) + "/"
		if db != '':
			client += db
	return client
```

åœ¨ä½¿ç”¨pymongoè¿›è¡Œæ•°æ®åº“è¿æ¥æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨pymongoä¸­çš„MongoClientç±»ï¼Œè¿™æ—¶éœ€è¦åˆ›å»ºä¸€ä¸ªMongoDBçš„å®¢æˆ·ç«¯ï¼Œç„¶åç”¨å®¢æˆ·ç«¯æ¥è¿æ¥MongoDBæ•°æ®åº“ï¼Œæ•´ä¸ªç±»å°±ä¼šå˜æˆä¸‹é¢è¿™ä¸ªæ ·å­ã€‚

```
import pymongo
 
class MongoDB(object):
    def __init__(self, db):
        mongo_client = self._connect('127.0.0.1', 27017, '', '', db)
        self.db_scrapy = mongo_client['scrapy_data']
        self.collection_test = self.db_scrapy['test_collections']
 
    def _connect(self, host, port, user, pwd, db):
        mongo_info = self._splicing(host, port, user, pwd, db)
        mongo_client = pymongo.MongoClient(mongo_info, connectTimeoutMS=12000, connect=False)
        return mongo_client
 
    @staticmethod
    def _splicing(host, port, user, pwd, db):
        client = 'mongodb://' + host + ":" + str(port) + "/"
        if user != '':
            client = 'mongodb://' + user + ':' + pwd + '@' + host + ":" + str(port) + "/"
            if db != '':
                client += db
        return client
```

æˆ‘ä»¬å†æ¥æ•´ä½“è§£è¯»ä¸€ä¸‹è¿™æ®µä»£ç ã€‚å®é™…ä¸Šè¿™æ®µä»£ç å°±æ˜¯ä¸€æ®µMongoDBçš„è¿æ¥ç±»ï¼Œä¸€èˆ¬ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ä¼šæŠŠå®ƒå•ç‹¬æ‹¿å‡ºæ¥ï¼Œå¹¶åœ¨æˆ‘ä»¬é¡¹ç›®çš„ç›®å½•ä¸­æ–°å»ºä¸€ä¸ªåä¸ºdaoçš„ç›®å½•ï¼Œå¹¶ä¸”æ–‡ä»¶å‘½åä¸ºmongo\_db.pyï¼Œåœ¨è¿™é‡Œæˆ‘çš„ç›®å½•ç»“æ„å°±æ˜¯sina\\sina\\dao\\mongo\_db.pyã€‚

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬åœ¨\_\_init\_\_å‡½æ•°ä¸­åˆå§‹åŒ–äº†å‡ ä¸ªå˜é‡ï¼Œåˆ†åˆ«æ˜¯ç”¨äºè¿æ¥çš„mongo\_clientã€åˆå§‹åŒ–æ•°æ®åº“scrapy\_dataä»¥åŠä¸€ä¸ªæµ‹è¯•collectionä¸ºtest\_collectionã€‚

## åœ¨Scrapyä¸­å¯¹æ•°æ®è¿›è¡Œå¤„ç†å’Œä¿å­˜

ç°åœ¨æˆ‘ä»¬å·²ç»å†™äº†ä¸€ä¸ªç®€å•çš„MongoDBæ•°æ®åº“çš„è¿æ¥ç±»ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦åœ¨Scrapyä¸­ä½¿ç”¨å®ƒæ¥è¿›è¡Œæ•°æ®çš„å­˜å–å·¥ä½œã€‚åœ¨Scrapyä¸­ï¼Œç®¡é“çš„åŠŸèƒ½æ˜¯è´Ÿè´£å¤„ç†Spiderä¸­è·å–åˆ°çš„Itemï¼Œå¹¶è¿›è¡ŒåæœŸå¤„ç†ï¼ˆæ¯”å¦‚å¯¹æ•°æ®è¿›è¡Œåˆ†æã€è¿‡æ»¤ã€å­˜å‚¨ç­‰ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨åšæ•°æ®å­˜å‚¨å’Œå¤„ç†çš„éƒ¨åˆ†å°±åº”è¯¥å†™åœ¨Scrapyæ¡†æ¶çš„pipelines.pyæ–‡ä»¶ä¸­ï¼Œè¿™ä¸ªæ–‡ä»¶çš„åˆå§‹ä»£ç å¦‚ä¸‹ã€‚

```
from itemadapter import ItemAdapter
 
class SinaPipeline:
    def process_item(self, item, spider):
        return item
```

è¿™æ®µä»£ç éå¸¸ç®€å•ï¼Œå°±æ˜¯å®šä¹‰äº†ä¸€ä¸ªé»˜è®¤çš„pipelineï¼Œç„¶åé»˜è®¤å¸¦äº†ä¸€ä¸ªprocess\_itemå‡½æ•°ï¼Œæˆ‘ä»¬å¤„ç†æ•°æ®å°±æ˜¯åœ¨è¿™ä¸ªå‡½æ•°ä¸­æ¥è¿›è¡Œå¤„ç†ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬å°±è¦æ¥**è¿™ä¸ªæ–‡ä»¶è¿›è¡Œæ”¹é€ ï¼Œå°†pymongoåŠ å…¥åˆ°è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œå¹¶æŠŠçˆ¬è™«çˆ¬å–åˆ°çš„æ•°æ®å­˜å…¥è¿›å»ã€‚**

åœ¨æ”¹é€ ä¹‹å‰æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸‹ä¸Šé¢çš„process\_itemè¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨è¿™ä¸ªå‡½æ•°çš„å‚æ•°ä¼ é€’éƒ¨åˆ†ä¸€å…±ä¼ å…¥äº†2ä¸ªå€¼ï¼Œåˆ†åˆ«æ˜¯Itemå’Œspiderï¼Œè¿™ä¸ªItemå®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬çˆ¬å–çš„æ•°æ®ã€‚

![](https://static001.geekbang.org/resource/image/02/b8/02ce18db3937d494e05ddcbdd60ee1b8.png?wh=1766x1039)

æˆ‘ä»¬è¿˜æ˜¯å…ˆæ¥çœ‹çœ‹è¿™å¼ ç»å…¸çš„æµç¨‹å›¾ã€‚åœ¨ä¸Šä¸€èŠ‚è¯¾ä¸­ï¼Œæˆ‘ä»¬åœ¨çˆ¬è™«ä»£ç çš„parse\_namedetail()å‡½æ•°ä¸­yieldå‡ºå»äº†ä¸€ä¸ªItemï¼Œè¿™ä¸ªyieldå‡ºå»çš„Itemå®é™…ä¸Šä¼šè¢«ä¼ å…¥åˆ°Item Pipelineä¸­ï¼Œè€Œè¿™ä¸ªItem Pipelineå®é™…ä¸Šå°±æ˜¯å¯¹åº”ç€æˆ‘ä»¬åˆšåˆšçš„pipelines.pyæ–‡ä»¶ã€‚

åœ¨pipelines.pyæ–‡ä»¶ä¸­æœ‰ä¸€ä¸ªå‡½æ•°process\_item()ï¼Œè¿™é‡Œæ‰€ä¼ å…¥çš„Itemå‚æ•°ï¼Œå®é™…ä¸Šå°±æ˜¯ä»çˆ¬è™«æ–‡ä»¶ä¸­yieldå‡ºæ¥çš„Itemã€‚æ¸…æ¥šäº†è¿™ä¸ªé€»è¾‘ä¹‹åä½ ä¼šå‘ç°ï¼Œå®é™…ä¸Šè¿™é‡Œæˆ‘ä»¬æ‰€æ‹¿åˆ°çš„æ•°æ®å°±æ˜¯çˆ¬è™«çˆ¬å–å›æ¥çš„æ•°æ®ã€‚è¿™ä¸ªæ—¶å€™è¦åšçš„å°±æ˜¯æ¥æ”¶è¿™ä¸ªæ•°æ®ï¼Œç„¶åå°†è¿™ä¸ªæ•°æ®ç»™å­˜å…¥MongoDBæ•°æ®åº“ä¸­ã€‚

å¦‚æœæƒ³åœ¨pipelines.pyæ–‡ä»¶ä¸­ä½¿ç”¨MongoDBæ•°æ®åº“ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšçš„å°±æ˜¯å¯¼å…¥æˆ‘ä»¬å†™å¥½çš„pymongoè¿æ¥çš„ç±»ï¼Œå¹¶é€‰æ‹©collectionç„¶åå°†æˆ‘ä»¬éœ€è¦çš„æ•°æ®æ’å…¥è¿›å»ã€‚ å› æ­¤åœ¨é¡¶éƒ¨æˆ‘ä»¬éœ€è¦ä½¿ç”¨importæ¥å¯¼å…¥æˆ‘ä»¬çš„ç±»ã€‚

```
from .dao.mongo_db import MongoDB
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨åŒ…çš„å‰é¢åŠ äº†ä¸ªâ€œ.â€ï¼Œè¯´æ˜è¿™ä¸ªæ˜¯åœ¨å½“å‰ç›®å½•ä¸‹çš„ã€‚å¯¼å…¥MongoDBæ•°æ®åº“ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å»é‡å†™æˆ‘ä»¬çš„SinaPipelineè¿™ä¸ªç±»äº†ã€‚å› ä¸ºä»£ç æ¯”è¾ƒç®€å•ï¼Œå› æ­¤æˆ‘ä»¬ç›´æ¥ä¸Šä»£ç ã€‚

```
from .dao.mongo_db import MongoDB
 
class SinaPipeline:
    def __init__(self):
        self.mongo = MongoDB(db='scrapy_data')
        self.collection = self.mongo.db_scrapy['content_ori']
 
    def process_item(self, item, spider):
        result_item = dict(item)
        self.collection.insert_one(result_item)
        return item
```

å¯¹äºæ¯”è¾ƒå¥½çš„å†™æ³•ï¼Œä»»ä½•ä¸€ä¸ªç±»æœ€å¥½éƒ½æœ‰ä¸€ä¸ª\_\_init\_\_å‡½æ•°ï¼ŒæŠŠæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–çš„å†…å®¹éƒ½æ”¾åœ¨è¿™é‡Œã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬é‡å†™äº†\_\_init\_\_å‡½æ•°ï¼Œå¹¶æŠŠMongoDBçš„è¿æ¥ä»¥åŠæŠŠè¦æ’å…¥çš„collectionè¿›è¡Œäº†å®šä¹‰ã€‚

è¿™é‡Œæˆ‘ä»¬é€‰æ‹©çš„æ•°æ®åº“ä¸ºscrapy\_dataï¼Œå¹¶ä¸”é€‰æ‹©ä¸€ä¸ªcollectionä¸ºâ€œcontent\_oriâ€ã€‚ä¹‹å‰æˆ‘ä»¬è®²è¿‡ï¼Œåœ¨MongoDBä¸­å¦‚æœå·²ç»æœ‰ä¸€ä¸ªcollectionäº†ï¼Œé€‰æ‹©çš„æ—¶å€™å°±ä¼šç›´æ¥å»ç”¨ã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ªcollectionï¼Œåˆ™ä¼šè‡ªåŠ¨åˆ›å»ºcollectionã€‚ç›®å‰åœ¨æˆ‘ä»¬çš„æ•°æ®åº“ä¸­å¹¶æ²¡æœ‰åä¸ºâ€œcontent\_oriâ€è¿™ä¸ªcollectionï¼Œä¼šè‡ªåŠ¨åˆ›å»ºã€‚

æ¥ä¸‹æ¥åšçš„äº‹æƒ…å°±æ˜¯æŠŠprocess\_item()è¿™ä¸ªå‡½æ•°è¿›è¡Œé‡å†™ï¼Œé‡å†™è¿™ä¸ªå‡½æ•°å¾ˆç®€å•ï¼Œåªéœ€æŠŠæ•°æ®è½¬æ¢æˆMongoDBæ‰€éœ€è¦çš„bsonç±»å‹ï¼Œç„¶åæ’å…¥å³å¯ã€‚

MongoDBä¸­çš„bsonç±»å‹å®é™…ä¸Šå’ŒPythonä¸­çš„dictæ ¼å¼ä¸€æ ·ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å°±å°†Itemè½¬æ¢æˆdictç±»å‹ï¼Œå¹¶èµ‹å€¼ç»™result\_itemè¿™ä¸ªå˜é‡ï¼Œç„¶åè°ƒç”¨self.collection.insert\_one()è¿™ä¸ªå‡½æ•°æ¥è¿›è¡Œæ•°æ®çš„æ’å…¥ã€‚

## å°†pipelineä¸çˆ¬è™«ç¨‹åºè¿›è¡Œå…³è”

è¿™ä¸ªæ—¶å€™æ˜¯ä¸æ˜¯å°±å¯ä»¥å°†æ•°æ®æ’å…¥åˆ°æ•°æ®åº“ä¸­äº†å‘¢ï¼Ÿç†è®ºä¸Šæ˜¯çš„ï¼Œä½†æ˜¯å®é™…ä¸Šè¿˜å·®äº†ä¸€æ­¥ã€‚ç›®å‰æˆ‘ä»¬éœ€è¦çš„ä»£ç éƒ½å·²ç»å®Œæˆäº†ï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜ç¼ºå°‘æœ€é‡è¦çš„ä¸€æ­¥ï¼Œé‚£å°±æ˜¯**å°†pipelineå’Œæˆ‘ä»¬çš„çˆ¬è™«ç¨‹åºè¿›è¡Œå…³è”ï¼Œè¿™ä¸ªå…³è”çš„æ“ä½œåœ¨settings.pyæ–‡ä»¶ä¸­è¿›è¡Œã€‚**

æˆ‘ä»¬æ‰“å¼€settingsæ–‡ä»¶ï¼Œå‘ç°è¿™é‡Œé¢æœ‰å¾ˆå¤šå·²ç»å†™å¥½çš„ä»£ç ï¼Œå¹¶ä¸”éƒ½æ˜¯é…ç½®æ–‡ä»¶ï¼Œå¹¶ä¸”éƒ½åŠ äº†æ³¨é‡Šï¼Œæˆ‘ä»¬æŒ‘ä¸€äº›é‡è¦çš„æ¥å¯¹è¿™ä¸ªæ–‡ä»¶åšä¸€ä¸ªç®€å•çš„è§£æã€‚

```
# Scrapy settings for sina project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html
 
BOT_NAME = 'sina'
 
SPIDER_MODULES = ['sina.spiders']
NEWSPIDER_MODULE = 'sina.spiders'
 
 
# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = 'sina (+http://www.yourdomain.com)'
 
# Obey robots.txt rules
ROBOTSTXT_OBEY = True
 
# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32
 
# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16
 
# Disable cookies (enabled by default)
#COOKIES_ENABLED = False
 
# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False
 
# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}
 
# Enable or disable spider middlewares
# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    'sina.middlewares.SinaSpiderMiddleware': 543,
#}
 
# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    'sina.middlewares.SinaDownloaderMiddleware': 543,
#}
 
# Enable or disable extensions
# See https://docs.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
#}
 
# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
#ITEM_PIPELINES = {
#    'sina.pipelines.SinaPipeline': 300,
#}
 
# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False
 
# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
```

åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œç¬¬10è¡Œçš„BOT\_NAMEå®šä¹‰äº†æˆ‘ä»¬é¡¹ç›®çš„åå­—ã€‚ç¬¬12è¡Œå®šä¹‰äº†çˆ¬è™«çš„Scrapyæœç´¢spiderçš„æ¨¡å—åˆ—è¡¨ï¼Œåœ¨æˆ‘ä»¬è¿™é‡Œçš„åˆ—è¡¨ä¸­åªæœ‰ä¸€ä¸ªå†…å®¹ï¼Œé‚£å°±æ˜¯â€œsina.spidersâ€ï¼Œè¯´æ˜ç›®å‰æˆ‘ä»¬åªæœ‰è¿™ä¸ªä¸€ä¸ªçˆ¬è™«æ–‡ä»¶ã€‚è€Œç¬¬13è¡Œåˆ™æ˜¯ä½¿ç”¨genspiderå‘½ä»¤åˆ›å»ºæ–°çš„spiderçš„æ¨¡å—ã€‚

åœ¨ç¬¬20è¡Œä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ªROBOTSTXT\_OBEYå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°é€‰æ‹©æ˜¯å¦é‡‡ç”¨robots.txtç­–ç•¥ï¼Œè¿˜è®°å¾—robotsåè®®å—ï¼Ÿå®é™…ä¸Šå°±æ˜¯åœ¨è¿™é‡Œé…ç½®çš„ã€‚å¦‚æœè¿™é‡Œä¸ºTrueï¼Œé‚£ä¹ˆçˆ¬è™«å°±ä¼šéµå®ˆrobots.txtçš„è§„åˆ™ã€‚

åœ¨ç¬¬23è¡Œä¸­ï¼Œæˆ‘ä»¬ä¼šå‘ç°ä¸€ä¸ªå·²ç»è¢«æ³¨é‡Šæ‰çš„CONCURRENT\_REQUESTSè¿™ä¸ªå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å®é™…ä¸ŠæŒ‡çš„æ˜¯æˆ‘ä»¬çš„çˆ¬è™«çš„ä¸‹è½½å™¨ï¼Œä¹Ÿå°±æ˜¯scrapy downloaderå¹¶å‘è¯·æ±‚æ•°çš„æœ€å¤§å€¼ã€‚è¿™ä¸ªå‚æ•°å¼€å¯äº†ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨å¤šçº¿ç¨‹è¿›è¡Œçˆ¬å–ï¼Œè¿™ä¸ªå€¼ä¸€èˆ¬å»ºè®®è®¾ç½®ä¸ºcpuçš„æ ¸å¿ƒæ•°ã€‚

åœ¨ç¬¬28è¡Œä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ªDOWNLOAD\_DELAYå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å®é™…ä¸Šæ˜¯å‘Šè¯‰çˆ¬è™«ï¼Œæˆ‘ä»¬çš„ä¸‹è½½å™¨åœ¨ä¸‹è½½é¡µé¢æ—¶ï¼Œä¸¤ä¸ªé¡µé¢ä¹‹é—´çš„ç­‰å¾…é—´éš”ã€‚è¿™ä¸ªå‚æ•°ä¸»è¦æ˜¯ä¸ºäº†é™åˆ¶çˆ¬è™«çš„é€Ÿåº¦ï¼Œå‡è½»æœåŠ¡å™¨çš„å‹åŠ›ã€‚

![](https://static001.geekbang.org/resource/image/1d/5b/1de319535af069a2d46e7fa5e412a65b.jpg?wh=2820x1880)

ä¸Šé¢è¿™äº›å°±æ˜¯ç›¸å¯¹æ¯”è¾ƒé‡è¦çš„ä¸€äº›å‚æ•°ã€‚ä½†åœ¨è¿™ä¸ªè®¾ç½®æ–‡ä»¶ä¸­ï¼Œå¹¶æ²¡æœ‰é»˜è®¤æŠŠçˆ¬è™«å’Œpipelineç»‘å®šåœ¨ä¸€èµ·çš„å‚æ•°ã€‚æ²¡å…³ç³»ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªå·±æ¥åŠ ã€‚æˆ‘ä»¬å¯ä»¥åœ¨NEWSPIDER\_MODULEå‚æ•°ä¸‹é¢å¢åŠ ä¸€ä¸ªITEM\_PIPELINESå‚æ•°ï¼Œå¹¶æŠŠæˆ‘ä»¬çš„pipelineçš„è·¯å¾„èµ‹å€¼ä¸Šå»ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘ä»¬çš„å‰åæ–‡å°±å˜æˆäº†è¿™æ ·ã€‚

```
BOT_NAME = 'sina'
 
SPIDER_MODULES = ['sina.spiders']
NEWSPIDER_MODULE = 'sina.spiders'
 
ITEM_PIPELINES = {
    'sina.pipelines.SinaPipeline': 300
}
```

ç„¶åæˆ‘ä»¬å†è¿è¡Œmain.pyè¿™ä¸ªæ–‡ä»¶ï¼Œç„¶åç­‰å¾…çˆ¬è™«çš„ç»“æŸã€‚

å½“çˆ¬è™«ç»“æŸçˆ¬å–å·¥ä½œä¹‹åï¼Œæˆ‘ä»¬æ¥éªŒè¯ä¸€ä¸‹æ•°æ®æ˜¯å¦å·²ç»å­˜å…¥åˆ°MongoDBæ•°æ®åº“ä¸­ã€‚è¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨cmdå‘½ä»¤ä¸­è¾“å…¥mongoï¼Œè¿›å…¥åˆ°MongoDBçš„æ§åˆ¶å°ï¼Œç„¶åè¾“å…¥ä¸‹é¢çš„ä»£ç ã€‚

```
use scrapy_data
```

å†è¾“å…¥ä¸‹é¢è¿™è¡Œä»£ç ã€‚

```
db.content_ori.find()
```

ç„¶åä½ ä¼šå‘ç°ï¼Œæˆ‘ä»¬å·²ç»å°†çˆ¬å–åˆ°çš„æ•°æ®å­˜å…¥åˆ°äº†æˆ‘ä»¬çš„æ•°æ®åº“ä¸­ã€‚

![](https://static001.geekbang.org/resource/image/22/6d/220845a37ee709aa8d53a50c6346ef6d.png?wh=3814x1241)

ä¸è¿‡è¿™æ ·è¿›è¡Œæ•°æ®çš„æŸ¥è¯¢èµ·æ¥å¤ªè´¹åŠ²äº†ï¼Œæˆ‘æ¨èä½ å»ä¸‹è½½ä¸€ä¸ªåå«robo3Tçš„è½¯ä»¶ï¼ˆä¸€æ¬¾å…è´¹çš„ä¸“é—¨é’ˆå¯¹MongoDBçš„æ•°æ®åº“å¯è§†åŒ–è½¯ä»¶ï¼‰ã€‚å®‰è£…ä¹‹ååˆ›å»ºä¸€ä¸ªé»˜è®¤çš„æ•°æ®åº“è¿æ¥ï¼Œå°±å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ•°æ®äº†ã€‚

![](https://static001.geekbang.org/resource/image/1d/12/1d31352ac753b2b9016e8b5abda9f412.png?wh=3840x2098)

åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å°±å·²ç»å®Œæˆäº†æ•°æ®çš„çˆ¬å–å’Œå­˜å‚¨å·¥ä½œã€‚

## æ€»ç»“

åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œæˆ‘ä»¬æœ¬èŠ‚è¯¾çš„ä»£ç å·²ç»å­¦å®Œäº†ï¼Œå…³äºçˆ¬è™«çš„éƒ¨åˆ†ä¹Ÿå·²ç»å®Œæˆï¼Œæˆ‘æ¥å¯¹è¿™èŠ‚è¯¾çš„å†…å®¹åšä¸€ä¸ªç®€å•çš„æ€»ç»“ã€‚

1. æˆ‘ä»¬åº”è¯¥çŸ¥é“ä»€ä¹ˆæ˜¯pymongoåº“ï¼Œä»¥åŠå¦‚ä½•å®‰è£…å’Œä½¿ç”¨å®ƒã€‚
2. ä½ éœ€è¦äº†è§£å¦‚ä½•åœ¨scrapyä¸­å¯¹æ•°æ®è¿›è¡Œå¤„ç†å’Œä¿å­˜ï¼ˆä¸€èˆ¬æ˜¯åœ¨pipelinesæ–‡ä»¶ä¸­æ“ä½œï¼‰ã€‚
3. ä¸€å®šè¦è®°ä½ï¼Œæœ€åè¿˜è¦åœ¨settings.pyæ–‡ä»¶ä¸‹åŠ å…¥æˆ‘ä»¬çš„pipelinesç›¸å…³çš„å†…å®¹ã€‚
4. æˆ‘ä»¬ä¹Ÿè¦ç†Ÿæ‚‰settings.pyæ–‡ä»¶ä¸‹çš„å„ä¸ªé…ç½®ã€‚

æ˜å¤©å°±æ˜¯äº”ä¸€å‡æœŸäº†ï¼Œåœ¨è¿™é‡Œæå‰ç¥ä½ äº”ä¸€å‡æœŸç©å¾—å¼€å¿ƒï¼æˆ‘ä»¬çš„è¯¾ç¨‹å°†ä¼šåœ¨äº”ä¸€æœŸé—´åœæ›´ï¼ˆ5.8æ—¥æ›´æ–°ï¼‰ï¼Œä¸€æ–¹é¢æ˜¯å¸Œæœ›ä½ åœ¨è¿™æ®µæ—¶é—´é‡Œæ•´ä½“è¿‡ä¸€éæ•°æ®ç¯‡çš„å†…å®¹ï¼Œä¸ºæ¥ä¸‹æ¥çš„å¬å›ç¯‡åšå‡†å¤‡ï¼›å¦ä¸€æ–¹é¢è¿™æ®µæ—¶é—´çœ‹åˆ°äº†å¾ˆå¤šå…³äºè¯¾ç¨‹çš„è®¤çœŸåé¦ˆï¼Œæˆ‘ä¼šåœ¨è¿™æœŸé—´æ•´ä½“ç›˜ä¸€ç›˜æ¥ä¸‹æ¥çš„å†…å®¹ï¼Œåšä¸€äº›æ­£æ–‡æˆ–è€…åç»­åŠ é¤çš„è¡¥å……ã€‚å¦‚æœä½ å¯¹äºè¿™ä¸ªè¯¾ç¨‹æœ‰ä»»ä½•è¿›ä¸€æ­¥çš„å»ºè®®ï¼Œæ¬¢è¿ç•™è¨€ï¼Œä½ çš„å»ºè®®æ˜¯æˆ‘ä¸æ–­æ”¹è¿›çš„åŠ¨åŠ›ã€‚

## è¯¾åé¢˜

æœ€åç»™ä½ å¸ƒç½®ä¸¤ä¸ªå°ä½œä¸šã€‚

1. è‡ªå·±ä½¿ç”¨Scrapyçˆ¬å–æ–°æµªç½‘çš„æ•°æ®ï¼Œå¹¶å­˜å…¥åˆ°æ•°æ®åº“ä¸­ã€‚
2. æ”¹é€ spiderä¸­çš„è¾“å…¥éƒ¨åˆ†ï¼Œå¢åŠ ç¿»é¡µå’Œå¢é‡çˆ¬å–çš„å†…å®¹ï¼ˆé‡éš¾ç‚¹ï¼‰ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ9ï¼‰</strong></div><ul>
<li><span>peter</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç¬¬07è¯¾ï¼Œåˆ›å»ºçˆ¬è™«ç¨‹åºå‡ºé”™çš„é—®é¢˜è§£å†³äº†ï¼Œè§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š
E:\Javaweb\study\recommendsysï¼Œè¿™ä¸ªç›®å½•ä¸‹é¢æœ‰envå’Œprojectä¸¤ä¸ªå­ç›®å½•ï¼ŒAnacondaå®‰è£…åœ¨envä¸‹é¢ï¼Œè™šæ‹Ÿç¯å¢ƒåœ¨Anacondaçš„å®‰è£…ç›®å½•ä¸‹é¢ã€‚çˆ¬è™«é¡¹ç›®åœ¨projectç›®å½•ä¸‹é¢ã€‚ç„¶åæ‰§è¡Œâ€œscrapy genspider sina_spider sina.com.cnâ€æ—¶å€™æŠ¥é”™äº†å‡ æ¬¡ã€‚
åæ¥åœ¨Eç›˜ä¸‹åˆ›å»ºgeekbangï¼Œå’Œè€å¸ˆçš„ç›®å½•ä¸€æ ·ï¼Œåˆåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„è™šæ‹Ÿç¯å¢ƒï¼Œç…§ç€è€å¸ˆçš„æµç¨‹åšä¸‹æ¥ï¼Œå°±æˆåŠŸåˆ›å»ºäº†çˆ¬è™«ç¨‹åºã€‚
é—®é¢˜è§£å†³äº†ï¼Œä¸çŸ¥é“å…·ä½“åŸå› ï¼Œæ„Ÿè§‰æ˜¯ç›®å½•é—®é¢˜ã€‚å¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œè€å¸ˆå¦‚æœæœ‰çœ‹æ³•å°±å‘Šè¯‰æˆ‘ä¸€ä¸‹ï¼Œæ¯”å¦‚ç›®å½•ä½ç½®æœ‰ä¸€äº›å‘ã€‚</p>2023-05-03</li><br/><li><span>peter</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç¬¬07è¯¾ï¼Œåˆ›å»ºçˆ¬è™«ç¨‹åºå‡ºé”™çš„é—®é¢˜è§£å†³äº†ï¼Œè§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š
E:\Javaweb\study\recommendsysï¼Œè¿™ä¸ªç›®å½•ä¸‹é¢æœ‰envå’Œprojectä¸¤ä¸ªå­ç›®å½•ï¼ŒAnacondaå®‰è£…åœ¨envä¸‹é¢ï¼Œè™šæ‹Ÿç¯å¢ƒåœ¨Anacondaçš„å®‰è£…ç›®å½•ä¸‹é¢ã€‚çˆ¬è™«é¡¹ç›®åœ¨projectç›®å½•ä¸‹é¢ã€‚ç„¶åæ‰§è¡Œâ€œscrapy genspider sina_spider sina.com.cnâ€æ—¶å€™æŠ¥é”™äº†å‡ æ¬¡ã€‚
åæ¥åœ¨Eç›˜ä¸‹åˆ›å»ºgeekbangï¼Œå’Œè€å¸ˆçš„ç›®å½•ä¸€æ ·ï¼Œåˆåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„è™šæ‹Ÿç¯å¢ƒï¼Œç…§ç€è€å¸ˆçš„æµç¨‹åšä¸‹æ¥ï¼Œå°±æˆåŠŸåˆ›å»ºäº†çˆ¬è™«ç¨‹åºã€‚
é—®é¢˜è§£å†³äº†ï¼Œä¸çŸ¥é“å…·ä½“åŸå› ï¼Œæ„Ÿè§‰æ˜¯ç›®å½•é—®é¢˜ã€‚å¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œè€å¸ˆå¦‚æœæœ‰çœ‹æ³•å°±å‘Šè¯‰æˆ‘ä¸€ä¸‹ï¼Œæ¯”å¦‚ç›®å½•ä½ç½®æœ‰ä¸€äº›å‘ã€‚
è¿è¡Œmain.pyåï¼Œé‡åˆ°ä¸¤ä¸ªé—®é¢˜ï¼š
Q1ï¼šROBOTSTXT_OBEY = Trueæ—¶çˆ¬å–å¤±è´¥
åˆ›å»ºmain.pyåï¼Œsettings.pyä¸­ï¼ŒROBOTSTXT_OBEY = Trueã€‚ è¿è¡ŒåæŠ¥å‘Šï¼š
.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty fileã€‚
è¯¥é”™è¯¯å¯¼è‡´å¦å¤–ä¸€ä¸ªé”™è¯¯ï¼š
robotstxt.py&quot;, line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode(&quot;utf-8&quot;)
UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xc3 in position 93: invalid continuation byte
ç½‘ä¸Šå»ºè®®ROBOTSTXT_OBEY æ”¹æˆ Falseï¼Œå¥½åƒæ˜¯æˆåŠŸäº†ã€‚
Q2ï¼šæˆåŠŸä¿¡æ¯å’Œè€å¸ˆçš„ä¸åŒï¼Œä¸ç¡®å®šæ˜¯å¦æˆåŠŸã€‚
A ä¸“æ ä¸Šçš„æˆåŠŸä¿¡æ¯å¾ˆå°‘ï¼Œæˆ‘è¿™é‡Œçš„ä¿¡æ¯éå¸¸å¤šï¼Œæ˜¯logè®¾ç½®ä¸åŒå—ï¼Ÿï¼ˆæˆ‘ç”¨PyCharm4.5ï¼‰ã€‚
B ä¸“æ ä¸Šçš„æˆåŠŸä¿¡æ¯ï¼Œæœ‰ä¸¤ä¸ªé“¾æ¥ï¼š
Get https:&#47;&#47;news.sina.com.cn&#47;robots.txt
Get https:&#47;&#47;news.sina.com.cn&#47;china
ä½†æˆ‘çš„è¾“å‡ºä¿¡æ¯ä¸­å¹¶æ²¡æœ‰è¿™ä¸¤ä¸ªé“¾æ¥ï¼Œè¿˜æ²¡æœ‰æˆåŠŸå—ï¼Ÿ
éƒ¨åˆ†ä¿¡æ¯å¦‚ä¸‹ï¼š
2023-05-02 12:56:00 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:56:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages&#47;min), scraped 0 items (at 0 items&#47;min)
2023-05-02 12:56:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:56:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET https:&#47;&#47;www.sina.com.cn&#47;&gt; from &lt;GET http:&#47;&#47;sina.com.cn&#47;&gt;
2023-05-02 [engine] DEBUG: Crawled (200) &lt;GET https:&#47;&#47;www.sina.com.cn&#47;&gt; (referer: None)
2023-05-02 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</p>2023-05-02</li><br/><li><span>GhostGuest</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç¬¬ 23 è¡Œä¸­ CONCURRENT_REQUESTS å‚æ•°è§£é‡Šæœ‰è¯¯, è¿™ä¸ªå‚æ•°æ˜¯è®¾ç½®çº¿ç¨‹æ•°ï¼Œå¹¶ä¸æ˜¯å¤šçº¿ç¨‹çš„å¼€å…³ï¼Œæ–‡ä¸­æè¿°å¼€å¯å°±å¯ä»¥åˆ©ç”¨å¤šçº¿ç¨‹è¿›è¡Œçˆ¬å–æœ‰ç‚¹æ­§ä¹‰äº†ï¼Œé»˜è®¤å°±æ˜¯å¤šçº¿ç¨‹çˆ¬å–çš„ï¼Œè¿™ä¸ªå‚æ•°åªæ˜¯è®¾ç½®å¹¶å‘é‡</p>2023-04-30</li><br/><li><span>19984598515</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆä½ å¥½ï¼Œä»€ä¹ˆæ—¶å€™èƒ½æœ‰å®Œæ•´æºç å‘¢</p>2023-04-29</li><br/><li><span>peter</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è°ƒç”¨æµç¨‹æ˜¯åœ¨å“ªé‡Œå®šä¹‰çš„ï¼Ÿæ¯”å¦‚ï¼Œå¯¹äºpipelines.pyæ–‡ä»¶ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨è°ƒç”¨å®ƒï¼Œå‡å¦‚æˆ‘æ”¹å˜æ–‡ä»¶åå­—ï¼Œåº”è¯¥å°±æ— æ³•æ­£å¸¸è¿è¡Œäº†ï¼›å¦‚æœçŸ¥é“æµç¨‹è°ƒç”¨å…³ç³»åœ¨å“ªé‡Œå®šä¹‰ï¼Œå°±å¯ä»¥åœ¨é‚£é‡Œä¿®æ”¹æ–‡ä»¶åå­—ã€‚ ï¼ˆæ˜¯settings.pyå—ï¼Ÿï¼‰</p>2023-04-28</li><br/><li><span>GACÂ·DU</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æŠŠå·²ç»çˆ¬è¿‡çš„æ–°é—»æ ‡é¢˜ï¼Œç”¨Redis seté›†åˆå­˜å‚¨ï¼Œä½œä¸ºå¢é‡è¿‡æ»¤å™¨ï¼Œå¦‚æœæ ‡é¢˜ä¸åœ¨é›†åˆä¸­åˆ™æŠ“å–æ•°æ®å¹¶ä¿å­˜æ•°æ®ï¼ŒåŒæ—¶å°†æ–°çš„æ ‡é¢˜æ”¾åˆ°é›†åˆä¸­ã€‚

æµ‹è¯•æˆåŠŸï¼Œéƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š
redisï¼š
class RedisDB:
    def __init__(self):
        self.host = &quot;ipåœ°å€&quot;
        self.port = 6379
        self.passwd = &quot;å¯†ç &quot;
        self.db = 2
        self.pool = redis.ConnectionPool(host=self.host, port=self.port, password=self.passwd, db=self.db, decode_responses=True)
        self.client = redis.Redis(connection_pool=self.pool)

    def add(self, key, value):
        return self.client.sadd(key, value)

    def exists(self, key, value):
        return self.client.sismember(key, value)

spideréƒ¨åˆ†:
if self.redis.exists(&quot;sina&quot;, items[&quot;news_title&quot;]):
    continue
self.redis.add(&quot;sina&quot;, items[&quot;news_title&quot;])

é‚£é‡Œæœ‰é—®é¢˜ï¼Œè¯·è€å¸ˆæŒ‡æ­£ï¼Œè°¢è°¢ã€‚</p>2023-04-28</li><br/><li><span>Baird</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>å¢é‡çˆ¬å–ï¼š
pipelines.py
æ–°å¢ä¸€ä¸ªæ–¹æ³•è·å–MongoDBé›†åˆä¸­çš„æœ€æ–°æ—¶é—´
def get_last_crawl_time(self):
    latest_entry = self.collection.find_one(
        sort=[(&quot;times&quot;, -1)],  # æŒ‰ times å­—æ®µé™åºæ’åº
        projection={&quot;times&quot;: 1} 
    )
    if latest_entry:
        return latest_entry[&quot;times&quot;]
    return None
sina_spider.py
åˆå§‹åŒ–ä¸€ä¸ªsinaPipelineå®ä¾‹ï¼Œè°ƒç”¨get_last_crawl_timeè·å–ä¸Šæ¬¡çˆ¬å–æ–°é—»çš„æœ€åæ—¶é—´
def __init__(self):
    self.start_urls = [&#39;https:&#47;&#47;news.sina.com.cn&#47;china&#47;&#39;]
    self.option = Options()
    self.option.add_argument(&#39;--no-sandbox&#39;)
    self.option.add_argument(&#39;--blink-settings=imagesEnabled=false&#39;)
    self.pipeline = SinaPipeline()
    self.last_crawl_time = self.pipeline.get_last_crawl_time()
å¾ªç¯ä¸­åˆ¤æ–­æ—¶é—´æ˜¯å¦å°äºä¸Šæ¬¡çˆ¬å–çš„æ–°é—»æ—¥æœŸï¼Œæ˜¯çš„è¯è·³è¿‡è¿™æ¬¡å¾ªç¯
if &#39;åˆ†é’Ÿå‰&#39; in eachtime:
    minute = int(eachtime.split(&#39;åˆ†é’Ÿå‰&#39;)[0])
    t = datetime.datetime.now() - datetime.timedelta(minutes=minute)
    t2 = datetime.datetime(year=t.year, month=t.month, day=t.day, hour=t.hour, minute=t.minute)
else:
    if &#39;å¹´&#39; not in eachtime:
        eachtime = str(today.year) + &#39;å¹´&#39; + eachtime
    t1 = re.split(&#39;[å¹´æœˆæ—¥:]&#39;, eachtime)
    t2 = datetime.datetime(year=int(t1[0]), month=int(t1[1]), day=int(t1[2]), hour=int(t1[3]),
                           minute=int(t1[4]))

if  t2 &lt;= self.last_crawl_time:
    continue</p>2024-10-17</li><br/><li><span>æ‚Ÿå°˜</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>æœ‰å¾®ä¿¡ç¾¤äº†å—ï¼Ÿæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œpymongoè¿ä¸ä¸Šmongoæ•°æ®åº“</p>2023-12-12</li><br/><li><span>peter</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è¿è¡Œmain.pyåï¼Œé‡åˆ°ä¸¤ä¸ªé—®é¢˜ï¼š
Q1ï¼šROBOTSTXT_OBEY = Trueæ—¶çˆ¬å–å¤±è´¥
åˆ›å»ºmain.pyåï¼Œsettings.pyä¸­ï¼ŒROBOTSTXT_OBEY = Trueã€‚ è¿è¡ŒåæŠ¥å‘Šï¼š
.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty fileã€‚
è¯¥é”™è¯¯å¯¼è‡´å¦å¤–ä¸€ä¸ªé”™è¯¯ï¼š
robotstxt.py&quot;, line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode(&quot;utf-8&quot;)
UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xc3 in position 93: invalid continuation byte
ç½‘ä¸Šå»ºè®®ROBOTSTXT_OBEY æ”¹æˆ Falseï¼Œå¥½åƒæ˜¯æˆåŠŸäº†ã€‚
Q2ï¼šæˆåŠŸä¿¡æ¯å’Œè€å¸ˆçš„ä¸åŒï¼Œä¸ç¡®å®šæ˜¯å¦æˆåŠŸã€‚
A ä¸“æ ä¸Šçš„æˆåŠŸä¿¡æ¯å¾ˆå°‘ï¼Œæˆ‘è¿™é‡Œçš„ä¿¡æ¯éå¸¸å¤šï¼Œæ˜¯logè®¾ç½®ä¸åŒå—ï¼Ÿï¼ˆæˆ‘ç”¨PyCharm4.5ï¼‰ã€‚
B ä¸“æ ä¸Šçš„æˆåŠŸä¿¡æ¯ï¼Œæœ‰ä¸¤ä¸ªé“¾æ¥ï¼š
Get https:&#47;&#47;news.sina.com.cn&#47;robots.txt
Get https:&#47;&#47;news.sina.com.cn&#47;china
ä½†æˆ‘çš„è¾“å‡ºä¿¡æ¯ä¸­å¹¶æ²¡æœ‰è¿™ä¸¤ä¸ªé“¾æ¥ï¼Œè¿˜æ²¡æœ‰æˆåŠŸå—ï¼Ÿ
éƒ¨åˆ†ä¿¡æ¯å¦‚ä¸‹ï¼š
2023-05-02 12:56:00 [scrapy.core.engine] INFO: Spider opened</p>2023-05-03</li><br/>
</ul>