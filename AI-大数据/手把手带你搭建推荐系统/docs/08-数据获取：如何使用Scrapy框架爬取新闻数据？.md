ä½ å¥½ï¼Œæˆ‘æ˜¯é»„é¸¿æ³¢ã€‚

ä¸Šä¸€èŠ‚è¯¾ï¼Œæˆ‘ä»¬å¯¹Scrapyæ¡†æ¶æœ‰äº†ä¸€ä¸ªæ•´ä½“çš„äº†è§£ï¼Œä¹Ÿå®é™…åœ°å®‰è£…äº†Scrapyåº“å¹¶æ­å»ºäº†ä¸€ä¸ªåŸºç¡€çš„Scrapyå·¥ç¨‹ã€‚è¿™èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°±ç»§ç»­åœ¨è¿™ä¸ªå·¥ç¨‹çš„åŸºç¡€ä¸Šçˆ¬å–æ–°æµªæ–°é—»ä¸­çš„æ•°æ®ï¼Œå¹¶å¯¹çˆ¬å–åˆ°çš„æ•°æ®è¿›è¡Œè§£æã€‚

## ä½¿ç”¨Scrapyæ¡†æ¶æŠ“å–æ•°æ®

æˆ‘ä»¬é¦–å…ˆæ‰“å¼€sina\\sina\\spiderä¸‹é¢çš„sina\_spider.pyæ–‡ä»¶ï¼Œåœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼ŒScrapyæ¡†æ¶ç»™æˆ‘ä»¬å†™äº†ä¸€ä¸ªåŸºç¡€çš„æ¡†æ¶ä»£ç ã€‚

```
import scrapy
 
class SinaSpiderSpider(scrapy.Spider):
    name = 'sina_spider'
    allowed_domains = ['sina.com.cn']
    start_urls = ['http://sina.com.cn/']
 
    def parse(self, response):
        pass
```

è¿™æ®µä»£ç ä¸»è¦æ˜¯å¯¹æ•´ä¸ªçˆ¬è™«ç¨‹åºåšäº†ä¸€ä¸ªç®€å•çš„å®šä¹‰ï¼Œå®šä¹‰äº†çˆ¬è™«çš„åå­—ä¸ºâ€œsina\_spiderâ€ï¼Œçˆ¬å–çš„åŸŸåä¸ºâ€œsina.com.cnâ€ï¼Œçˆ¬å–çš„URLæ˜¯â€œ[http://sina.com.cn/](http://sina.com.cn/)â€ã€‚æœ€åå®ƒè¿˜è´´å¿ƒåœ°å¸®æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªè§£æå‡½æ•°ï¼Œè¿™ä¸ªè§£æå‡½æ•°çš„å…¥å‚å°±æ˜¯æœåŠ¡å™¨è¿”å›çš„responseå€¼ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬è¦å¼€å§‹åˆ†ææˆ‘ä»¬è¦çˆ¬å–çš„å†…å®¹ï¼Œå¹¶å¯¹è¿™ä¸ªå‡½æ•°è¿›è¡Œæ”¹å†™ã€‚

### é¡µé¢åˆ†æ

æˆ‘ä»¬å…ˆä»¥ç½‘æ˜“çš„å›½å†…æ–°é—»ä¸ºä¾‹æ¥åˆ†æä¸€ä¸‹ã€‚æˆ‘ä»¬å…ˆçœ‹ä¸‹é¢è¿™ä¸ªç•Œé¢ã€‚

![](https://static001.geekbang.org/resource/image/10/c5/105f1037b22ba0d30f68346c39c065c5.png?wh=2412x1391)

æˆ‘ä»¬è¦åˆ†æçš„æ˜¯ç•Œé¢é‡Œæœ€æ–°æ–°é—»è¿™ä¸ªéƒ¨åˆ†ã€‚å¯ä»¥çœ‹åˆ°è¿™ä¸ªæ–°é—»åˆ—è¡¨ä¸­ä¸€å…±åŒ…å«äº†ä¸‹é¢è¿™å‡ éƒ¨åˆ†ï¼šæ ‡é¢˜ã€æ‘˜è¦ã€æ—¶é—´ã€å…³é”®è¯ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œæ—¶é—´åœ¨1å°æ—¶ä¹‹å†…çš„ä¼šæ˜¾ç¤ºä¸ºâ€œXXåˆ†é’Ÿå‰â€ï¼Œåœ¨1å°æ—¶ä»¥ä¸Šçš„ä¼šæ˜¾ç¤ºä»Šå¤©å…·ä½“çš„æŸä¸ªæ—¶é—´ç‚¹ã€‚

æ¥ç€æˆ‘ä»¬æŠŠé¡µé¢æ‹‰åˆ°æœ€ä¸‹é¢ã€‚

![](https://static001.geekbang.org/resource/image/43/9e/43ed7a9c7a42f8cf44244f795372ba9e.png?wh=1659x1830)

å¯ä»¥çœ‹åˆ°ï¼Œä»Šå¤©ä¹‹å‰çš„æ–°é—»ä¼šæ˜¾ç¤ºå‡ºå…·ä½“çš„æ—¥æœŸï¼Œå¹¶ä¸”æœ€ä¸‹é¢æœ‰ä¸€ä¸ªå¯¼èˆªæ¡ç”¨æ¥ç¿»é¡µã€‚

æˆ‘ä»¬éšä¾¿ç‚¹å‡»ä¸€æ¡æ–°é—»è¿›å…¥è¯¦æƒ…é¡µçœ‹ä¸€ä¸‹ã€‚å¯ä»¥çœ‹åˆ°é‡Œé¢åŒ…å«äº†å›¾ç‰‡å’Œæ–‡å­—ï¼Œå…¶ä¸­æ–‡å­—éƒ¨åˆ†æœ€ä¸Šé¢æœ‰æ ‡é¢˜ï¼Œä¸‹é¢æœ‰æ—¥æœŸå’Œæ—¶é—´ï¼Œå†ä¸‹é¢æ˜¯æ­£æ–‡ã€‚å½“ç„¶ï¼Œè¿˜å……æ–¥ç€å¹¿å‘Šå’Œæˆ‘ä»¬ä¸éœ€è¦çš„ä¿¡æ¯ï¼Œè¿™äº›æˆ‘ä»¬æš‚æ—¶ä¸ç”¨ç®¡ã€‚

![](https://static001.geekbang.org/resource/image/b9/2c/b9eb79c35ae40f4e2d3c4e607d7fa12c.png?wh=1475x1298)

æ¥ä¸‹æ¥æˆ‘ä»¬åˆ†æä¸€ä¸‹å‰é¢çš„åˆ—è¡¨ä»¥åŠè¿™ä¸ªè¯¦æƒ…é¡µçš„å†…å®¹ï¼ŒæŠ“å–æˆ‘ä»¬æƒ³è¦çš„ä¿¡æ¯ã€‚

æˆ‘ä»¬çŸ¥é“ï¼Œæ‰€æœ‰çš„é¡µé¢ä»æ ¹æœ¬ä¸Šæ¥è¯´éƒ½æ˜¯ç”±HTMLé¡µé¢æ„æˆçš„ï¼Œçˆ¬è™«æƒ³è¦çˆ¬å–çš„å†…å®¹å°±è—åœ¨è¿™äº›HTMLé¡µé¢ä¸­ã€‚åœ¨Chromeæµè§ˆå™¨ä¸­ï¼Œæˆ‘ä»¬æŒ‰ä¸‹é”®ç›˜ä¸Šçš„F12é”®å°±èƒ½å¤Ÿæ‰“å¼€å¼€å‘è€…å·¥å…·æ¨¡å¼ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ¨¡å¼æŸ¥çœ‹ç½‘é¡µçš„HTMLæºæ–‡ä»¶ã€è¯·æ±‚çš„ä¿¡æ¯æ–‡ä»¶ä»¥åŠç½‘ç»œè¿”å›ç­‰ã€‚æˆ‘ä»¬é€‰æ‹©ä¸Šé¢çš„Elementé€‰é¡¹å¡ï¼Œå°±èƒ½çœ‹åˆ°ç½‘é¡µçš„HTMLæºæ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](https://static001.geekbang.org/resource/image/0c/cf/0cca5213c59b5eb24c2813850fef0dcf.png?wh=2734x1465)

å› ä¸ºæˆ‘ä»¬è¦æ‰¾çš„æ˜¯æˆ‘ä»¬éœ€è¦çš„åˆ—è¡¨ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç‚¹å‡»å¼€å‘è€…å·¥å…·å·¦ä¸Šè§’çš„å°ç®­å¤´ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](https://static001.geekbang.org/resource/image/0c/8f/0c257c8072f7b1fb8649eb7e1f53cf8f.png?wh=2036x290)

ç„¶åç”¨é¼ æ ‡ç‚¹å‡»æˆ‘ä»¬æƒ³è¦çš„åˆ—è¡¨ï¼Œå³ä¾§çš„HTMLä»£ç å°±ä¼šè·Ÿç€è·³è½¬åˆ°ç›¸åº”çš„éƒ¨åˆ†ã€‚

![](https://static001.geekbang.org/resource/image/dd/8b/dd94c05aa27ebc26148f56cb9c90258b.png?wh=2330x1065)

é‚£ä¹ˆè¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°è¿™é‡Œé¢çš„å…¶ä¸­ä¸€æ¡ï¼Œç„¶åæŸ¥çœ‹å³é¢çš„HTMLæºæ–‡ä»¶ã€‚

![](https://static001.geekbang.org/resource/image/4f/85/4fff3041b2f175a092604a3d29b70285.png?wh=2335x994)

æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå®é™…ä¸Šï¼Œåœ¨è¿™ä¸ªåˆ—è¡¨ä¸­ï¼Œæ¯ä¸€æ¡å†…å®¹éƒ½ä¼šè¢«åŒ…å«åœ¨classä¸ºâ€œfeed-card-itemâ€çš„æ ‡ç­¾ä¸­ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªæ ‡ç­¾å±•å¼€æ¥è¯¦ç»†åœ°åˆ†æä¸€ä¸‹ã€‚

![](https://static001.geekbang.org/resource/image/5a/d8/5a27522b9d887103a5b270d4930ca6d8.png?wh=1893x719)

å¯ä»¥çœ‹åˆ°ï¼Œæ ‡é¢˜è¢«åŒ…å«åœ¨h2æ ‡ç­¾é‡Œçš„aæ ‡ç­¾ä¸­ï¼Œæ—¶é—´è¢«åŒ…å«åœ¨h2æ ‡ç­¾é‡Œclassä¸ºfeed-card-a feed-card-clearfixä¸‹é¢çš„feed-card-timeä¸­ï¼Œç„¶åè¿™æ¡å†…å®¹çš„é“¾æ¥å°±æ˜¯h2æ ‡ç­¾é‡Œçš„aæ ‡ç­¾çš„é“¾æ¥ã€‚

å¥½äº†ï¼ŒçŸ¥é“äº†æˆ‘ä»¬è¦çš„æ ‡é¢˜ã€æ—¶é—´ä»¥åŠå¯¹åº”çš„é“¾æ¥ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡çˆ¬è™«æŠŠå®ƒä»¬æ‹¿ä¸‹æ¥äº†ã€‚

### çˆ¬å–åˆ—è¡¨

ä½¿ç”¨Scrapyæ‹¿æ ‡ç­¾ï¼Œæ¯”è¾ƒæ–¹ä¾¿çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨Seleniumåº“ã€‚

Seleniumæ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•Webåº”ç”¨ç¨‹åºçš„å·¥å…·ï¼ŒSeleniumæµ‹è¯•å¯ä»¥ç›´æ¥è¿è¡Œåœ¨æµè§ˆå™¨ä¸­ï¼Œå°±åƒçœŸæ­£çš„ç”¨æˆ·åœ¨æ“ä½œä¸€æ ·ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Seleniumåº“æ¥æ¨¡æ‹Ÿç‚¹å‡»ã€ä¸Šæ»‘å’Œä¸‹æ»‘ç­‰æ“ä½œã€‚

è¦æƒ³ä½¿ç”¨è¿™ä¸ªåº“ï¼Œé¦–å…ˆè¦åœ¨å¼€å‘ç¯å¢ƒä¸­å®‰è£…å®ƒã€‚å®‰è£…æ–¹æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œç›´æ¥åœ¨æˆ‘ä»¬çš„Anacondaç¯å¢ƒä¸­ä½¿ç”¨pipå®‰è£…å°±å¥½ã€‚å…·ä½“åšæ³•æ˜¯åˆ‡æ¢åˆ°scrapy\_recommendationç¯å¢ƒä¸­ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚

```
pip install selenium
```

å®‰è£…å®Œæˆåå¦‚å›¾æ‰€ç¤ºã€‚

![](https://static001.geekbang.org/resource/image/45/78/45128e532b0a6bcd316f8ca61500c078.png?wh=1905x983)

æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨Seleniumæ¥è¿›è¡Œæµè§ˆå™¨é¡µé¢è®¿é—®å·¥ä½œäº†ï¼Œæˆ‘ä»¬æŠŠä¸Šé¢çš„ä»£ç æ”¹å†™å¦‚ä¸‹ã€‚

```
import scrapy
from scrapy.http import Request
from selenium import webdriver
 
 
class SinaSpiderSpider(scrapy.Spider):
    name = 'sina_spider'
 
 
    def __init__(self):
        self.start_urls = ['https://news.sina.com.cn/china/']
        self.option = webdriver.ChromeOptions()
        self.option.add_argument('no=sandbox')
        self.option.add_argument('--blink-setting=imagesEnable=false')
 
    def start_requests(self):
        for url in self.start_urls:
            yield Request(url=url, callback=self.parse)
 
    def parse(self, response):
        driver = webdriver.Chrome(chrome_options=self.option)
        driver.set_page_load_timeout(30)
        driver.get(response.url)
 
        title = driver.find_elements_by_xpath("//h2[@class='undefined']/a[@target='_blank']")
        time = driver.find_elements_by_xpath("//h2[@class='undefined']/../div[@class='feed-card-a "
                                             "feed-card-clearfix']/div[@class='feed-card-time']")
 
        for i in range(len(title)):
            print(title[i].text)
            print(time[i].text)
```

å¯ä»¥çœ‹åˆ°ï¼Œè¿™æ®µä»£ç ç›¸æ¯”ä¸Šé¢é‚£æ®µä»£ç æœ‰äº†éå¸¸å¤§çš„æ”¹å˜ã€‚é¦–å…ˆï¼Œåœ¨æœ€ä¸Šé¢ï¼Œæˆ‘ä»¬å¤šå¯¼å…¥äº†ä¸¤ä¸ªåŒ…ã€‚

```
from scrapy.http import Request
from selenium import webdriver
```

è¿™æ®µä»£ç ç¬¬ä¸€è¡Œçš„æ„æ€æ˜¯ä»Scrapyçš„httpæ¨¡å—å¯¼å…¥Requestè¿™ä¸ªåŒ…ï¼Œç¬¬äºŒè¡Œçš„æ„æ€æ˜¯ä»Seleniumåº“å¯¼å…¥Webdriverè¿™ä¸ªåŒ…ã€‚

RequeståŒ…é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯ç”¨æ¥åšHTTPè¯·æ±‚ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬åœ¨å‘ç½‘ç«™æœåŠ¡å™¨è¯·æ±‚æ•°æ®æ—¶ï¼Œå°±æ˜¯RequeståŒ…åœ¨èµ·ä½œç”¨ã€‚è€ŒSeleniumä¸‹é¢çš„webdriveråŒ…æ˜¯ä¸€ç»„å¼€æºçš„APIï¼Œç”¨äºè‡ªåŠ¨åŒ–æµ‹è¯•Webåº”ç”¨ç¨‹åºã€‚åœ¨æˆ‘ä»¬çš„ç¨‹åºä¸­ï¼Œä¸»è¦æ˜¯åˆ©ç”¨å®ƒæ¥æ‰“å¼€æµè§ˆå™¨ï¼Œä»¥åŠè®¾ç½®æ‰“å¼€æ—¶çš„ä¸€äº›ä¿¡æ¯ã€‚

æ¥ç€ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªclassè¿›è¡Œäº†é‡æ„ï¼ŒåŠ å…¥äº†\_\_init\_\_è¿™ä¸ªæ„é€ å‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥å…ˆç²—ç•¥åœ°ç†è§£ä¸ºï¼Œå½“æˆ‘ä»¬è¿è¡Œè¿™ä¸ªPythonæ–‡ä»¶æ—¶ï¼Œå°±ä¼šå…ˆå»æ‰§è¡Œ\_\_init\_\_å‡½æ•°é‡Œé¢çš„å†…å®¹ã€‚

æˆ‘ä»¬å°†ä¹‹å‰çš„start\_urlsåŠ å…¥åˆ°äº†\_\_init\_\_ä¸­ï¼Œå¹¶åœ¨å‰é¢åŠ ä¸Šselfã€‚æ¥ç€ï¼Œæˆ‘ä»¬å®šä¹‰äº†webdriverä¸­å…³äºChromeæµè§ˆå™¨çš„ä¸€äº›å‚æ•°ã€‚

```
self.option = webdriver.ChromeOptions()
self.option.add_argument('no=sandbox')
self.option.add_argument('--blink-setting=imagesEnable=false')
```

æˆ‘åœ¨è¿™é‡ŒåŠ äº†ä¸¤ä¸ªå‚æ•°ï¼Œä¸€ä¸ªæ˜¯â€œno=sandboxâ€ï¼Œå®ƒè¡¨ç¤ºå–æ¶ˆæ²™ç›’æ¨¡å¼ï¼Œä¹Ÿå°±æ˜¯è¯´è®©å®ƒåœ¨rootæƒé™ä¸‹æ‰§è¡Œã€‚å¦ä¸€ä¸ªå‚æ•°æ˜¯â€œâ€“blink-setting=imagesEnable=falseâ€ï¼Œå®ƒè¡¨ç¤ºä¸åŠ è½½å›¾ç‰‡ï¼Œå› ä¸ºæˆ‘ä»¬åªæƒ³è¦æ–‡å­—éƒ¨åˆ†ï¼ŒåŠ ä¸Šè¿™ä¸€å¥å¯ä»¥æå‡çˆ¬å–é€Ÿåº¦å’Œæ•ˆç‡ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¢åŠ äº†ä¸€ä¸ªstart\_requests()å‡½æ•°ï¼Œè¿™å®é™…ä¸Šä¹Ÿæ˜¯Scrapyè‡ªå¸¦çš„å‡½æ•°ï¼Œå®ƒçš„ä¸»è¦ä½œç”¨æ˜¯å®šä¹‰Scrapyæ¡†æ¶çš„èµ·å§‹è¯·æ±‚ï¼Œå¦‚æœåœ¨è¿™ä¸ªèµ·å§‹è¯·æ±‚ä¸­æœ‰é‡å¤çš„URLï¼Œå®ƒä¼šè‡ªåŠ¨è¿›è¡Œå»é‡æ“ä½œã€‚

```
def start_requests(self):
        for url in self.start_urls:
            yield Request(url=url, callback=self.parse)
```

ç„¶ååœ¨è§£æå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªdriverï¼Œå®ƒè°ƒç”¨äº†webdriverçš„chromeå‡½æ•°ï¼Œä»£è¡¨è¿™æ˜¯ä½¿ç”¨Chromeæµè§ˆå™¨æ¥çˆ¬å–çš„ã€‚æˆ‘ä»¬è¿˜æŠŠåŠ è½½é¡µé¢çš„è¶…æ—¶æ—¶é—´è®¾ç½®ä¸ºäº†30ç§’ï¼Œä¹Ÿå°±æ˜¯è¯´å¦‚æœ30ç§’è¿˜åŠ è½½ä¸å‡ºæ¥ï¼Œå°±å»è¯·æ±‚ä¸‹ä¸€ä¸ªé¡µé¢ï¼Œè€Œè¿™ä¸‹ä¸€ä¸ªé¡µé¢å°±æ˜¯ä»start\_requests()å‡½æ•°ä¸­è·å¾—çš„ã€‚ç„¶åæˆ‘ä»¬è°ƒç”¨driver.getæ¥è·å–responseçš„URLï¼Œå°±å¯ä»¥æ‹¿åˆ°responseä¿¡æ¯äº†ã€‚

```
def parse(self, response):
    driver = webdriver.Chrome(chrome_options=self.option)
    driver.set_page_load_timeout(30)
    driver.get(response.url)


    title = driver.find_elements_by_xpath("//h2[@class='undefined']/a[@target='_blank']")
    time = driver.find_elements_by_xpath("//h2[@class='undefined']/../div[@class='feed-card-a "
                                         "feed-card-clearfix']/div[@class='feed-card-time']")


    for i in range(len(title)):
        print(title[i].text)
        print(time[i].text)
```

åœ¨åé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦åˆåšäº†ä¸¤ä»¶äº‹ã€‚ä¸€ä¸ªæ˜¯è·å–titleä¿¡æ¯ï¼Œä¸€ä¸ªæ˜¯è·å–timeä¿¡æ¯ã€‚

æˆ‘ä»¬ä½¿ç”¨driver.find\_elements\_by\_xpath()å‡½æ•°è·å–HTMLæ ‡ç­¾ä¸­çš„å†…å®¹ï¼Œæ ¹æ®æˆ‘ä»¬åœ¨æœ€å‰é¢çš„åˆ†æï¼Œtitleè¢«å­˜åœ¨â€œ//h2\[@class=â€˜undefinedâ€™]/a\[@target=â€˜\_blankâ€™]â€ä¸­ï¼Œè€Œtimeè¢«å­˜åœ¨â€œ//h2\[@class=â€˜undefinedâ€™]/â€¦/div\[@class=â€˜feed-card-a feed-card-clearfixâ€™]/div\[@class=â€˜feed-card-timeâ€™]â€ä¸­ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡driver.find\_elements\_by\_xpath()è·å–åˆ°é‡Œé¢çš„å†…å®¹ã€‚

**è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è·å–åˆ°çš„å†…å®¹ä¸€èˆ¬æ˜¯ä»¥ä¸€ä¸ªlistçš„å½¢å¼å­˜æ”¾ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜éœ€è¦ä½¿ç”¨forå¾ªç¯æ‹¿åˆ°é‡Œé¢çš„ä¿¡æ¯ã€‚**

æ­£å¸¸æ¥è®²ï¼Œå®Œæˆä¸Šé¢è¿™æ®µä»£ç ä¹‹åï¼Œè¿è¡Œmain.pyæ–‡ä»¶ï¼Œå°±ä¼šå¾—åˆ°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„ç»“æœã€‚

![](https://static001.geekbang.org/resource/image/3d/87/3d2c52211yy4acc2b2b45350ac59df87.png?wh=2895x1299)

å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬æƒ³è¦çš„æ—¶é—´å’Œæ ‡é¢˜éƒ½å·²ç»è¾“å‡ºå‡ºæ¥äº†ã€‚

ä¸è¿‡ï¼Œè™½ç„¶æˆ‘ä»¬ç°åœ¨å·²ç»å¾—åˆ°äº†æ—¶é—´ï¼Œä½†æ˜¯è¾“å‡ºçš„æ ¼å¼å´ä¸ç»Ÿä¸€ï¼šæœ‰çš„æ˜¾ç¤ºçš„æ˜¯ä»Šå¤©çš„æŸä¸ªæ—¶é—´ï¼Œæœ‰çš„æ˜¾ç¤ºçš„æ˜¯æ—¥æœŸåŠ æ—¶é—´ã€‚æ‰€ä»¥æˆ‘ä»¬è¦å¯¹æ—¶é—´åšè¿›ä¸€æ­¥å¤„ç†ï¼Œå¯ä»¥åœ¨åˆšåˆšçš„forå¾ªç¯ä»£ç ä¸‹é¢åŠ ä¸Šå¤„ç†ä»£ç ã€‚

```
	today = datetime.datetime.now()
            eachtime = time[i].text
            eachtime = eachtime.replace('ä»Šå¤©', str(today.month) + 'æœˆ' + str(today.day) + 'æ—¥')
 
            if 'åˆ†é’Ÿå‰' in eachtime:
                minute = int(eachtime.split('åˆ†é’Ÿå‰')[0])
                t = datetime.datetime.now() - datetime.timedelta(minutes=minute)
                t2 = datetime.datetime(year=t.year, month=t.month, day=t.day, hour=t.hour, minute=t.minute)
            else:
                if 'å¹´' not in eachtime:
                    eachtime = str(today.year) + 'å¹´' + eachtime
                t1 = re.split('[å¹´æœˆæ—¥:]', eachtime)
                t2 = datetime.datetime(year=int(t1[0]), month=int(t1[1]), day=int(t1[2]), hour=int(t1[3]),
                                       minute=int(t1[4]))
 
            print(t2)
```

æˆ‘ä»¬å†è¿è¡Œä¸€ä¸‹ç¨‹åºï¼Œå¾—åˆ°å¦‚ä¸‹ç»“æœã€‚

![](https://static001.geekbang.org/resource/image/ed/46/edb1674167760b7b852d23e8cc29e046.png?wh=2552x870)

å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨æ—¶é—´å·²ç»å˜æˆäº†æˆ‘ä»¬æƒ³è¦çš„æ ·å­ã€‚**åˆ°è¿™é‡Œæˆ‘ä»¬çš„åˆ—è¡¨çˆ¬å–å·¥ä½œå°±å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥å¼€å§‹çˆ¬å–è¯¦æƒ…é¡µçš„ä¿¡æ¯ã€‚**

### çˆ¬å–è¯¦æƒ…é¡µ

æˆ‘ä»¬çŸ¥é“ï¼Œå¦‚æœæ˜¯äººä¸ºæ“ä½œï¼Œéœ€è¦ç‚¹å‡»ç›¸åº”æ ‡é¢˜è¿›å…¥è¯¦æƒ…é¡µã€‚å¯¹äºçˆ¬è™«ç¨‹åºæ¥è¯´ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œæˆ‘ä»¬éœ€è¦ä»HTMLæ–‡ä»¶ä¸­æå–æ ‡é¢˜å¯¹åº”çš„é“¾æ¥ï¼Œç„¶åå†ä¼ ç»™çˆ¬è™«ç¨‹åºè¿›è¡Œæ•°æ®çš„çˆ¬å–ï¼Œæœ€åå¤„ç†å¯¹åº”çš„responseã€‚

æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹æ€ä¹ˆè·å–æˆ‘ä»¬æ‰€éœ€è¦çš„é“¾æ¥ã€‚

![](https://static001.geekbang.org/resource/image/43/2b/43b7d4b9ed070a150056a4c3d224d42b.png?wh=2007x1249)

é€šè¿‡åˆ†æHTMLæºæ–‡ä»¶æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€ä¸ªæ ‡é¢˜éƒ½åœ¨aæ ‡ç­¾ä¸­ï¼Œè€Œaæ ‡ç­¾é‡Œé¢çš„hrefå°±æ˜¯å®ƒæ‰€å¯¹åº”çš„é“¾æ¥ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬åªéœ€è¦å–å‡ºhrefï¼Œå°±å¯ä»¥å–å‡ºè¯¦æƒ…é¡µçš„é“¾æ¥äº†ã€‚

è¦å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨è§£ætitleåé¢åŠ ä¸Šä¸‹é¢è¿™æ¡ä»£ç ã€‚

```
href = title[i].get_attribute('href')
```

æ¥ä¸‹æ¥ï¼Œä¸ºäº†è·å–æ­£æ–‡ä¿¡æ¯ï¼Œåœ¨Scrapyä¸­ï¼Œæˆ‘ä»¬éœ€è¦æŠŠrequestè¯·æ±‚ç»™yieldå‡ºå»ï¼Œåœ¨é‡Œé¢æ”¾ä¸Šé“¾æ¥ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬æŠŠéœ€è¦çš„å†…å®¹yieldåˆ°ä¸‹ä¸€é¡µï¼Œä¼ é€’å®ƒæœ€å¥½çš„æ–¹å¼å°±æ˜¯ä½¿ç”¨ItemPipelineã€‚å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åˆ›å»ºScrapyå·¥ç¨‹ä¹‹åï¼Œå·¥ç¨‹ç›®å½•ä¸‹ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªitems.pyã€‚æˆ‘ä»¬æ‰“å¼€è¿™ä¸ªæ–‡ä»¶ï¼Œä¼šå‘ç°å®ƒè‡ªå¸¦äº†ä¸€äº›ä»£ç ã€‚

```
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html
 
import scrapy
 
 
class SinaItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass
```

è¿™æ®µä»£ç é‡Œå¯¼å…¥äº†scrapyåŒ…ï¼Œå¹¶è‡ªåŠ¨å¸®æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªSinaItemç±»ï¼Œè¿™é‡Œè¿˜é€šè¿‡æ³¨é‡Šçš„æ–¹æ³•å‘Šè¯‰äº†æˆ‘ä»¬è¿™ä¸ªç±»é‡Œé¢åº”è¯¥æ€ä¹ˆå†™ã€‚æˆ‘ä»¬å°±æ¥ç…§è‘«èŠ¦ç”»ç“¢ï¼ŒæŠŠæˆ‘ä»¬æƒ³è¦çš„å­—æ®µåŠ å…¥è¿›æ¥ã€‚

ä»£ç ä¼šå˜æˆå¦‚ä¸‹å½¢å¼ã€‚

```
class SinaItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()
    desc = scrapy.Field()
    times = scrapy.Field()
	type = scrapy.Field()
```

æˆ‘ä»¬åœ¨é‡Œé¢å®šä¹‰äº†å››ä¸ªå­—æ®µï¼Œåˆ†åˆ«æ˜¯titleã€descã€timeså’Œtypeï¼Œåˆ†åˆ«ç”¨æ¥è¡¨ç¤ºæ ‡é¢˜ã€å†…å®¹ã€æ—¶é—´å’Œç±»å‹ã€‚è¿™é‡Œçš„æ ‡é¢˜å’Œæ—¶é—´é€šè¿‡åˆ—è¡¨æ¥è·å–ï¼Œå†…å®¹æ˜¯è¯¦æƒ…é¡µé‡Œçš„ã€‚è€Œç±»å‹æˆ‘ä»¬é»˜è®¤ä¸ºå›½å†…ï¼Œå¦‚æœåé¢æˆ‘ä»¬éœ€è¦çˆ¬å–å…¶ä»–çš„ç±»åˆ«ï¼Œæ¯”å¦‚ç»¼è‰ºã€ä½“è‚²ç­‰ï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦æ–°å»ºclassäº†ï¼Œåªè¦é‡æ–°åœ¨è¿™ä¸ªtypeå­—æ®µä¸­èµ‹å€¼å³å¯ã€‚

å¥½äº†ï¼Œè¿›è¡Œåˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å°±å¯ä»¥å›åˆ°æˆ‘ä»¬çš„çˆ¬è™«ä»£ç å»å¼•å…¥è¿™éƒ¨åˆ†å†…å®¹äº†ã€‚é¦–å…ˆæˆ‘ä»¬è¦åœ¨ç¬¬ä¸€è¡Œå¼•å…¥æˆ‘ä»¬çš„itemæ–‡ä»¶ï¼ŒåŠ å…¥ä¸‹é¢çš„ä»£ç ã€‚

```
from sina.items import SinaItem
```

ç„¶åï¼Œæˆ‘ä»¬åœ¨çˆ¬è™«æ–‡ä»¶çš„parseå‡½æ•°ä¸­å¼•å…¥SinaItemç±»ï¼Œå¹¶åœ¨å‡½æ•°çš„æœ«å°¾å¯¹å…¶èµ‹å€¼æˆ‘ä»¬æƒ³è¦çš„å†…å®¹ã€‚

```
item = SinaItem()
item['type'] = 'news'
item['title'] = title[i].text
item['times'] = t2
```

æ¥ç€ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åæŠŠitemç»™yieldå‡ºå»ï¼Œåœ¨ä»£ç çš„æœ€ååŠ å…¥å¦‚ä¸‹å†…å®¹ã€‚

```
yield Request(url=response.urljoin(href), meta={'name': item}, callback=self.parse_namedetail)
```

è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥é¡ºåˆ©åœ°æŠŠitemä¿¡æ¯å’ŒURLç»™yieldå‡ºå»äº†ã€‚

æˆ‘ä»¬æ¥çœ‹è¿™ä¸ªyieldçš„å†™æ³•ã€‚æˆ‘ä»¬yieldå‡ºå»çš„å†…å®¹æœ‰ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯URLä¿¡æ¯å’Œitemã€‚itemç”¨Key-Valueçš„å½¢å¼ä¼ è¾“ï¼Œæˆ‘ä»¬æŠŠå®ƒèµ‹å€¼åˆ°äº†Keyä¸ºâ€œnameâ€çš„é”®å€¼å¯¹ä¸­ã€‚

ç„¶åæˆ‘ä»¬åœ¨è¿™é‡Œè¿˜ç”¨åˆ°äº†ä¸€ä¸ªcallbackå‡½æ•°ï¼Œå®ƒå®é™…ä¸Šæ˜¯å›è°ƒäº†ä¸€ä¸ªåä¸ºparse\_namedetailçš„å‡½æ•°ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸‹é¢å»ºç«‹è¿™ä¸ªå‡½æ•°è§£ææˆ‘ä»¬çš„è¯¦æƒ…é¡µä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨parseå‡½æ•°çš„ä¸‹é¢æ–°å»ºä¸€ä¸ªå‡½æ•°parse\_namedeatalå¹¶å®ç°å®ƒï¼Œä»£ç å¦‚ä¸‹ã€‚

```
def parse_namedetail(self, response):
	selector = Selector(response)
	desc = selector.xpath("//div[@class='article']/p/text()").extract()
	item = response.meta['name']
	desc = list(map(str.strip, desc))
	item['desc'] = ''.join(desc)
	print(item)
	yield item
```

åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªSelectorï¼Œå®ƒä¸»è¦æ˜¯Responseç”¨æ¥æå–æ•°æ®çš„ã€‚å½“Spiderçš„Requestå¾—åˆ°Responseä¹‹åï¼ŒSpiderå¯ä»¥ä½¿ç”¨Selectoræå–Responseä¸­çš„æœ‰ç”¨çš„æ•°æ®ã€‚å› æ­¤ï¼Œè¿™é‡Œæˆ‘ä»¬ä¼ å…¥çš„æ˜¯ä¸Šé¢çš„Responseä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯è¯¦æƒ…é¡µçš„Responseã€‚

ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨XPathè¯­æ³•è§£æresponseä¸­çš„HTMLä»£ç ã€‚æˆ‘ä»¬å…ˆæ¥çœ‹ä¸‹æœ‰å“ªäº›XPathè¡¨è¾¾å¼ï¼Œä»¥ä¾¿åç»­æˆ‘ä»¬æ›´å¥½åœ°ä½¿ç”¨å®ƒã€‚

![](https://static001.geekbang.org/resource/image/e2/8b/e276155304958748328c3b3b8855708b.jpg?wh=2076x2000)

ç„¶åæˆ‘ä»¬å†æ¥çœ‹ä¸€ä¸‹è¯¦æƒ…é¡µçš„æ­£æ–‡éƒ¨åˆ†åœ¨Chromeçš„å¼€å‘è€…å·¥å…·ä¸­çš„æºä»£ç ã€‚

![](https://static001.geekbang.org/resource/image/f8/c4/f857999da6671773df7a051d8d1174c4.png?wh=2701x1236)

æ”¾å¤§ä»£ç éƒ¨åˆ†å¾—åˆ°å¦‚ä¸‹ç»“æœã€‚

![](https://static001.geekbang.org/resource/image/6d/70/6dfc9455479797619e9dbf7ba2532470.png?wh=2154x1224)

å¯ä»¥çœ‹åˆ°ï¼Œæ‰€æœ‰çš„æ­£æ–‡éƒ¨åˆ†éƒ½åœ¨classä¸ºarticleå’Œidä¸ºarticleçš„`<div>`æ ‡ç­¾ä¸­ï¼Œå¹¶ä¸”æ¯ä¸ªæ®µè½éƒ½ç”¨`<p>`æ ‡ç­¾åŒ…è£¹ç€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬åªéœ€è¦æ‹¿åˆ°classå’Œidçš„æ ‡ç­¾ï¼Œå¹¶ä¸”æ‹¿åˆ°æ‰€æœ‰çš„pæ ‡ç­¾ï¼Œå°±å¯ä»¥æ‹¿å‡ºæ‰€æœ‰çš„å†…å®¹ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢è¿™è¡Œä»£ç æ¥è·å–æ‰€æœ‰çš„æ­£æ–‡å†…å®¹ã€‚

```
desc = selector.xpath("//div[@class='article']/p/text()").extract()
```

ç®€å•è§£é‡Šä¸€ä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ `//` æ ‡å¿—è·¨èŠ‚ç‚¹è·å–åˆ°äº†classä¸ºarticleçš„æ®µè½ä¸‹é¢æ‰€æœ‰pæ ‡ç­¾ä¸­çš„å†…å®¹ï¼Œå¹¶æŠŠå®ƒä»¬æå–äº†å‡ºæ¥ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ‹¿åˆ°äº†å‰é¢ä¼ å…¥çš„itemä¿¡æ¯ï¼Œå¹¶æŠŠdescåŠ å…¥åˆ°äº†itemä¸­ã€‚

```
item = response.meta['name']
desc = list(map(str.strip, desc))
item['desc'] = ''.join(desc)
```

æœ€åï¼Œæˆ‘ä»¬å†æŠŠè¿™ä¸ªitemç»™yieldå‡ºå»å°±å¯ä»¥äº†ã€‚ç°åœ¨æˆ‘ä»¬å†è¿è¡Œä¸€ä¸‹ä»£ç ä¼šå¾—åˆ°å¦‚ä¸‹è¾“å‡ºã€‚

![](https://static001.geekbang.org/resource/image/4f/d3/4f8d9f3b6c23e9c610fe20b6d54cf7d3.png?wh=2730x396)

åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬çš„æ•°æ®çˆ¬å–å·¥ä½œçœ‹èµ·æ¥å°±å·²ç»å®Œæˆäº†ã€‚

ä½†æ˜¯ç­‰ä¸€ç­‰ï¼Œä½ æœ‰æ²¡æœ‰å‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œç°åœ¨è™½ç„¶å·²ç»èƒ½å¤Ÿçˆ¬å–åˆ°æ•°æ®ï¼Œä½†æ˜¯åªèƒ½çˆ¬å–ä¸€é¡µçš„å†…å®¹ã€‚**è¿™æ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°±ç”¨ç¨‹åºæ¥å®ç°ç¿»é¡µæŒ‰é’®çš„ç‚¹å‡»åŠŸèƒ½ã€‚**

å®é™…ä¸Šï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨parseå‡½æ•°ä¸­åŠ å…¥å¦‚ä¸‹ä»£ç å³å¯ã€‚

```
try:
	driver.find_element_by_xpath("//div[@class='feed-card-page']/span[@class='pagebox_next']/a").click()
except:
	Break
```

è¿™æ®µä»£ç ä¹Ÿä¸éš¾ç†è§£ï¼Œå°±æ˜¯æ‰¾åˆ°ç¿»é¡µå¯¼èˆªæ¡çš„HTMLæ ‡ç­¾ï¼Œç„¶åæ‰¾åˆ°å®ƒçš„`<a>`é“¾æ¥ï¼Œæ‰§è¡Œç‚¹å‡»ã€‚

æœ€åï¼Œæˆ‘ä»¬è¦åœ¨æœ€ä¸Šé¢åŠ ä¸Šä¸€ä¸ªç¿»é¡µæ“ä½œï¼Œå‡è®¾æˆ‘ä»¬åªéœ€è¦ç¿»5é¡µï¼Œæ­¤æ—¶parseå‡½æ•°çš„ä»£ç å¦‚ä¸‹ã€‚

```
def parse(self, response):
	driver = webdriver.Chrome(chrome_options=self.option)
	driver.set_page_load_timeout(30)
	driver.get(response.url)
 
	for i in range(5):
		while not driver.find_element_by_xpath("//div[@class='feed-card-page']").text:
			driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
		title = driver.find_elements_by_xpath("//h2[@class='undefined']/a[@target='_blank']")
		time = driver.find_elements_by_xpath(
			"//h2[@class='undefined']/../div[@class='feed-card-a feed-card-clearfix']/div[@class='feed-card-time']")
		for i in range(len(title)):
			print(title[i].text)
			print(time[i].text)
 
			today = datetime.datetime.now()
			eachtime = time[i].text
			eachtime = eachtime.replace('ä»Šå¤©', str(today.month) + 'æœˆ' + str(today.day) + 'æ—¥')
 
			href = title[i].get_attribute('href')
 
			if 'åˆ†é’Ÿå‰' in eachtime:
				minute = int(eachtime.split('åˆ†é’Ÿå‰')[0])
				t = datetime.datetime.now() - datetime.timedelta(minutes=minute)
				t2 = datetime.datetime(year=t.year, month=t.month, day=t.day, hour=t.hour, minute=t.minute)
			else:
				if 'å¹´' not in eachtime:
					eachtime = str(today.year) + 'å¹´' + eachtime
				t1 = re.split('[å¹´æœˆæ—¥:]', eachtime)
				t2 = datetime.datetime(year=int(t1[0]), month=int(t1[1]), day=int(t1[2]), hour=int(t1[3]),
									   minute=int(t1[4]))
 
			print(t2)
 
			item = SinaItem()
			item['type'] = 'news'
			item['title'] = title[i].text
			item['times'] = t2
 
			yield Request(url=response.urljoin(href), meta={'name': item}, callback=self.parse_namedetail)
 
		try:
			driver.find_element_by_xpath("//div[@class='feed-card-page']/span[@class='pagebox_next']/a").click()
		except:
			Break
```

æˆ‘ä»¬å†è¿è¡Œç¨‹åºï¼Œè¿™æ—¶æˆ‘ä»¬å°±å¯ä»¥çˆ¬å–5é¡µçš„å†…å®¹äº†ï¼Œåœ¨æ­¤ä¹‹åï¼Œçˆ¬è™«ä¼šè‡ªåŠ¨åœæ­¢ã€‚

![](https://static001.geekbang.org/resource/image/c4/d8/c4a396f83ba2b9a060a267cf116656d8.png?wh=3637x623)

è¿™æ ·ï¼Œæˆ‘ä»¬å·²ç»èƒ½å¤Ÿé’ˆå¯¹ä¸€ä¸ªæ ç›®æ¥çˆ¬å–æ•°æ®äº†ã€‚åœ¨åé¢çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šå»¶ç»­è¿™ä¸ªæ€è·¯ï¼Œç„¶åæŠŠæ•°æ®å­˜å‚¨èµ·æ¥å¹¶åšç›¸åº”çš„å¤„ç†ã€‚

## æ€»ç»“

è¿™èŠ‚è¯¾åˆ°è¿™é‡Œä¹Ÿå°±æ¥è¿‘å°¾å£°äº†ï¼Œæˆ‘æ¥ç»™ä½ æ¢³ç†ä¸€ä¸‹è¿™èŠ‚è¯¾çš„ä¸»è¦å†…å®¹ã€‚

1. åœ¨Chromeæµè§ˆå™¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Chrome å¼€å‘è€…å·¥å…·æ¥æŸ¥çœ‹é¡µé¢ä¸Šçš„å…ƒç´ ã€æ ‡è®°å’Œå±æ€§ï¼Œä»¥åŠæŸ¥çœ‹ç½‘ç»œè¯·æ±‚ã€å“åº”å’Œå…¶ä»–è¯Šæ–­ä¿¡æ¯ã€‚
2. Scrapy æä¾›äº†è®¸å¤šå†…ç½®çš„è§£æå™¨ï¼ŒåŒ…æ‹¬ XPath å’Œ CSS é€‰æ‹©å™¨ç­‰ï¼Œè¿™äº›è§£æå™¨å¯ä»¥å¸®åŠ©æˆ‘ä»¬è½»æ¾åœ°ä» HTML é¡µé¢ä¸­æå–æ‰€éœ€æ•°æ®ã€‚
3. åœ¨ Scrapy ä¸­ï¼Œcallback å‡½æ•°å¯ä»¥è¿”å›å¤šä¸ªè¯·æ±‚ï¼Œå¹¶åœ¨ç»“æœè¿”å›æ—¶ä½¿ç”¨ yield å…³é”®å­—ä¼ é€’ç»™ Scrapy å¼•æ“ã€‚
4. item.py æ–‡ä»¶æ˜¯ Scrapy ä¸­ç”¨äºè§£æå’Œå­˜å‚¨æ•°æ®çš„ Python ç±»ã€‚åœ¨ item.py æ–‡ä»¶ä¸­ï¼Œä½ å¯ä»¥å®šä¹‰æ•°æ®æ¨¡å‹ï¼Œç¡®å®šè¦æå–çš„å­—æ®µï¼Œå¹¶å®šä¹‰æ•°æ®çš„ç±»å‹å’Œæ ¼å¼ã€‚
5. åœ¨çˆ¬å–åŒ…å«å¤šä¸ªé¡µé¢çš„ç½‘ç«™æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ next\_page() æ–¹æ³•æ¥æ¨¡æ‹Ÿç”¨æˆ·æ“ä½œï¼Œè°ƒç”¨click(ï¼‰å‡½æ•°è¿›è¡Œç‚¹å‡»ã€‚

## è¯¾åé¢˜

å­¦å®Œè¿™èŠ‚è¯¾ï¼Œè¯·ä½ è¯•ç€å®Œæˆä¸‹é¢ä¸¤é¡¹ä»»åŠ¡ã€‚

1. è·Ÿç€æˆ‘çš„è®²è§£ï¼Œè‡ªå·±å®ç°ä¸€éè¿™èŠ‚è¯¾æ‰€è®²çš„å†…å®¹ã€‚
2. çˆ¬å–æ–°æµªç½‘ç«™ç”µå½±æ¿å—çš„å†…å®¹ã€‚

æ¬¢è¿ä½ åœ¨ç•™è¨€åŒºä¸æˆ‘äº¤æµè®¨è®ºï¼Œä½ ä¹Ÿå¯ä»¥æŠŠä»£ç é“¾æ¥é™„åœ¨è¯„è®ºåŒºï¼Œæˆ‘ä¼šé€‰å–æœ‰ä»£è¡¨æ€§çš„ä»£ç è¿›è¡Œç‚¹è¯„ï¼Œæˆ‘ä»¬ä¸‹èŠ‚è¯¾è§ï¼
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ13ï¼‰</strong></div><ul>
<li><span>Geek_ccc0fd</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æ–°å®‰è£…selemiumçš„APIå˜äº†ï¼Œè€Œä¸”xpathè·å–çš„è·¯å¾„æœ‰ç‚¹é—®é¢˜ï¼Œæˆ‘è¿™é‡Œè·å–ä¸åˆ°ä¸€é¡µçš„å…¨éƒ¨å†…å®¹ï¼Œæˆ‘ä¿®æ”¹äº†ä¸€ä¸‹ï¼š
title = driver.find_elements(By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-item&#39;]&#47;h2&#47;a[@target=&#39;_blank&#39;]&quot;)
time = driver.find_elements(By.XPATH,&quot;&#47;&#47;div[@class=&#39;feed-card-item&#39;]&#47;h2&#47;..&#47;div[@class=&#39;feed-card-a feed-card-clearfix&#39;]&#47;div[@class=&#39;feed-card-time&#39;]&quot;)
ç„¶åå°±æ˜¯ç¿»é¡µç‚¹å‡»é‚£é‡Œæˆ‘è¿™è¾¹è·‘ä¸‹æ¥ä¹Ÿæœ‰é—®é¢˜ï¼Œæ ¹æ®xpathä¼šè·å–ä¸¤ä¸ªaæ ‡ç­¾,æ‰€ä»¥éœ€è¦å¢åŠ ç´¢å¼•ï¼š
driver.find_elements(By.XPATH,&quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;)[0].click()</p>2023-05-06</li><br/><li><span>æœªæ¥å·²æ¥</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>æˆªæ­¢åˆ° 5æœˆ3æ—¥ï¼Œæ–°å®‰è£…çš„ selemium åªæœ‰ find_elements æ–¹æ³•ï¼Œè€å¸ˆçš„ä»£ç éœ€æ”¹ä¸ºï¼š
`title = driver.find_elements(By.XPATH, &quot;&#47;&#47;h2[@class=&#39;undefined&#39;]&#47;a[@target=&#39;_blank&#39;]&quot;)`
`time = driver.find_elements(By.XPATH, &quot;&#47;&#47;h2[@class=&#39;undefined&#39;]&#47;..&#47;div[@class=&#39;feed-card-a feed-card-clearfix&#39;]&#47;div[@class=&#39;feed-card-time&#39;]&quot;)`
ä»¥æ­¤ç±»æ¨</p>2023-05-03</li><br/><li><span>Abigail</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>åº”è¯¥è®¾è®¡ä¸€ä¸ªç®€å•ç‚¹çš„ä¾‹å­, python èµ·ç ä¹Ÿè¦ç”¨ 3.9 å•Š</p>2023-10-24</li><br/><li><span>alexliu</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>åœ¨è¿è¡Œä¸‹ä¸€é¡µclick()çš„æ—¶å€™ï¼Œæœ‰å¯èƒ½å‡ºç°ElementNotInteractableExceptioné”™è¯¯ï¼Œè§£å†³æ–¹æ¡ˆï¼š
1ã€åœ¨driver.get(response.url)å’Œclick()åæ·»åŠ å»¶æ—¶time.sleep(2)
2ã€ä¿æŒchromeçš„çª—å£å¤§å°ä¸€è‡´ self.option.add_argument(&quot;--window-size=1960,1080&quot;)
try... except...éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š
            try:
                _next = driver.find_elements(By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;)
                _next[0].click()
                _time.sleep(2)
            except StaleElementReferenceException as e:
                print(&quot; page failed.&quot;, e)
                _next = driver.find_elements(By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;)
                _next[0].click()
                _time.sleep(2)
            except ElementNotInteractableException as e:
                print(&quot; not found page.&quot;, e)
                break
            except Exception as e:
                print(&quot;unkwon error: &quot;, e)</p>2023-06-01</li><br/><li><span>Geek_ccc0fd</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æˆ‘ä»¬åœ¨parseé‡Œé¢å¯ä»¥ç›´æ¥ä½¿ç”¨response.xpathè·å–å…ƒç´ ï¼Œå’Œä½¿ç”¨ driver.find_elementsæ˜¯åŒæ ·çš„æ•ˆæœï¼Œä¸ºä»€ä¹ˆè¿˜è¦ç”¨seleniumæ¥åšæµè§ˆå™¨çš„æ“ä½œå‘¢ï¼Ÿ</p>2023-05-06</li><br/><li><span>å®‰è²å°”å¾·</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¯·é—®å“ªæœ‰main.pyæ–‡ä»¶å‘¢ï¼Œæ²¡æœ‰çœ‹åˆ°</p>2023-05-02</li><br/><li><span>peter</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>Q3ï¼šæºç æ”¾åœ¨ä»€ä¹ˆåœ°æ–¹å•Šï¼Ÿèƒ½å¦æŠŠæºç é›†ä¸­æ”¾åˆ°ä¸€ä¸ªå…¬å…±åœ°æ–¹? æ¯”å¦‚githubç­‰ã€‚</p>2023-04-26</li><br/><li><span>peter</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>Q1ï¼šç¬¬ä¸ƒè¯¾ï¼Œåˆ›å»ºç¯å¢ƒçš„æœ€åå‡ æ­¥ï¼Œä¸åœå‡ºé”™ï¼Œæœ€åä¸€ä¸ªé”™è¯¯æ˜¯ï¼šæ‰§è¡Œâ€œscrapy genspider sina_spider sina.com.cnâ€ï¼ŒæŠ¥å‘Šï¼šlib\string.py&quot;, line 132, in substitute
    return self.pattern.sub(convert, self.template)
TypeError: cannot use a string pattern on a bytes-like object
ç½‘ä¸Šæœäº†ï¼Œå¤§æ„è¯´æ˜¯python2å’Œpython3ä¸åŒ¹é…å¯¼è‡´çš„ã€‚ æˆ‘æ˜¯å®Œå…¨æŒ‰ç…§è€å¸ˆçš„æ­¥éª¤æ¥å®‰è£…çš„ï¼Œå®‰è£…çš„æ˜¯pythno3ï¼Œæ€ä¹ˆä¼šæœ‰python2å‘¢ï¼Ÿå½“ç„¶ï¼Œè¿™ä¸ªæ–‡ä»¶è¿˜æ²¡æœ‰è§£å†³ï¼Œè¿›è¡Œä¸ä¸‹å»äº†ï¼Œéƒé—·å•Šã€‚ 
Q2ï¼šèƒ½å¦å»ºä¸€ä¸ªå¾®ä¿¡ç¾¤ï¼Ÿé‡åˆ°é—®é¢˜å¯ä»¥åå•†ã€‚ å¦å¤–ï¼Œè€å¸ˆèƒ½å¦æ›´åŠæ—¶åœ°å›å¤ç•™è¨€ï¼Ÿ</p>2023-04-26</li><br/><li><span>GACÂ·DU</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œå…³äºä»£ç æœ‰äº›ç–‘æƒ‘ï¼Œç¬¬ä¸€ï¼šä¸ºä»€ä¹ˆparse_namedetailæ–¹æ³•ä¸å†ä½¿ç”¨driverå‘èµ·httpè¯·æ±‚å’Œè·å–htmlæ ‡ç­¾å†…å®¹ï¼Ÿ
ç¬¬äºŒï¼šdesc = response.xpath(&quot;&#47;&#47;div[@class=&#39;article&#39;]&#47;p&#47;text()&quot;).extract()
        desc = selector.xpath(&quot;&#47;&#47;div[@class=&#39;article&#39;]&#47;p&#47;text()&quot;).extract() 
æˆ‘æµ‹è¯•äº†ä¸¤ä¸ªä»£ç éƒ½å¯ä»¥ä½¿ç”¨ï¼Œé‚£ä¸ºä»€ä¹ˆä¸ç›´æ¥ä½¿ç”¨responseï¼Œåè€Œè¦ç”Ÿæˆä¸€ä¸ªselectorï¼Ÿ</p>2023-04-26</li><br/><li><span>WeitzenbÃ¶ck</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>å¦‚æœå‡ºç°è¿™ä¸ªé”™è¯¯ï¼š__init__() got an unexpected keyword argument &#39;chrome_options&#39; ä»£ç é‡Œæ”¹ä¸º driver = webdriver.Chrome(options=self.option)å…·ä½“æºç æ˜¯ï¼š
class WebDriver(ChromiumDriver):
    &quot;&quot;&quot;Controls the ChromeDriver and allows you to drive the browser.&quot;&quot;&quot;

    def __init__(
        self,
        options: Options = None,
        service: Service = None,
        keep_alive: bool = True,
    ) -&gt; None:
        &quot;&quot;&quot;Creates a new instance of the chrome driver. Starts the service and
        then creates new instance of chrome driver.

        :Args:
         - options - this takes an instance of ChromeOptions
         - service - Service object for handling the browser driver if you need to pass extra details
         - keep_alive - Whether to configure ChromeRemoteConnection to use HTTP keep-alive.
        &quot;&quot;&quot;
        self.service = service if service else Service()
        self.options = options if options else Options()
        self.keep_alive = keep_alive

        self.service.path = DriverFinder.get_path(self.service, self.options)

        super().__init__(
            DesiredCapabilities.CHROME[&quot;browserName&quot;],
            &quot;goog&quot;,
            self.options,
            self.service,
            self.keep_alive,
        )</p>2023-06-15</li><br/><li><span>é£è½»æ‰¬</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>macç³»ç»Ÿï¼Œçˆ¬å–è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼šæŠ¥é”™ï¼šæ— æ³•æ‰“å¼€chromedriverï¼Œå› ä¸ºæ— æ³•éªŒè¯å¼€å‘è€…ã€‚
å¦‚æœæ˜¯brewå®‰è£…çš„chromedriverï¼Œå¯ä»¥æ‰§è¡Œï¼šxattr -d com.apple.quarantine &#47;opt&#47;homebrew&#47;bin&#47;chromedriverè¿›è¡Œå¯ä¿¡æˆæƒã€‚
å¦‚æœä¸æ˜¯brewå®‰è£…çš„ï¼Œéœ€è¦è‡ªå·±æ‰¾åˆ°chromedriverçš„å®‰è£…è·¯å¾„ï¼Œç„¶åæ‰§è¡Œxattr -d com.apple.quarantine ä½ çš„chromedriverçš„è·¯å¾„</p>2024-08-22</li><br/><li><span>æ‚Ÿå°˜</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p> # å°è¯•ç‚¹å‡»ä¸‹ä¸€é¡µ
            try:
                # next_page_link = WebDriverWait(driver, 30).until(
                #     EC.element_to_be_clickable(
                #         (By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;))
                # )
                # next_page_link.click()
                driver.find_elements(By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;)[0].click()
            except Exception as e:
                print(f&quot;Error clicking next page link: {e}&quot;)
                break

æ‰“å‡ºçš„å¼‚å¸¸æ˜¯ï¼šError clicking next page link: Message: stale element reference: stale element not found

è¿™æ˜¯å› ä¸ºä»€ä¹ˆï¼Ÿ</p>2023-12-12</li><br/><li><span>æ‚Ÿå°˜</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>ä¸ºä»€ä¹ˆæˆ‘çš„ç¿»é¡µä¸èµ·ä½œç”¨å‘¢ï¼Ÿ
</p>2023-12-12</li><br/>
</ul>