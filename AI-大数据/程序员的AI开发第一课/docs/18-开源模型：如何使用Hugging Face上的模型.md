ä½ å¥½ï¼Œæˆ‘æ˜¯éƒ‘æ™”ï¼

ç»è¿‡å‰é¢å‡ ä¸ªéƒ¨åˆ†çš„è®²è§£ï¼Œæˆ‘ä»¬ç°åœ¨å·²ç»å®Œå…¨æœ‰èƒ½åŠ›æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„å¤§æ¨¡å‹åº”ç”¨äº†ã€‚ä¸è¿‡ï¼Œä¹‹å‰æ‰€æœ‰çš„åº”ç”¨éƒ½æ˜¯åŸºäº OpenAI çš„ GPT æ¨¡å‹æ„å»ºçš„ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ è®²ï¼Œæˆ‘ä»¬ä¼šè°ˆè°ˆå…¶å®ƒçš„æ¨¡å‹ã€‚

æ—¢ç„¶ GPT æ¨¡å‹å·²ç»å¾ˆå¥½ç”¨äº†ï¼Œä¸ºä»€ä¹ˆè¦ä½¿ç”¨å…¶å®ƒæ¨¡å‹å‘¢ï¼Ÿä¸€ä¸ªå¾ˆç›´æ¥çš„é—®é¢˜å°±æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ GPTï¼Œè¿™å°±æ„å‘³ç€ï¼Œæ‰€æœ‰çš„è¯·æ±‚éƒ½ä¼šå‘é€ç»™ç¬¬ä¸‰æ–¹ã€‚è¿™ç§åšæ³•å¯¹äºå¾ˆå¤šä¼ä¸šæ¥è¯´æ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œä½†è¿˜æœ‰å¾ˆå¤šå¤§ä¼ä¸šï¼Œå®ƒä»¬ç‰¹åˆ«åœ¨æ„çš„å°±æ˜¯æ•°æ®å®‰å…¨æ€§ï¼ŒæŠŠæ•°æ®å‘åˆ°å¤–éƒ¨æ˜¯ä¸€ä»¶æ— æ³•æ¥å—çš„äº‹æƒ…ï¼Œæ›´æœ‰ç”šè€…ï¼Œå…¶å†…éƒ¨æœåŠ¡ä¸å¤–éƒ¨ç½‘ç»œæ˜¯æ–­å¼€çš„ã€‚

æ‰€ä»¥ï¼Œåœ¨ä¼ ç»Ÿå¼€å‘ä¸­ï¼Œå¦‚æœä¸ºè¿™äº›ä¼ä¸šæœåŠ¡ï¼Œä¸€ä¸ªé‡è¦çš„è®®é¢˜å°±æ˜¯ç§æœ‰åŒ–éƒ¨ç½²ï¼Œä½†åƒ OpenAI è¿™æ ·çš„æœåŠ¡æ˜¾ç„¶æ˜¯æ— æ³•æ»¡è¶³ç§æœ‰åŒ–éƒ¨ç½²çš„éœ€æ±‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªèƒ½å¤Ÿè¿è¡Œåœ¨æœ¬åœ°çš„å¤§æ¨¡å‹å°±æ˜¾å¾—è‡³å…³é‡è¦äº†ï¼Œä¹‹æ‰€ä»¥æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œæ¨¡å‹ï¼Œæ˜¯å› ä¸ºæœ‰äººæŠŠå·²ç»åšå¥½çš„æ¨¡å‹å¼€æ”¾äº†å‡ºæ¥ï¼Œè¿™å°±æ˜¯å¼€æºçš„åŠ›é‡ã€‚

è¿™ä¸€è®²ï¼Œæˆ‘ä»¬è®²è®²å¦‚ä½•ä½¿ç”¨å¼€æºæ¨¡å‹ã€‚

## Hugging Face

è°ˆåŠå¼€æºæ¨¡å‹ï¼Œç›®å‰æœ€çŸ¥åçš„ AI æ¨¡å‹å¼€æºç¤¾åŒºæ˜¯ Hugging Faceã€‚Hugging Face ä¹‹äº AI ç¤¾åŒºï¼ŒçŠ¹å¦‚ Github ä¹‹äºç¨‹åºå‘˜ç¤¾åŒºã€‚

![](https://static001.geekbang.org/resource/image/e0/3a/e0bc6cbd37851005c06278e36215913a.jpg?wh=1094x716)

Hugging Face åŸæœ¬æ˜¯ä¸€å®¶é’ˆå¯¹é’å°‘å¹´æä¾›èŠå¤©æœºå™¨äººçš„å…¬å¸ï¼Œæ‰€ä»¥ï¼Œé€‰æ‹©äº†ä¸€ä¸ªè¡¨æƒ…ç¬¦å·ï¼ˆğŸ¤—ï¼‰æ¥ç»™å…¬å¸å‘½åã€‚ä¸è¿‡ï¼Œåˆ›å§‹äººå¾ˆå¿«å°±æ„è¯†åˆ°çœŸæ­£æœ‰æ½œåŠ›çš„å¹¶ä¸æ˜¯èŠå¤©æœºå™¨äººæœ¬èº«ï¼Œè€Œæ˜¯å…¶èƒŒåçš„æ¨¡å‹ã€‚

äºæ˜¯ï¼ŒHugging Face è½¬å‹æˆäº†ä¸€ä¸ª AI ç¤¾åŒºï¼Œå®ƒå¼€æ”¾äº†è‡ªå·±åº”ç”¨åº•å±‚çš„ä¸€äº›ä»£ç ï¼Œæ–¹ä¾¿è®¿é—®ç¤¾åŒºé‡Œçš„æ¨¡å‹ã€‚å¤§æ¨¡å‹çš„æ ¸å¿ƒæœºåˆ¶æ˜¯ Transformer æ¶æ„ï¼Œå…¶è®ºæ–‡å‘è¡¨äº 2017 å¹´ï¼Œè€Œ Hugging Face çš„ç«çˆ†å§‹äº 2018 å¹´ï¼Œä¸€ä¸ªå¤‡å—å…³æ³¨çš„æ¨¡å‹æ¶æ„æœ‰äº†ä¸€ä¸ªå¯ä»¥åˆ†äº«è®¨è®ºçš„ç¤¾åŒºï¼ŒäºŒè€…ç›¸äº’ä¿ƒè¿›ï¼Œç›¸å¾—ç›Šå½°ã€‚

å¦‚ä»Šçš„ Hugging Face ä¸»è¦é™¤äº†å¯ä»¥åˆ†äº«æ¨¡å‹ï¼ˆmodelï¼‰ï¼Œè¿˜å¯ä»¥åˆ†äº«æ•°æ®é›†ï¼ˆdatasetï¼‰ï¼Œè¿˜æœ‰éƒ¨ç½²èµ·æ¥çš„åº”ç”¨ï¼ˆSpaceï¼‰ã€‚æ¨¡å‹ä¹Ÿä¸æ­¢æ˜¯è¯­è¨€æ¨¡å‹ï¼Œè€Œæ˜¯å…¶å®ƒå„å¼å„æ ·çš„æ¨¡å‹ï¼Œæ¯”å¦‚å›¾åƒç”Ÿæˆæ¨¡å‹ã€è¯­éŸ³ç”Ÿæˆæ¨¡å‹ç­‰ç­‰ã€‚ä¸ºäº†è®©ç”¨æˆ·æ›´å¥½åœ°ä½¿ç”¨å…¶æœåŠ¡ï¼ŒHugging Face è¿˜æä¾›äº†å¾ˆå¤šçš„å·¥å…·ï¼Œæ¯”å¦‚ç®€åŒ–æ¨¡å‹ä½¿ç”¨çš„ transformers ç¨‹åºåº“ã€ç®€åŒ– AI åº”ç”¨å¼€å‘å’Œéƒ¨ç½²çš„ Gradio ç­‰ç­‰ã€‚

å¥½äº†ï¼Œæˆ‘ä»¬å·²ç»å¯¹ Hugging Face æœ‰äº†ä¸€ä¸ªåˆæ­¥çš„äº†è§£ã€‚ä»Šå¤©çš„ Hugging Face å·²ç»æˆä¸ºäº†ä»»ä½•ä¸€ä¸ªå›¢é˜Ÿå‘å¸ƒæ¨¡å‹çš„é¦–é€‰ä¹‹åœ°ï¼Œæˆ‘ä»¬è¦ä½¿ç”¨çš„å„ç§æ¨¡å‹å‡ ä¹éƒ½èƒ½åœ¨ Hugging Face ä¸Šæ‰¾åˆ°ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±æ¥çœ‹çœ‹å¦‚ä½•åœ¨æˆ‘ä»¬çš„ä»£ç é‡Œè°ƒç”¨ Hugging Face ä¸Šçš„æ¨¡å‹ã€‚

## ä½¿ç”¨ pipeline è°ƒç”¨æ¨¡å‹

è°ƒç”¨ Hugging Face æ¨¡å‹æœ‰ä¸¤ç§æ–¹å¼ï¼šé«˜å±‚æ¥å£å’Œåº•å±‚æ¥å£ã€‚é«˜å±‚æ¥å£ç›¸å½“äºæ˜¯å¯¹åº•å±‚æ¥å£åšäº†å°è£…ï¼Œè®©ä»£ç ä½¿ç”¨èµ·æ¥æ›´å®¹æ˜“ï¼Œç›¸å¯¹è€Œè¨€ï¼Œåº•å±‚æ¥å£åˆ™æœ‰æ›´å¤§çš„çµæ´»æ€§ã€‚åœ¨é«˜å±‚æ¥å£é‡Œï¼Œæˆ‘ä»¬æ ¸å¿ƒè¦çŸ¥é“çš„ä¸€ä¸ªæ¦‚å¿µå°±æ˜¯ç®¡é“ï¼ˆpipelineï¼‰ã€‚å’Œè½¯ä»¶é¢†åŸŸæ‰€æœ‰å«ç®¡é“çš„æ¦‚å¿µä¸€æ ·ï¼Œå®ƒè¦åšçš„å°±æ˜¯ä¸€æ­¥ä¸€æ­¥åœ°è¿›è¡Œå¤„ç†ï¼Œä¸€ä¸ªé˜¶æ®µå®Œæˆä¹‹åï¼Œäº¤ç»™ä¸‹ä¸€ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```python
import torch
from transformers import pipeline

device = "cuda" if torch.cuda.is_available() else "cpu"

messages = [
    {"role": "user", "content": "è¯·å†™ä¸€é¦–èµç¾ç§‹å¤©çš„äº”è¨€ç»å¥"},
]
pipe = pipeline("text-generation", model="Qwen/Qwen2.5-0.5B-Instruct", device=device, max_new_tokens=100)
result = pipe(messages)
print(result[-1]['generated_text'][-1]['content'])

```

æˆ‘ç®€åŒ–äº†ä¸€ä¸‹ä»£ç ï¼Œè¿™æ®µä»£ç çš„æ ¸å¿ƒå°±æ˜¯è¿™ä¸¤å¥ï¼š

```python
pipe = pipeline("text-generation", model="Qwen/Qwen2.5-0.5B-Instruct")
result = pipe(messages)

```

æˆ‘ä»¬å…ˆæ„å»ºäº†ä¸€ä¸ªç®¡é“ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æŒ‡å®šäº†å®ƒçš„ç”¨é€”ï¼Œè¿™é‡Œæ˜¯æ–‡æœ¬ç”Ÿæˆï¼ˆtext-generationï¼‰ï¼Œ `pipeline` ä¼šæ ¹æ®ä¸åŒçš„ç”¨é€”è¿›è¡Œä¸åŒçš„ç®¡é“é…ç½®ã€‚ç¬¬äºŒä¸ªå‚æ•°æ˜¯æ¨¡å‹ï¼Œåœ¨è¿™ä¸ªä¾‹å­é‡Œé¢ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ¨¡å‹æ˜¯é˜¿é‡Œçš„é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œå¼•ç”¨æ¨¡å‹çš„æ–¹å¼å°±æ˜¯â€œç”¨æˆ·å/æ¨¡å‹åâ€ï¼Œåœ¨è¿™é‡Œå°±æ˜¯â€œQwen/Qwen2.5-0.5B-Instructâ€ã€‚

æ„å»ºå¥½äº†ç®¡é“ï¼Œå°±å¯ä»¥è°ƒç”¨æ¨¡å‹äº†ï¼Œæˆ‘ä»¬æŠŠæ¶ˆæ¯ä¼ ç»™æ¨¡å‹ï¼Œå®ƒå°±ä¼šç»™æˆ‘ä»¬äº§ç”Ÿä¸€ä¸ªç»“æœã€‚ä¸‹é¢æ˜¯æˆ‘ä¸€æ¬¡æ‰§è¡Œçš„ç»“æœï¼š

```python
ç§‹é£é€çˆ½è‡³ï¼Œè½å¶é“ºé‡‘åœ°ã€‚
ä¸°æ”¶ç¨»è°·é¦™ï¼Œä¸°æ”¶å–œæ‚¦èµ·ã€‚

```

è¿™æ®µä»£ç æœ¬èº«å¾ˆå®¹æ˜“ç†è§£ï¼Œä¸è¿‡ï¼Œç¬¬ä¸€æ¬¡æ‰§è¡Œè¿™æ®µä»£ç å¯èƒ½ä¼šé‡åˆ°å¾ˆå¤šé—®é¢˜ã€‚æœ‰çš„æ¨¡å‹åœ¨è®¿é—®ä¸Šæ˜¯éœ€è¦å—åˆ°é™åˆ¶çš„ï¼Œæ¯”å¦‚ï¼ŒMeta çš„ Llamaï¼Œéœ€è¦æˆ‘ä»¬å…ˆå»åšç›¸åº”çš„ç”³è¯·ã€‚æˆ‘ä»¬è¦æœ‰ Hugging Face è´¦æˆ·ï¼Œå†å»åšå¯¹åº”çš„ç”³è¯·ï¼Œç„¶åï¼Œè¿˜éœ€è¦é…ç½®è‡ªå·±çš„ Access Tokenï¼Œå†å»åšç›¸åº”çš„é…ç½®ï¼š

```bash
export HUGGINGFACE_TOKEN=<AccessToken>

```

å†æœ‰ï¼Œåœ¨å›½å†…è®¿é—® Hugging Face é€Ÿåº¦ä¸Šå¯èƒ½ä¼šæœ‰äº›é—®é¢˜ã€‚è¿™æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘ä½¿ç”¨é•œåƒç«™ï¼Œè¿™ä¹Ÿéœ€è¦åšä¸€äº›é…ç½®ï¼š

```bash
export HF_ENDPOINT=<é•œåƒç½‘ç«™>

```

å‰é¢è¯´äº†ï¼Œpipeline æ¨¡å‹çš„ç¬¬ä¸€ä¸ªå‚æ•°æŒ‡å®šäº†ç”¨é€”ã€‚é™¤äº†åƒå¤§æ¨¡å‹åšæ–‡æœ¬ç”Ÿæˆï¼ŒHugging Face æä¾›äº†å¤§é‡çš„ä¸åŒæ¨¡å‹ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬å®Œæˆå…¶å®ƒçš„å·¥ä½œã€‚æ¯”å¦‚åé¢è¿™ä¸ªä¾‹å­ï¼š

```python
import torch
from transformers import pipeline

device = "cuda" if torch.cuda.is_available() else "cpu"

pipe = pipeline("translation", model="Helsinki-NLP/opus-mt-zh-en", device=device)
result = pipe("ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘æƒ³å‡ºå»ç©ã€‚")
print(result[-1]['translation_text'])

```

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªç¿»è¯‘åŠŸèƒ½ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ä¸ªèƒ½å¤Ÿå°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡çš„æ¨¡å‹ã€‚ä»ä»£ç ä¸Šçœ‹ï¼Œå®ƒå’Œå‰é¢çš„ä»£ç å‡ ä¹æ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚åªæ˜¯é€‰æ‹©ä¸åŒçš„ç”¨é€”ä»¥åŠå¯¹åº”çš„æ¨¡å‹ï¼Œç›¸åº”åœ°ï¼Œè¿”å›ç»“æœä¹Ÿæ ¹æ®æˆ‘ä»¬çš„é€‰æ‹©æœ‰æ‰€å·®å¼‚ã€‚

ä¸‹é¢æ˜¯ä¸€æ¬¡æ‰§è¡Œçš„ç»“æœï¼š

```python
It's a nice day. I want to go out.

```

é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæˆ‘ä»¬åœ¨åˆå§‹åŒ– pipeline çš„æ—¶å€™ï¼Œä¹Ÿå¯ä»¥ä¸ä¼ æ¨¡å‹ï¼Œè¿™æ ·çš„è¯ï¼Œåœ¨æ„å»ºçš„æ—¶å€™ï¼Œ `pipeline` ä¼šé€‰æ‹©é»˜è®¤çš„æ¨¡å‹ã€‚å¦‚æœä½ æœ‰å…´è¶£ï¼Œä¸å¦¨åˆ° `pipeline` çš„æºç ä¸­å»çœ‹çœ‹é»˜è®¤çš„æ¨¡å‹éƒ½æ˜¯ä»€ä¹ˆã€‚

äº‹å®ä¸Šï¼ŒHugging Face ä¸Šæœ‰å¾ˆå¤šçš„ [ä¸åŒæ¨¡å‹](https://huggingface.co/models)ï¼Œèƒ½å¤Ÿå®Œæˆå„å¼å„æ ·çš„ä»»åŠ¡ï¼Œéå¸¸å€¼å¾—æˆ‘ä»¬å»æŒ–æ˜ã€‚æˆ‘å»ºè®®ä½ å…ˆå»äº†è§£ä¸€ä¸‹è¿™äº›æ¨¡å‹éƒ½æœ‰å“ªäº›ç”¨é€”ï¼Œç„¶åï¼Œåœ¨éœ€è¦çš„æ—¶å€™ï¼Œé€‰æ‹©ä¸åŒçš„æ¨¡å‹ã€‚

## ç”¨åº•å±‚å®ç°è°ƒç”¨æ¨¡å‹

å‰é¢æˆ‘ä»¬å·²ç»äº†è§£äº†ç®¡é“çš„æ¦‚å¿µï¼Œå¯¹é«˜å±‚æ¥å£æœ‰äº†åŸºæœ¬çš„äº†è§£ã€‚ä¸è¿‡ï¼Œç®¡é“å°è£…èµ·æ¥çš„æµç¨‹æ˜¯ä»€ä¹ˆæ ·çš„å‘¢ï¼Ÿè¿™å°±éœ€è¦æˆ‘ä»¬äº†è§£ä¸€ä¸‹åº•å±‚å®ç°ã€‚

ä¸‹é¢å°±æ˜¯ä¸€ä¸ªä½¿ç”¨åº•å±‚çš„å®ç°ï¼Œå®ƒå®ç°çš„åŠŸèƒ½ä¸ä¸Šä¸€æ®µä»£ç å®Œå…¨ä¸€æ ·ï¼Œç†è§£äº†å®ƒçš„å®ç°ï¼Œä½ å°±åŸºæœ¬ä¸ŠçŸ¥é“ pipeline æ˜¯æ€æ ·å®ç°çš„äº†ã€‚

```plain
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

messages = [
    {"role": "user", "content": "è¯·å†™ä¸€é¦–èµç¾æ˜¥å¤©çš„è¯—ï¼Œè¦æ±‚ä¸åŒ…å«æ˜¥å­—"},
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)

```

ä»ä»£ç ä¸Šçœ‹ï¼Œå®ƒçš„æµç¨‹å¤æ‚å¤šäº†ã€‚ä½†å®é™…ä¸Šï¼Œåªè¦æˆ‘ä»¬ç†è§£äº†å¤§æ¨¡å‹çš„å¤„ç†è¿‡ç¨‹ï¼Œè¿™æ®µä»£ç å°±å¾ˆå®¹æ˜“ç†è§£äº†ã€‚è¿™ä¸ªè¿‡ç¨‹çš„æ ¸å¿ƒå°±æ˜¯ä¸‰æ­¥ã€‚

ç¬¬ä¸€æ­¥ï¼ŒæŠŠè¾“å…¥è½¬æ¢æˆ Tokenã€‚

```python
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

```

ç¬¬äºŒæ­¥ï¼Œå¤§æ¨¡å‹æ ¹æ®è¾“å…¥ç”Ÿæˆç›¸åº”çš„å†…å®¹ã€‚

```python
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)

```

ç¬¬ä¸‰æ­¥ï¼Œç”Ÿæˆçš„ç»“æœæ˜¯ Tokenï¼Œè¿˜éœ€è¦æŠŠå®ƒè½¬æˆæ–‡æœ¬ã€‚

```python
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

```

è¿˜è®°å¾—æˆ‘ä»¬åœ¨ [02 è®²](https://time.geekbang.org/column/intro/100839101) ä»‹ç»è¿‡çš„å¤§æ¨¡å‹å¤„ç†æµç¨‹å—ï¼ŸåŸºæœ¬ä¸Šå°±æ˜¯è¿™æ ·ä¸€ä¸ªè¿‡ç¨‹ã€‚å› ä¸ºè¿‡ç¨‹ä¸­æ¶‰åŠåˆ°äº† Token å’Œæ–‡æœ¬ä¹‹é—´çš„è½¬æ¢ï¼Œæ‰€ä»¥ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ª Tokenizerï¼Œå®ƒå°±æ˜¯è´Ÿè´£è½¬æ¢çš„æ¨¡å‹ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¤§æ¨¡å‹éƒ½è¦å’Œå¯¹åº”çš„ Tokenizer ä¸€èµ·ä½¿ç”¨ï¼Œæ‰€ä»¥ï¼Œä½ ä¼šçœ‹åˆ°å®ƒä¿©å¾€å¾€ç»™å‡ºçš„æ˜¯åŒä¸€ä¸ªæ¨¡å‹åå­—ï¼š

```python
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

```

## æµå¼è¾“å‡º

å‰é¢å®ç°çš„åŠŸèƒ½éƒ½æ˜¯ç”±å¤§æ¨¡å‹ä¸€æ¬¡æ€§ç”Ÿæˆçš„ã€‚åœ¨èŠå¤©æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä½¿ç”¨æµå¼è¾“å‡ºã€‚åœ¨ Hugging Face çš„ API é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Streamer æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä»åå­—ä¸Šå°±å¯ä»¥çŸ¥é“ï¼Œå®ƒæ˜¯ç”¨æ¥å¤„ç†æµå¼è¾“å‡ºçš„ã€‚ä¸‹é¢å°±æ˜¯ä¸€ä¸ªæ¼”ç¤ºäº†æµå¼è¾“å‡ºçš„ä¾‹å­ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

messages = [
    {"role": "user", "content": "è¯·å†™ä¸€é¦–èµç¾ç§‹å¤©çš„äº”è¨€ç»å¥"},
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    streamer=streamer,
)

```

åœ¨è¿™ä¸ªä¾‹å­é‡Œï¼Œæˆ‘ä»¬ç”¨åˆ°äº† TextStreamerï¼Œå®ƒä¼šç›´æ¥æŠŠç”Ÿæˆç»“æœè¾“å‡ºåˆ°æ§åˆ¶å°ä¸Šã€‚å¦‚æœæˆ‘ä»¬è¦å®ç°ä¸€ä¸ªæ§åˆ¶å°åº”ç”¨ï¼Œå®ƒæ˜¯å¯ä»¥ç”¨çš„ã€‚ä½†æ›´å¤šçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ‹¿åˆ°è¾“å‡ºç»“æœï¼Œå†å»åšç›¸åº”çš„å¤„ç†ï¼Œæ¯”å¦‚ï¼ŒæœåŠ¡ç«¯æŠŠç”Ÿæˆçš„å†…å®¹å‘é€ç»™å®¢æˆ·ç«¯ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ TextIteratorStreamerï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, TextIteratorStreamer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

messages = [
    {"role": "user", "content": "è¯·å†™ä¸€é¦–èµç¾ç§‹å¤©çš„äº”è¨€ç»å¥"},
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
generation_kwargs = dict(model_inputs, streamer=streamer, max_new_tokens=20)
thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

for text in streamer:
    print(text)

```

ä¸ä¹‹å‰æœ€å¤§çš„ä¸åŒæ˜¯ï¼Œè¿™æ®µä»£ç å¯ç”¨äº†å¤šçº¿ç¨‹ï¼Œè¿™æ ·ä¸€æ¥ï¼Œç”Ÿæˆå’Œè¾“å‡ºæ˜¯å¼‚æ­¥å¤„ç†çš„ï¼Œä¸ä¼šå½¼æ­¤é˜»å¡ï¼Œæ›´ç¬¦åˆçœŸå®ä»£ç ä¸­çš„å¤„ç†ã€‚æ­£å¦‚ TextIteratorStreamer è¿™ä¸ªåå­—æ‰€æ˜¾ç¤ºçš„ï¼Œå®ƒå®ç°äº† Iteratorï¼Œæ‰€ä»¥ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å…¶ä¸Šè¿›è¡Œè¿­ä»£ã€‚

æœ‰äº†å¯¹ Streamer çš„ç†è§£ï¼Œæˆ‘ä»¬å°±å¯ä»¥å›åˆ° pipeline ä¸Šï¼Œç»™ pipeline å¢åŠ æµå¼è¾“å‡ºçš„èƒ½åŠ›ï¼š

```python
import torch
from transformers import pipeline, TextIteratorStreamer

device = "cuda" if torch.cuda.is_available() else "cpu"

messages = [
    {"role": "user", "content": "è¯·å†™ä¸€é¦–èµç¾ç§‹å¤©çš„äº”è¨€ç»å¥"},
]

pipe = pipeline("text-generation", model="Qwen/Qwen2.5-0.5B-Instruct", device=device, max_new_tokens=100)

streamer = TextIteratorStreamer(pipe.tokenizer, skip_prompt=True, skip_special_tokens=True)

generation_kwargs = dict(text_inputs=messages, streamer=streamer)
thread = Thread(target=pipe, kwargs=generation_kwargs)
thread.start()

for text in streamer:
    print(text)

```

è‡³æ­¤ï¼Œæˆ‘ä»¬å¯¹å¦‚ä½•è°ƒç”¨ Hugging Face ä¸Šçš„æ¨¡å‹æœ‰äº†ä¸€ä¸ªåˆæ­¥çš„äº†è§£ï¼Œæœ‰äº†è¿™ä¸ªåŸºç¡€ï¼Œæˆ‘ä»¬ä¸‹ä¸€è®²æ¥çœ‹çœ‹å¦‚ä½•åœ¨é¡¹ç›®ä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚

## æ€»ç»“æ—¶åˆ»

Hugging Face è¿™ä¸ªç›®å‰æœ€çŸ¥åçš„ AI å¼€æºç¤¾åŒºï¼Œå®ƒç›¸å½“äºç¨‹åºå‘˜çš„ Githubã€‚å®ƒèšé›†äº†å¤§é‡ AI æ¨¡å‹ï¼Œå¯¹æˆ‘ä»¬æ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ªå€¼å¾—æŒ–æ˜çš„å®è—ã€‚

è°ƒç”¨ Hugging Face çš„æ¨¡å‹æœ‰ä¸¤ç§æ–¹å¼ï¼Œé«˜å±‚çš„ç®¡é“æ–¹å¼ï¼Œä»¥åŠåº•å±‚é¢å¯¹æ¨¡å‹çš„æ–¹å¼ã€‚ç®¡é“çš„æ–¹å¼æ›´æ˜“ç”¨ï¼Œè€Œåº•å±‚çš„æ–¹å¼åˆ™éœ€è¦å¯¹å¤§æ¨¡å‹çš„å¤„ç†æµç¨‹æœ‰æ‰€äº†è§£ã€‚ç®¡é“åŸºæœ¬ä¸Šå°±æ˜¯å¯¹åº•å±‚å¤„ç†æµç¨‹çš„å°è£…ã€‚

é™¤äº†ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ä¹‹å¤–ï¼ŒHugging Face ä¸Šè¿˜æœ‰å¤§é‡çš„ä¸åŒç”¨é€”çš„æ¨¡å‹ã€‚æˆ‘ä»¬å®Œå…¨å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦é€‰æ‹©ç›¸åº”çš„æ¨¡å‹ã€‚

æœ€åï¼Œæˆ‘ä»¬è¿˜äº†è§£äº†å¦‚ä½•ä½¿ç”¨é«˜å±‚å’Œåº•å±‚æ¥å£å®ç°å¤§æ¨¡å‹çš„æµå¼è¾“å‡ºï¼Œä¸»è¦å°±æ˜¯ä½¿ç”¨äº† Streamer çš„æ¦‚å¿µã€‚

å¦‚æœä»Šå¤©çš„å†…å®¹ä½ åªèƒ½è®°ä½ä¸€ä»¶äº‹ï¼Œé‚£è¯·è®°ä½ï¼Œ **Hugging Face ä¸ºæˆ‘ä»¬æä¾›äº†å¤§é‡å¯ä»¥å®Œæˆå„ç§å·¥ä½œçš„æ¨¡å‹**ã€‚

## æ€è€ƒé¢˜

æ—¢ç„¶å„ä½å·²ç»çŸ¥é“äº† Hugging Faceï¼Œä¸ºäº†ä¸é”™è¿‡å¥½ä¸œè¥¿ï¼Œæˆ‘å»ºè®®ä½ å»çœ‹çœ‹å®ƒä¸Šé¢éƒ½æœ‰ä»€ä¹ˆæ ·çš„æ¨¡å‹ï¼Œæ¬¢è¿åœ¨ç•™è¨€åŒºåˆ†äº«ä½ çš„æ‰€å¾—ã€‚