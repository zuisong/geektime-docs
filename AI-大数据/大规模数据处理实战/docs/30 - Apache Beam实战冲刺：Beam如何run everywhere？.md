ä½ å¥½ï¼Œæˆ‘æ˜¯è”¡å…ƒæ¥ ã€‚

ä»Šå¤©æˆ‘è¦ä¸ä½ åˆ†äº«çš„ä¸»é¢˜æ˜¯â€œApache Beamå®æˆ˜å†²åˆºï¼šBeamå¦‚ä½•run everywhereâ€ã€‚

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œè‡ªç¬¬26è®²åˆ°ç¬¬29è®²ï¼Œä»Pipelineçš„è¾“å…¥è¾“å‡ºï¼Œåˆ°Pipelineçš„è®¾è®¡ï¼Œå†åˆ°Pipelineçš„æµ‹è¯•ï¼ŒBeam Pipelineçš„æ¦‚å¿µä¸€ç›´è´¯ç©¿ç€æ–‡ç« è„‰ç»œã€‚é‚£ä¹ˆè¿™ä¸€è®²ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ä¸€ä¸ªå®Œæ•´çš„Beam Pipelineç©¶ç«Ÿæ˜¯å¦‚ä½•ç¼–å†™çš„ã€‚

## Beam Pipeline

ä¸€ä¸ªPipelineï¼Œæˆ–è€…è¯´æ˜¯ä¸€ä¸ªæ•°æ®å¤„ç†ä»»åŠ¡ï¼ŒåŸºæœ¬ä¸Šéƒ½ä¼šåŒ…å«ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. è¯»å–è¾“å…¥æ•°æ®åˆ°PCollectionã€‚
2. å¯¹è¯»è¿›æ¥çš„PCollectionåšæŸäº›æ“ä½œï¼ˆä¹Ÿå°±æ˜¯Transformï¼‰ï¼Œå¾—åˆ°å¦ä¸€ä¸ªPCollectionã€‚
3. è¾“å‡ºä½ çš„ç»“æœPCollectionã€‚

è¿™ä¹ˆè¯´ï¼Œçœ‹èµ·æ¥å¾ˆç®€å•ï¼Œä½†ä½ å¯èƒ½ä¼šæœ‰äº›è¿·æƒ‘ï¼šè¿™äº›æ­¥éª¤å…·ä½“è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿå…¶å®è¿™äº›æ­¥éª¤å…·ä½“åˆ°Pipelineçš„å®é™…ç¼–ç¨‹ä¸­ï¼Œå°±ä¼šåŒ…å«ä»¥ä¸‹è¿™äº›ä»£ç æ¨¡å—ï¼š

Java

```
// Start by defining the options for the pipeline.
PipelineOptions options = PipelineOptionsFactory.create();

// Then create the pipeline.
Pipeline pipeline = Pipeline.create(options);

PCollection<String> lines = pipeline.apply(
  "ReadLines", TextIO.read().from("gs://some/inputData.txt"));

PCollection<String> filteredLines = lines.apply(new FilterLines());

filteredLines.apply("WriteMyFile", TextIO.write().to("gs://some/outputData.txt"));

pipeline.run().waitUntilFinish();
```

ä»ä¸Šé¢çš„ä»£ç ä¾‹å­ä¸­ä½ å¯ä»¥çœ‹åˆ°ï¼Œç¬¬ä¸€è¡Œå’Œç¬¬äºŒè¡Œä»£ç æ˜¯åˆ›å»ºPipelineå®ä¾‹ã€‚ä»»ä½•ä¸€ä¸ªBeamç¨‹åºéƒ½éœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªPipelineçš„å®ä¾‹ã€‚Pipelineå®ä¾‹å°±æ˜¯ç”¨æ¥è¡¨è¾¾Pipelineç±»å‹çš„å¯¹è±¡ã€‚è¿™é‡Œä½ éœ€è¦æ³¨æ„ï¼Œä¸€ä¸ªäºŒè¿›åˆ¶ç¨‹åºå¯ä»¥åŠ¨æ€åŒ…å«å¤šä¸ªPipelineå®ä¾‹ã€‚

è¿˜æ˜¯ä»¥ä¹‹å‰çš„ç¾å›¢å¤–å–ç”µåŠ¨è½¦å¤„ç†çš„ä¾‹å­æ¥åšè¯´æ˜å§ã€‚

æ¯”å¦‚ï¼Œæˆ‘ä»¬çš„ç¨‹åºå¯ä»¥åŠ¨æ€åˆ¤æ–­æ˜¯å¦å­˜åœ¨ç¬¬ä¸‰æ–¹çš„ç”µåŠ¨è½¦å›¾ç‰‡ï¼Œåªæœ‰å½“æœ‰éœ€è¦å¤„ç†å›¾ç‰‡æ—¶ï¼Œæˆ‘ä»¬æ‰å»åˆ›å»ºä¸€ä¸ªPipelineå®ä¾‹å¤„ç†ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åŠ¨æ€åˆ¤æ–­æ˜¯å¦å­˜åœ¨éœ€è¦è½¬æ¢å›¾ç‰‡æ ¼å¼ï¼Œæœ‰éœ€è¦æ—¶ï¼Œæˆ‘ä»¬å†å»åˆ›å»ºç¬¬äºŒä¸ªPipelineå®ä¾‹ã€‚è¿™æ—¶å€™ä½ çš„äºŒè¿›åˆ¶ç¨‹åºï¼Œå¯èƒ½åŒ…å«0ä¸ªã€1ä¸ªï¼Œæˆ–è€…æ˜¯2ä¸ªPipelineå®ä¾‹ã€‚æ¯ä¸€ä¸ªå®ä¾‹éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå®ƒå°è£…äº†ä½ è¦è¿›è¡Œæ“ä½œçš„æ•°æ®ï¼Œå’Œä½ è¦è¿›è¡Œçš„æ“ä½œTransformã€‚

Pipelineå®ä¾‹çš„åˆ›å»ºæ˜¯ä½¿ç”¨Pipeline.create(options)è¿™ä¸ªæ–¹æ³•ã€‚å…¶ä¸­optionsæ˜¯ä¼ é€’è¿›å»çš„å‚æ•°ï¼Œoptionsæ˜¯ä¸€ä¸ªPipelineOptionsè¿™ä¸ªç±»çš„å®ä¾‹ã€‚æˆ‘ä»¬ä¼šåœ¨ååŠéƒ¨åˆ†å±•å¼€PipelineOptionsçš„ä¸°å¯Œå˜åŒ–ã€‚

ç¬¬ä¸‰è¡Œä»£ç ï¼Œæˆ‘ä»¬ç”¨TextIO.read()è¿™ä¸ªTransformè¯»å–äº†æ¥è‡ªå¤–éƒ¨æ–‡æœ¬æ–‡ä»¶çš„å†…å®¹ï¼ŒæŠŠæ‰€æœ‰çš„è¡Œè¡¨ç¤ºä¸ºä¸€ä¸ªPCollectionã€‚

ç¬¬å››è¡Œä»£ç ï¼Œç”¨ lines.apply(new FilterLines()) å¯¹è¯»è¿›æ¥çš„PCollectionè¿›è¡Œäº†è¿‡æ»¤æ“ä½œã€‚

ç¬¬äº”è¡Œä»£ç  filteredLines.apply(â€œWriteMyFileâ€, TextIO.write().to(â€œgs://some/outputData.txtâ€))ï¼Œè¡¨ç¤ºæŠŠæœ€ç»ˆçš„PCollectionç»“æœè¾“å‡ºåˆ°å¦ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ã€‚

ç¨‹åºè¿è¡Œåˆ°ç¬¬äº”è¡Œçš„æ—¶å€™ï¼Œæ˜¯ä¸æ˜¯æˆ‘ä»¬çš„æ•°æ®å¤„ç†ä»»åŠ¡å°±å®Œæˆäº†å‘¢ï¼Ÿå¹¶ä¸æ˜¯ã€‚

è®°å¾—æˆ‘ä»¬åœ¨ç¬¬24è®²ã€ç¬¬25è®²ä¸­æè¿‡ï¼ŒBeamæ˜¯å»¶è¿Ÿè¿è¡Œçš„ã€‚ç¨‹åºè·‘åˆ°ç¬¬äº”è¡Œçš„æ—¶å€™ï¼Œåªæ˜¯æ„å»ºäº†Beamæ‰€éœ€è¦çš„æ•°æ®å¤„ç†DAGç”¨æ¥ä¼˜åŒ–å’Œåˆ†é…è®¡ç®—èµ„æºï¼ŒçœŸæ­£çš„è¿ç®—å®Œå…¨æ²¡æœ‰å‘ç”Ÿã€‚

æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦æœ€åä¸€è¡Œpipeline.run().waitUntilFinish()ï¼Œè¿™æ‰æ˜¯æ•°æ®çœŸæ­£å¼€å§‹è¢«å¤„ç†çš„è¯­å¥ã€‚

è¿™æ—¶å€™è¿è¡Œæˆ‘ä»¬çš„ä»£ç ï¼Œæ˜¯ä¸æ˜¯å°±å¤§åŠŸå‘Šæˆå‘¢ï¼Ÿåˆ«æ€¥ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å¤„ç†å¥½ç¨‹åºåœ¨å“ªé‡Œè¿è¡Œçš„é—®é¢˜ã€‚ä½ ä¸€å®šä¼šå¥½å¥‡ï¼Œæˆ‘ä»¬çš„ç¨‹åºç©¶ç«Ÿåœ¨å“ªé‡Œè¿è¡Œï¼Œä¸æ˜¯è¯´å¥½äº†åˆ†å¸ƒå¼æ•°æ®å¤„ç†å—ï¼Ÿ

åœ¨ä¸Šä¸€è®²ã€Šå¦‚ä½•æµ‹è¯•Beam Pipelineã€‹ä¸­æˆ‘ä»¬å­¦ä¼šäº†åœ¨å•å…ƒæµ‹è¯•ç¯å¢ƒä¸­è¿è¡ŒBeam Pipelineã€‚å°±å¦‚åŒä¸‹é¢çš„ä»£ç ã€‚å’Œä¸Šæ–‡çš„ä»£ç ç±»ä¼¼ï¼Œæˆ‘ä»¬æŠŠPipeline.create(options)æ›¿æ¢æˆäº†TestPipeline.create()ã€‚

Java

```
Pipeline p = TestPipeline.create();

PCollection<String> input = p.apply(Create.of(WORDS)).setCoder(StringUtf8Coder.of());

PCollection<String> output = input.apply(new CountWords());

PAssert.that(output).containsInAnyOrder(COUNTS_ARRAY);

p.run();
```

TestPipelineæ˜¯Beam Pipelineä¸­ç‰¹æ®Šçš„ä¸€ç§ï¼Œè®©ä½ èƒ½å¤Ÿåœ¨å•æœºä¸Šè¿è¡Œå°è§„æ¨¡çš„æ•°æ®é›†ã€‚ä¹‹å‰æˆ‘ä»¬åœ¨åˆ†æBeamçš„è®¾è®¡ç†å¿µæ—¶æåˆ°è¿‡ï¼ŒBeamæƒ³è¦æŠŠåº”ç”¨å±‚çš„æ•°æ®å¤„ç†ä¸šåŠ¡é€»è¾‘å’Œåº•å±‚çš„è¿ç®—å¼•æ“åˆ†ç¦»å¼€æ¥ã€‚

ç°å¦‚ä»ŠBeamå¯ä»¥åšåˆ°è®©ä½ çš„Pipelineä»£ç æ— éœ€ä¿®æ”¹ï¼Œå°±å¯ä»¥åœ¨æœ¬åœ°ã€Sparkã€Flinkï¼Œæˆ–è€…åœ¨Google Cloud DataFlowä¸Šè¿è¡Œã€‚è¿™äº›éƒ½æ˜¯é€šè¿‡Pipeline.create(options) è¿™è¡Œä»£ç ä¸­ä¼ é€’çš„PipelineOptionså®ç°çš„ã€‚

åœ¨å®æˆ˜ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨åˆ°çš„æ‰€æœ‰optionå…¶å®éƒ½æ˜¯å®ç°äº†PipelineOptionsè¿™ä¸ªæ¥å£ã€‚

ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›å°†æ•°æ®æµæ°´çº¿æ”¾åœ¨Sparkè¿™ä¸ªåº•å±‚æ•°æ®å¼•æ“è¿è¡Œçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¾¿å¯ä»¥ä½¿ç”¨SparkPipelineOptionsã€‚å¦‚æœæˆ‘ä»¬æƒ³æŠŠæ•°æ®æµæ°´çº¿æ”¾åœ¨Flinkä¸Šè¿è¡Œï¼Œå°±å¯ä»¥ä½¿ç”¨FlinkPipelineOptionsã€‚è€Œè¿™äº›éƒ½æ˜¯extendsäº†PipelineOptionsçš„æ¥å£ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

Java

```
options = PipelineOptionsFactory.as(SparkPipelineOptions.class);
Pipeline pipeline = Pipeline.create(options);
```

é€šå¸¸ä¸€ä¸ªPipelineOptionæ˜¯ç”¨PipelineOptionsFactoryè¿™ä¸ªå·¥å‚ç±»æ¥åˆ›å»ºçš„ï¼Œå®ƒæä¾›äº†ä¸¤ä¸ªé™æ€å·¥å‚æ–¹æ³•ç»™æˆ‘ä»¬å»åˆ›å»ºï¼Œåˆ†åˆ«æ˜¯PipelineOptionsFactory.as(Class)å’ŒPipelineOptionsFactory.create()ã€‚åƒä¸Šé¢çš„ç¤ºä¾‹ä»£ç å°±æ˜¯ç”¨PipelineOptionsFactory.as(Class)è¿™ä¸ªé™æ€å·¥å‚æ–¹æ³•æ¥åˆ›å»ºçš„ã€‚

å½“ç„¶äº†ï¼Œæ›´åŠ å¸¸è§çš„åˆ›å»ºæ–¹æ³•æ˜¯ä»å‘½ä»¤è¡Œä¸­è¯»å–å‚æ•°æ¥åˆ›å»ºPipelineOptionï¼Œä½¿ç”¨çš„æ˜¯PipelineOptionsFactory#fromArgs(String\[])è¿™ä¸ªæ–¹æ³•ï¼Œä¾‹å¦‚ï¼š

Java

```
public static void main(String[] args) {
     PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();
     Pipeline p = Pipeline.create(options);
}
```

ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹ä¸åŒçš„è¿è¡Œæ¨¡å¼çš„å…·ä½“ä½¿ç”¨æ–¹æ³•ã€‚

## ç›´æ¥è¿è¡Œæ¨¡å¼

æˆ‘ä»¬å…ˆä»ç›´æ¥è¿è¡Œæ¨¡å¼å¼€å§‹è®²ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨æœ¬åœ°è¿›è¡Œæµ‹è¯•ï¼Œæˆ–è€…è°ƒè¯•æ—¶å€¾å‘ä½¿ç”¨çš„æ¨¡å¼ã€‚åœ¨ç›´æ¥è¿è¡Œæ¨¡å¼çš„æ—¶å€™ï¼ŒBeamä¼šåœ¨å•æœºä¸Šç”¨å¤šçº¿ç¨‹æ¥æ¨¡æ‹Ÿåˆ†å¸ƒå¼çš„å¹¶è¡Œå¤„ç†ã€‚

ä½¿ç”¨Java Beam SDKæ—¶ï¼Œæˆ‘ä»¬è¦ç»™ç¨‹åºæ·»åŠ Direct Runnerçš„ä¾èµ–å…³ç³»ã€‚åœ¨ä¸‹é¢è¿™ä¸ªmavenä¾èµ–å…³ç³»å®šä¹‰æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬æŒ‡å®šäº†beam-runners-direct-javaè¿™æ ·ä¸€ä¸ªä¾èµ–å…³ç³»ã€‚

```
pom.xml
<dependency>
   <groupId>org.apache.beam</groupId>
   <artifactId>beam-runners-direct-java</artifactId>
   <version>2.13.0</version>
   <scope>runtime</scope>
</dependency>
```

ä¸€èˆ¬æˆ‘ä»¬ä¼šæŠŠrunneré€šè¿‡å‘½ä»¤è¡ŒæŒ‡ä»¤ä¼ é€’è¿›ç¨‹åºã€‚å°±éœ€è¦ä½¿ç”¨PipelineOptionsFactory.fromArgs(args)æ¥åˆ›å»ºPipelineOptionsã€‚PipelineOptionsFactory.fromArgs()æ˜¯ä¸€ä¸ªå·¥å‚æ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©ç”Ÿæˆä¸åŒçš„PipelineOptionså­ç±»ã€‚

```
PipelineOptions options =
       PipelineOptionsFactory.fromArgs(args).create();
```

åœ¨å®éªŒç¨‹åºä¸­ä¹Ÿå¯ä»¥å¼ºè¡Œä½¿ç”¨Direct Runnerã€‚æ¯”å¦‚ï¼š

```
PipelineOptions options = PipelineOptionsFactory.create();
options.setRunner(DirectRunner.class);
// æˆ–è€…è¿™æ ·
options = PipelineOptionsFactory.as(DirectRunner.class);
Pipeline pipeline = Pipeline.create(options);
```

å¦‚æœæ˜¯åœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®šRunnerçš„è¯ï¼Œé‚£ä¹ˆåœ¨è°ƒç”¨è¿™ä¸ªç¨‹åºæ—¶å€™ï¼Œéœ€è¦æŒ‡å®šè¿™æ ·ä¸€ä¸ªå‚æ•°â€“runner=DirectRunnerã€‚æ¯”å¦‚ï¼š

```
mvn compile exec:java -Dexec.mainClass=YourMainClass \
     -Dexec.args="--runner=DirectRunner" -Pdirect-runner
```

## Sparkè¿è¡Œæ¨¡å¼

å¦‚æœæˆ‘ä»¬å¸Œæœ›å°†æ•°æ®æµæ°´çº¿æ”¾åœ¨Sparkè¿™ä¸ªåº•å±‚æ•°æ®å¼•æ“è¿è¡Œçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¾¿å¯ä»¥ä½¿ç”¨Spark Runnerã€‚Spark Runneræ‰§è¡ŒBeamç¨‹åºæ—¶ï¼Œèƒ½å¤ŸåƒåŸç”Ÿçš„Sparkç¨‹åºä¸€æ ·ã€‚æ¯”å¦‚ï¼Œåœ¨Sparkæœ¬åœ°æ¨¡å¼éƒ¨ç½²åº”ç”¨ï¼Œè·‘åœ¨Sparkçš„RMä¸Šï¼Œæˆ–è€…ç”¨YARNã€‚

Spark Runnerä¸ºåœ¨Apache Sparkä¸Šè¿è¡ŒBeam Pipelineæä¾›äº†ä»¥ä¸‹åŠŸèƒ½ï¼š

1. Batch å’Œstreamingçš„æ•°æ®æµæ°´çº¿ï¼›
2. å’ŒåŸç”ŸRDDå’ŒDStreamä¸€æ ·çš„å®¹é”™ä¿è¯ï¼›
3. å’ŒåŸç”ŸSparkåŒæ ·çš„å®‰å…¨æ€§èƒ½ï¼›
4. å¯ä»¥ç”¨Sparkçš„æ•°æ®å›æŠ¥ç³»ç»Ÿï¼›
5. ä½¿ç”¨Spark Broadcastå®ç°çš„Beam side-inputã€‚

ç›®å‰ä½¿ç”¨Spark Runnerå¿…é¡»ä½¿ç”¨Spark 2.2ç‰ˆæœ¬ä»¥ä¸Šã€‚

è¿™é‡Œï¼Œæˆ‘ä»¬å…ˆæ·»åŠ beam-runners-sparkçš„ä¾èµ–å…³ç³»ã€‚

```
<dependency>
  <groupId>org.apache.beam</groupId>
  <artifactId>beam-runners-spark</artifactId>
  <version>2.13.0</version>
</dependency>
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.10</artifactId>
  <version>${spark.version}</version>
</dependency>
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-streaming_2.10</artifactId>
  <version>${spark.version}</version>
</dependency>
```

ç„¶åï¼Œè¦ä½¿ç”¨SparkPipelineOptionsä¼ é€’è¿›Pipeline.create()æ–¹æ³•ã€‚å¸¸è§çš„åˆ›å»ºæ–¹æ³•æ˜¯ä»å‘½ä»¤è¡Œä¸­è¯»å–å‚æ•°æ¥åˆ›å»ºPipelineOptionï¼Œä½¿ç”¨çš„æ˜¯PipelineOptionsFactory.fromArgs(String\[])è¿™ä¸ªæ–¹æ³•ã€‚åœ¨å‘½ä»¤è¡Œä¸­ï¼Œä½ éœ€è¦æŒ‡å®šrunner=SparkRunnerï¼š

```
mvn exec:java -Dexec.mainClass=YourMainClass \
    -Pspark-runner \
    -Dexec.args="--runner=SparkRunner \
      --sparkMaster=<spark master url>"
```

ä¹Ÿå¯ä»¥åœ¨Sparkçš„ç‹¬ç«‹é›†ç¾¤ä¸Šè¿è¡Œï¼Œè¿™æ—¶å€™sparkçš„æäº¤å‘½ä»¤ï¼Œspark-submitã€‚

```
spark-submit --class YourMainClass --master spark://HOST:PORT target/...jar --runner=SparkRunner
```

å½“Beamç¨‹åºåœ¨Sparkä¸Šè¿è¡Œæ—¶ï¼Œä½ ä¹Ÿå¯ä»¥åŒæ ·ç”¨Sparkçš„ç½‘é¡µç›‘æ§æ•°æ®æµæ°´çº¿è¿›åº¦ã€‚

## Flinkè¿è¡Œæ¨¡å¼

Flink Runneræ˜¯Beamæä¾›çš„ç”¨æ¥åœ¨Flinkä¸Šè¿è¡ŒBeam Pipelineçš„æ¨¡å¼ã€‚ä½ å¯ä»¥é€‰æ‹©åœ¨è®¡ç®—é›†ç¾¤ä¸Šæ¯”å¦‚ Yarn/Kubernetes/Mesos æˆ–è€…æœ¬åœ°Flinkä¸Šè¿è¡Œã€‚Flink Runneré€‚åˆå¤§è§„æ¨¡ï¼Œè¿ç»­çš„æ•°æ®å¤„ç†ä»»åŠ¡ï¼ŒåŒ…å«äº†ä»¥ä¸‹åŠŸèƒ½ï¼š

1. ä»¥Streamingä¸ºä¸­å¿ƒï¼Œæ”¯æŒstreamingå¤„ç†å’Œbatchå¤„ç†ï¼›
2. å’Œflinkä¸€æ ·çš„å®¹é”™æ€§ï¼Œå’Œexactly-onceçš„å¤„ç†è¯­ä¹‰ï¼›
3. å¯ä»¥è‡ªå®šä¹‰å†…å­˜ç®¡ç†æ¨¡å‹ï¼›
4. å’Œå…¶ä»–ï¼ˆä¾‹å¦‚YARNï¼‰çš„Apache Hadoopç”Ÿæ€æ•´åˆæ¯”è¾ƒå¥½ã€‚

å…¶å®çœ‹åˆ°è¿™é‡Œï¼Œä½ å¯èƒ½å·²ç»æŒæ¡äº†è¿™é‡Œé¢çš„è¯€çªã€‚å°±æ˜¯é€šè¿‡PipelineOptionsæ¥æŒ‡å®šrunnerï¼Œè€Œä½ çš„æ•°æ®å¤„ç†ä»£ç ä¸éœ€è¦ä¿®æ”¹ã€‚PipelineOptionså¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šã€‚é‚£ä¹ˆç±»ä¼¼Spark Runnerï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨Flinkæ¥è¿è¡ŒBeamç¨‹åºã€‚

åŒæ ·çš„ï¼Œé¦–å…ˆä½ éœ€è¦åœ¨pom.xmlä¸­æ·»åŠ Flink Runnerçš„ä¾èµ–ã€‚

```
<dependency>
  <groupId>org.apache.beam</groupId>
  <artifactId>beam-runners-flink-1.6</artifactId>
  <version>2.13.0</version>
</dependency>
```

ç„¶ååœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®šflink runnerï¼š

```
mvn exec:java -Dexec.mainClass=YourMainClass \
    -Pflink-runner \
    -Dexec.args="--runner=FlinkRunner \
      --flinkMaster=<flink master url>"
```

## Google Dataflow è¿è¡Œæ¨¡å¼

Beam Pipelineä¹Ÿèƒ½ç›´æ¥åœ¨äº‘ç«¯è¿è¡Œã€‚Google Cloud Dataflowå°±æ˜¯å®Œå…¨æ‰˜ç®¡çš„Beam Runnerã€‚å½“ä½ ä½¿ç”¨Google Cloud DataflowæœåŠ¡æ¥è¿è¡ŒBeam Pipelineæ—¶ï¼Œå®ƒä¼šå…ˆä¸Šä¼ ä½ çš„äºŒè¿›åˆ¶ç¨‹åºåˆ°Google Cloudï¼Œéšåè‡ªåŠ¨åˆ†é…è®¡ç®—èµ„æºåˆ›å»ºCloud Dataflowä»»åŠ¡ã€‚

åŒå‰é¢è®²åˆ°çš„Direct Runnerå’ŒSpark Runnerç±»ä¼¼ï¼Œä½ è¿˜æ˜¯éœ€è¦ä¸ºCloud Dataflowæ·»åŠ beam-runners-google-cloud-dataflow-javaä¾èµ–å…³ç³»ï¼š

```
<dependency>
  <groupId>org.apache.beam</groupId>
  <artifactId>beam-runners-google-cloud-dataflow-java</artifactId>
  <version>2.13.0</version>
  <scope>runtime</scope>
</dependency>
```

æˆ‘ä»¬å‡è®¾ä½ å·²ç»åœ¨Google Cloudä¸Šåˆ›å»ºäº†projectï¼Œé‚£ä¹ˆå°±å¯ä»¥ç”¨ç±»ä¼¼çš„å‘½ä»¤è¡Œæäº¤ä»»åŠ¡ï¼š

```
mvn -Pdataflow-runner compile exec:java \
      -Dexec.mainClass=<YourMainClass> \
      -Dexec.args="--project=<PROJECT_ID> \
      --stagingLocation=gs://<STORAGE_BUCKET>/staging/ \
      --output=gs://<STORAGE_BUCKET>/output \
      --runner=DataflowRunner"
```

## å°ç»“

è¿™ä¸€è®²æˆ‘ä»¬å…ˆæ€»ç»“äº†å‰é¢å‡ è®²Pipelineçš„å®Œæ•´ä½¿ç”¨æ–¹æ³•ã€‚ä¹‹åä¸€èµ·æ¢ç´¢äº†Beamçš„é‡è¦ç‰¹æ€§ï¼Œå°±æ˜¯Pipelineå¯ä»¥é€šè¿‡PipelineOptionåŠ¨æ€é€‰æ‹©åŒæ ·çš„æ•°æ®å¤„ç†æµæ°´çº¿åœ¨å“ªé‡Œè¿è¡Œã€‚å¹¶ä¸”ï¼Œåˆ†åˆ«å±•å¼€è®²è§£äº†ç›´æ¥è¿è¡Œæ¨¡å¼ã€Sparkè¿è¡Œæ¨¡å¼ã€Flinkè¿è¡Œæ¨¡å¼å’ŒGoogle Cloud Dataflowè¿è¡Œæ¨¡å¼ã€‚åœ¨å®è·µä¸­ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªèº«éœ€è¦ï¼Œå»é€‰æ‹©ä¸åŒçš„è¿è¡Œæ¨¡å¼ã€‚

## æ€è€ƒé¢˜

Beamçš„è®¾è®¡æ¨¡å¼æ˜¯å¯¹è®¡ç®—å¼•æ“åŠ¨æ€é€‰æ‹©ï¼Œå®ƒä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡ï¼Ÿ

æ¬¢è¿ä½ æŠŠç­”æ¡ˆå†™åœ¨ç•™è¨€åŒºï¼Œä¸æˆ‘å’Œå…¶ä»–åŒå­¦ä¸€èµ·è®¨è®ºã€‚å¦‚æœä½ è§‰å¾—æœ‰æ‰€æ”¶è·ï¼Œä¹Ÿæ¬¢è¿æŠŠæ–‡ç« åˆ†äº«ç»™ä½ çš„æœ‹å‹ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ5ï¼‰</strong></div><ul>
<li><span>suncar</span> ğŸ‘ï¼ˆ11ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¯·é—®ä¸€ä¸‹è€å¸ˆï¼Œå¯ä¸å¯æä¾›å‡ ä¸ªè·å–å¤§é‡æµ‹è¯•æ•°æ®çš„ç½‘æ­¢ã€‚è°¢è°¢</p>2019-07-01</li><br/><li><span>æ˜ç¿¼</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ6ï¼‰<p>æƒ³é—®ä¸‹è¯»è€…ä¸­å¤šå°‘äººç”¨beamåœ¨ç”Ÿäº§ç¯å¢ƒâ€¦</p>2019-07-02</li><br/><li><span>hugo</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>runneræ˜¯å¦‚ä½•åœ¨å¤šå¹³å°ï¼Œå¤šè¯­è¨€é—´å®ç°å…¼å®¹çš„ï¼Ÿåƒflinkï¼Œgo runnerä¼šåœ¨æœ¬åœ°è°ƒç”¨java runnerå—</p>2020-10-23</li><br/><li><span>David</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è¯·æ•™ä¸€ä¸‹ï¼ŒGCPä¸ŠåŒæ—¶æœ‰Composer&#47;Airflowå’ŒDataflow&#47;Beamä¸¤ç§å¯ä»¥ç”¨æ¥å®ŒæˆETLå·¥ä½œçš„äº§å“ã€‚
æ˜¯å¦å¯ä»¥è®²ä¸€ä¸‹ä¸¤è€…çš„æ¯”è¾ƒï¼Œå’Œåœ¨æŠ€æœ¯ä¸Šå¦‚ä½•è¿›è¡Œé€‰å‹ï¼Ÿ
è°¢è°¢ï¼</p>2020-03-04</li><br/><li><span>ditiki</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è¯·æ•™ä¸¤ä¸ªproductioné‡åˆ°çš„é—®é¢˜.

In a beam pipeline (dataflow), one step is to send http request to schema registry to validate event schema. A groupby event type before this step and static cache are used to reduce calls to schema registry. How does beam (or the underline runner) optimise IO ? Is it a good practice to use a thread pool for asynchronous http calls ? 

The event object has a Json (json4s library) payload, each time we try to update the Dataflow pipeline, we get the error says that the Kryo coder generated for the JSON has changed, such that the current pipeline canâ€™t be updated in place. We did a work a round by serialise the Json payload to string in a custom coder, which should be very inefficient. Have you ever seen this before ? Does Kryo generate a different coder at each compile time ? 

å¤šè°¢å•¦ï¼</p>2019-07-03</li><br/>
</ul>