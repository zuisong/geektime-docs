你好，我是独行。

今天我们来聊一个非常有趣的话题——Sora，相信你之前也听说过这个震动AI音视频领域的大模型了，今天我们就深入这个模型，看看Sora是如何一步步走到今天的。我们先来看一组官方提供的示例视频。

难以想象，这些是Sora生成的内容，没有任何后期人工制作。看过这些视频后，我第一反应就是，视频领域要被颠覆了，无论是长视频、短视频还是电影。

Sora是OpenAI开发的一种视频生成大模型，利用先进的人工智能和虚拟现实技术，能够根据文本、图片甚至视频输入直接生成2D和3D视频。这不仅改变了视频制作的流程，还会颠覆视频观看的方式。

## Sora对视频领域的冲击

### AI生成剧本

传统电影制作需要庞大的团队，耗时耗力，从编剧、拍摄到后期制作，每一个环节都充满挑战。而Sora的出现改变了这一局面。Sora利用自然语言处理技术，自动生成剧本。早期的AI剧本生成可能显得有些生硬，但随着深度学习和生成对抗网络（GAN）的进步，AI可以生成更自然、更有创意的剧本。这些剧本不仅可以独立存在，还可以根据观众反馈进行动态调整。

### 低成本的制作模式

Sora不仅改变了电影制作和观影方式，还将对整个电影市场产生深远的影响。由于AI和VR技术降低了制作成本，同时提升了影片质量，独立制片人和小型电影公司将有更多的机会进入市场，打破了传统电影巨头的垄断格局。早期，电影制作需要昂贵的设备和大量人力，而现在，借助开源工具和云计算平台，任何人都可以用相对低廉的成本制作出高质量的电影。

### 实时场景构建与预览

Sora利用虚拟现实（VR）和增强现实（AR）技术，将视频制作的虚拟场景变得更加真实并具有互动感。早期的电影特效依赖于复杂的后期处理，但现在借助实时渲染和图形处理单元的强大计算能力，视频创作者可以在制作过程中实时预览虚拟场景。这些技术最初在游戏开发中得到了广泛应用，而现在，视频行业也将充分受益于此。

当然，除了基础制作方面，未来Sora可能还会有很多高级的玩法，比如，利用面部识别和情感分析技术，实时捕捉观众的情绪变化，从而动态调整电影的节奏和内容，如果能做到这个，那就非常厉害了！甚至有人还说，可以根据不同的人的反馈，对于同一个视频，生成不同的效果，想象空间非常大。

## 文生视频技术难点

**目前文生视频最大的挑战在于在不同场景中保持一致性和可重复性。** 因为在单独生成每个场景或帧时，要完全理解前一个场景的上下文和细节，并将它们适当地继承到下一个场景中，是非常困难的。然而，Sora通过对语言的深刻理解与视觉上下文相结合，并准确理解提示，成功地保持了叙事的一致性。它还能从给定的提示中捕捉人物的情绪和个性，并在视频中将其描绘成富有表现力的角色。

Sora能够理解提示中描述的元素在物理世界中的存在和运作方式，使模型能准确地在视频中表现用户想要的动作和行为。例如，它可以逼真地重现一个人奔跑的景象或自然现象的运动。此外，它还能精准地重现多个角色的细节、运动类型以及主体和背景的细节。

虽然Sora不是第一个人工智能视频模型，但它是第一个表现出如此高水平一致性、持续时间和照片真实感的模型。那么，Sora到底是如何做到如此强大的呢？

## Sora技术演进之路

接下来我们就来看看Sora是如何发展到现在这种程度的，主要是Vision Transformer和扩散模型发挥的作用，我汇总成了一张示意图，你先大概了解一下，下面我们细细讲解。

![](https://static001.geekbang.org/resource/image/87/f6/87ac2775b1539cd0584575d4392d11f6.png?wh=4142x1822)

### 视觉转换器（ViT）

Transformer架构彻底改变了注意力机制的实现，并在自然语言处理领域取得了非常显著的成果，人们预见到它在计算机视觉领域的应用也只是时间问题。最终，这个目标在2020年得以实现，Dosovitskiy等人发表了论文 [《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》](https://arxiv.org/pdf/2010.11929)（一张图片胜过16x16个单词：用于大规模图像识别的Transformers），实现了Vision Transformer (ViT)。

在 Transformer 的原始论文中，token 主要表示单词或句子的部分，分析这些 token 之间的关系可以深入了解句子的含义。在这项研究中，为了将 token 的概念应用于视觉数据，将图像分成 16x16 的小块（patch），每个 patch 在 Transformer 中被视为一个“token”。这种方法使模型能够学习每个 patch 在整幅图像中的关系，从而在此基础上识别和理解整幅图像。 **它突破了传统 CNN 模型在图像识别中固定接受域大小的限制，可以灵活地捕捉图像中的任意位置关系。**

![图片](https://static001.geekbang.org/resource/image/2f/9e/2f430ce39eee3bf72040be0be157ed9e.png?wh=720x367)

### 视频视觉转换器（ViViT）

2021年，Arnab等人在他们的论文 [《ViViT: A Video Vision Transformer》](https://arxiv.org/pdf/2103.15691)（ViViT: 视频视觉转换器）中进一步 **扩展了Vision Transformer的概念，将其应用于视频的多维数据**。视频数据更加复杂，因为它既包含静态图像信息，比如空间元素，也包含随时间变化的动态信息，比如时间元素。ViViT通过将视频分解为 **时空块**，并将其视为Transformer模型中的标记，成功实现了这一点。通过引入时空块，ViViT能够同时捕捉视频中的静态和动态元素，并模拟它们之间的复杂关系。

![图片](https://static001.geekbang.org/resource/image/4b/c1/4b84e16225d109ba8352c7536558a6c1.png?wh=720x413)

### 掩蔽自动编码器（MAE）

2022年，He等人在他们的研究 [《Masked Autoencoders Are Scalable Vision Learners》](https://arxiv.org/pdf/2111.06377)（蒙蔽自动编码器是可扩展的视觉学习器）中，提出了一种名为“Masked Autoencoder”的自监督预训练方法。这个方法显著改善了传统方法在处理高计算成本和高维、海量信息大数据集时的低效率问题。

具体来说， **它通过对输入图像进行部分掩蔽，训练网络预测隐藏部分的信息，从而更高效地学习图像中的重要特征和结构，并获得丰富的视觉数据表征。** 这个过程不仅使数据的压缩和表征学习更加高效，还降低了计算成本，增强了对不同类型视觉数据和任务的通用性。

这种方法与 BERT 的演化有密切关联。BERT通过掩码语言模型（MLM）实现了对文本数据的深度上下文理解，而He等人将类似的 **掩码技术** 应用于视觉数据，实现了对图像更深入的理解和表示。

![图片](https://static001.geekbang.org/resource/image/1e/59/1e5e07dee741ec6934130634d8147459.png?wh=720x415)

### 原始分辨率视觉变换器（NaViT）

2023年，Dehghani等人在他们的研究 [《Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution》](https://arxiv.org/pdf/2307.06304)（Patch n’Pack: NaViT，适用于任何长宽比和分辨率的视觉转换器）中，提出了原生分辨率ViTransformer（NaViT）。 **这个模型旨在进一步扩展视觉转换器（ViT）对任何长宽比或分辨率图像的适用性。**

#### **传统ViT的挑战**

Vision Transformer引入了一种突破性的方法，将图像划分为固定大小的块，并将这些块视为标记，从而将Transformer模型应用于图像识别任务。然而，这种方法假设模型针对特定分辨率或长宽比进行了优化，需要针对不同大小或形状的图像重新调整模型。这是一个重大限制，因为现实世界中的应用通常需要处理各种大小和长宽比的图像。

#### **NaViT的创新**

**NaViT旨在高效处理任何长宽比或分辨率的图像，无需事先调整即可直接将其输入模型**。Sora也将这种灵活性应用于视频，能够无缝处理各种尺寸和形状的视频和图像，从而显著提高了灵活性和适应性。

![图片](https://static001.geekbang.org/resource/image/c2/c0/c2e71b8cda6eb491969727bd615867c0.png?wh=720x525)

### 扩散模型

2015年，Sohl-Dickstein等人在他们的研究 [《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》](https://arxiv.org/pdf/1503.03585)（利用非平衡热力学进行深度无监督学习）中，提出了扩散模型的理论基础。 **扩散模型与Transformer一起，构成了Sora的支柱技术。** 这种模型使用非平衡热力学进行深度学习，通过引入扩散过程，从随机噪声（没有任何模式的数据）开始，逐渐消除这种噪声，最终创建出类似于实际图像或视频的数据。

举例来说，你可以想象从简单的随机点开始，逐渐转变为美丽风景或人物的视频。这种方法后来被应用于生成图像和声音等复杂数据，有助于开发高质量的生成模型。

![图片](https://static001.geekbang.org/resource/image/9f/b1/9f115125d1cb844a01ba4b5c37ec9ab1.png?wh=720x164)

2020 年 Ho 等人在其论文 [《Denoising Diffusion Probabilistic Models》](https://arxiv.org/abs/2006.11239)（去噪扩散概率模型）中，以及 Nichol 和 Dhariwal 在其 2021 年的论文《 [Improved Denoising Diffusion Probabilistic Models》](https://arxiv.org/abs/2102.09672)（改进的去噪扩散概率模型）中，基于 Sohl-Dickstein 等人（2015 年）的理论框架，开发了一种实用的数据生成模型，即 **去噪扩散概率模型 (DDPM)**。该模型在高质量图像生成方面表现出显著的效果，证明了扩散模型的有效性。

那么，扩散模型对 Sora 有什么影响呢？

通常，训练机器学习模型需要大量标记数据（例如，被告知“这是一张猫的图像”）。然而，扩散模型也可以从未标记的数据中学习，这使得它们能够利用互联网上大量的视觉内容来生成各种类型的视频。换句话说， **Sora 可以观察不同的视频和图像并学习**。

### 潜在扩散模型

2022年，Rombach等人在他们的论文 [《High-resolution image synthesis with latent diffusion models》](https://arxiv.org/abs/2112.10752)（利用潜在扩散模型进行高分辨率图像合成）中，提出了一种方法，与直接生成高分辨率图像相比，该方法在保持质量的同时，利用潜在空间中的扩散模型，大大降低了计算成本。换句话说，它表明，通过对潜在空间中表示的数据进行编码和引入扩散过程，可以用 **更少的计算资源** 实现这一目标，而不是直接处理图像。

Sora 将此技术运用到视频数据中，将视频的时间+空间数据压缩到低维的潜在空间中，再进行时空块的分解， **这种高效的潜在空间数据处理和生成能力，对于 Sora 能够更快速地生成更高质量的视觉内容起到了至关重要的作用。** 这项研究为利用扩散模型进行高分辨率图像合成领域做出了非常大的贡献。

### 扩散Transformer（DiT）

2023年，Peebles 和 Xie在他们的论文 [《Scalable diffusion models with transformers》](https://arxiv.org/abs/2212.09748)（带有变换器的可扩展扩散模型）中提出了实现 Sora 的关键。正如 OpenAI 发布的技术报告中所述，Sora 使用的不是普通Transformer，而是扩散Transformer（DiT）。该结构通过 Transformer 对潜在块的操作实现了潜在扩散模型。这种方法可以更有效地处理图像块，从而能够 **在有效利用计算资源的同时生成高质量的图像**。与 Stability AI 在 2022 年宣布的稳定扩散不同，加入这种Transformer 被认为有助于更自然的视频生成。

## Sora核心技术

根据官方的 [技术报告](https://openai.com/index/video-generation-models-as-world-simulators/)， **Sora使用 Transformer 结构取代了扩散模型中常用的 U-net 组件**。有人总结了一个架构图，你可以参考一下。

![图片](https://static001.geekbang.org/resource/image/ba/d6/bab6d63cb60eb95956a21887e3bb91d6.png?wh=720x342)

里面有几个核心技术点。

- **视频压缩网络**

OpenAI训练了一个降低视觉数据维度的网络，这个网络接受原始视频作为输入，然后进行视频压缩，也就是降低数据维度，最后输出的是 **在时间和空间上压缩过的表示形式**。压缩并不意味着忽略原始数据的特性，而是将它们转化为一种对Sora来说更小、更容易理解和操作的格式。

- **空间时间补丁**

接下来，Sora 将这些压缩后的数据进一步分解为“空间时间补丁”（Spacetime Patches），这些补丁可以看作是视觉内容的基本构建块，例如照片可以分解为包含独特景观、颜色和纹理的小片段。这样不管原始视频的长度、分辨率或风格如何，Sora 都可以将它们处理成一致的格式。

![图片](https://static001.geekbang.org/resource/image/78/b1/785yyb28547181eaacd691975ed40fb1.png?wh=1080x201)

- **Diffusion Transformer模型**

Sora的核心是刚刚提到的，一个基于Transformer模型改造过的Diffusion图像扩散模型，像Midjourney和Stable Diffusion等图像处理产品就是基于Diffusion扩散模型的。扩散模型生成图像的基本流程，不是我们想象的那样，从空白画面开始逐渐绘制图像，而是从一张随机生成的填满了噪点的图像开始，逐渐去除噪点来还原图像。类似下面这样的过程：

![图片](https://static001.geekbang.org/resource/image/dc/4b/dc02df280b2652a5541fbe5569c62e4b.png?wh=1080x276)

- **语言理解与提示**

OpenAI发现训练文本到视频生成系统需要大量带有相应文本标题的视频。这里，OpenAI将DALL·E 3中介绍的标题生成技术用到了视频领域，训练了一个具备高度描述性的视频标题生成模型，使用这个模型为所有的视频训练数据生成了高质量文本标题，再将视频和高质量标题作为视频-文本对进行训练。

通过这样的高质量的训练数据，保障了文本和视频数据之间高度的对齐。而在生成阶段，Sora会基于OpenAI的GPT模型对于用户的prompt进行改写，生成高质量且具备很好描述性的高质量prompt，再送到视频生成模型完成生成工作。

## 小结

从Sora模型的技术报告中，我们可以看到Sora模型的实现是建立在社区及OpenAI一系列坚实的历史技术工作的沉淀基础上的：包括但不限于Clip、Video Caption等。正如社区的一位开发者说，虽然其中依然有非常多的技术细节OpenAI并没有披露，但是OpenAI画了一条“模糊”的路，有了这条模糊的路，大家就可以去尝试，从而画出通往视频生成的正确的清晰的路。

Sora也被称为世界模拟器，也有人说，Sora最先颠覆的不是视频和电影，而是游戏，我觉得这些观点都有道理，虽然现在Sora仅支持制作1分钟长度的视频，但是我相信很快这个限制就会被突破。

对于我们普通的开发人员，基本上使用Sora问题不大，和一般的大模型Prompt没有区别，但是目前类似Sora这类视频大模型基本还不成熟，包括Sora，官方仅仅是公布了一些示例视频和技术细节，可以生成1分钟长度的视频，实际上也没有放出来给公众体验。其他一些类似产品，只能生成几秒的视频，说白了也不具备很高的实用价值，我们可以再等等看。

从技术角度讲，如果你想详细体验或者了解视频生成大模型原理，可以看看开源的大模型：Open Sora，是北京大学与兔展智能联合发起的Sora复现计划，旨在集结开源社区力量完成对Sora的复现，目标是让所有人都能轻松制作高效视频。

6月17日，Open Sora发布了最新的1.2版本，引入了和Sora类似的视频压缩技术，感兴趣的话你可以部署研究，推理需要的GPU比文本生成大模型更高，我们看一下官方给出的数据：在 80G H100 GPU 上，生成速度和峰值内存使用量是这样的：

![图片](https://static001.geekbang.org/resource/image/59/a4/59fc55yya83fe429c426930bf3555ca4.png?wh=1402x330)

个人如果要试玩儿的话，起码需要准备40G的显存。

## 思考题

你可以思考一下，为什么Sora最多只能生成1分钟长度的视频？其实1分钟已经算非常长的了，很多相关产品，只能生产1～2秒，长一点的能到10～15秒，请思考一下这个问题，欢迎把想法打在评论区，我们一起讨论，如果你觉得这节课的内容对你有帮助的话，也欢迎你分享给其他朋友，我们下节课再见！