ä½ å¥½ï¼Œæˆ‘æ˜¯ç‹¬è¡Œã€‚

ä¸ŠèŠ‚è¯¾æˆ‘ä»¬ä¸€èµ·å­¦ä¹ äº†Word2Vecï¼ŒWord2Vecçš„ä¸»è¦èƒ½åŠ›æ˜¯æŠŠè¯æ±‡æ”¾åœ¨å¤šç»´çš„ç©ºé—´é‡Œï¼Œç›¸ä¼¼çš„è¯æ±‡ä¼šè¢«æ”¾åœ¨é‚»è¿‘çš„ä½ç½®ã€‚è¿™èŠ‚è¯¾æˆ‘ä»¬å°†è¿›å…¥Seq2Seqçš„é¢†åŸŸï¼Œäº†è§£è¿™ç§æ›´ä¸ºå¤æ‚ä¸”åŠŸèƒ½å¼ºå¤§çš„æ¨¡å‹ï¼Œå®ƒä¸ä»…èƒ½ç†è§£è¯æ±‡ï¼Œè¿˜èƒ½æŠŠè¿™äº›è¯æ±‡ä¸²è”æˆå®Œæ•´çš„å¥å­ã€‚

## Seq2Seq

Seq2Seqï¼ˆSequence-to-Sequenceï¼‰ï¼Œé¡¾åæ€ä¹‰æ˜¯**ä»ä¸€ä¸ªåºåˆ—åˆ°å¦ä¸€ä¸ªåºåˆ—çš„è½¬æ¢**ã€‚å®ƒä¸ä»…ä»…èƒ½ç†è§£å•è¯ä¹‹é—´çš„å…³ç³»ï¼Œè€Œä¸”è¿˜èƒ½æŠŠæ•´ä¸ªå¥å­çš„æ„æ€æ‰“åŒ…ï¼Œå¹¶è§£å‹æˆå¦ä¸€ç§å½¢å¼çš„è¡¨è¾¾ã€‚å¦‚æœè¯´Word2Vecæ˜¯è®©æˆ‘ä»¬çš„æœºå™¨å­¦ä¼šäº†ç†è§£è¯æ±‡çš„è¯ï¼Œé‚£Seq2Seqåˆ™æ˜¯æ•™ä¼šäº†æœºå™¨å¦‚ä½•ç†è§£å¥å­å¹¶è¿›è¡Œç›¸åº”åœ°è½¬åŒ–ã€‚

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šé‡åˆ°ä¸¤ä¸ªæ ¸å¿ƒçš„è§’è‰²ï¼š**ç¼–ç å™¨**ï¼ˆEncoderï¼‰å’Œ**è§£ç å™¨**ï¼ˆDecoderï¼‰ã€‚ç¼–ç å™¨çš„ä»»åŠ¡æ˜¯ç†è§£å’Œå‹ç¼©ä¿¡æ¯ï¼Œå°±åƒæ˜¯æŠŠä¸€å°é•¿ä¿¡å‡½æ•´ç†æˆä¸€ä¸ªç²¾ç®€çš„æ‘˜è¦ï¼›è€Œè§£ç å™¨åˆ™éœ€è¦å°†è¿™ä¸ªæ‘˜è¦å±•å¼€ï¼Œç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€æˆ–å½¢å¼çš„å®Œæ•´ä¿¡æ¯ã€‚è¿™ä¸ªè¿‡ç¨‹æœ‰ä¸€å®šçš„æŒ‘æˆ˜ï¼Œæ¯”å¦‚å¦‚ä½•ç¡®ä¿ä¿¡æ¯åœ¨è¿™æ¬¡è½¬æ¢ä¸­ä¸ä¸¢å¤±ç²¾é«“ï¼Œè€Œæ˜¯ä»¥æ–°çš„é¢è²Œç²¾å‡†åœ°å‘ˆç°å‡ºæ¥ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ¥ä¸‹æ¥è¦æ¢ç´¢çš„å†…å®¹ä¹‹ä¸€ã€‚

## åŸºæœ¬æ¦‚å¿µ

Seq2Seqä¹Ÿæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ¨¡å‹çš„æ ¸å¿ƒç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ã€‚ä½ å¯ä»¥çœ‹ä¸€ä¸‹è¿™ä¸ªæ¶æ„çš„ç¤ºæ„å›¾ã€‚

![](https://static001.geekbang.org/resource/image/1b/73/1b8d0b11f635f071ab7930a493054973.png?wh=2042x648)

#### ç¼–ç å™¨

ç¼–ç å™¨çš„ä»»åŠ¡æ˜¯**è¯»å–å¹¶ç†è§£è¾“å…¥åºåˆ—ï¼Œç„¶åæŠŠå®ƒè½¬æ¢ä¸ºä¸€ä¸ªå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡**ï¼Œä¹Ÿå«ä½œçŠ¶æ€å‘é‡ã€‚è¿™ä¸ªå‘é‡æ˜¯è¾“å…¥åºåˆ—çš„ä¸€ç§å†…éƒ¨è¡¨ç¤ºï¼Œæ•æ‰äº†åºåˆ—çš„å…³é”®ä¿¡æ¯ã€‚ç¼–ç å™¨é€šå¸¸æ˜¯ä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æˆ–å…¶å˜ä½“ï¼Œæ¯”å¦‚é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰æˆ–é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ï¼Œå®ƒä»¬èƒ½å¤Ÿå¤„ç†ä¸åŒé•¿åº¦çš„è¾“å…¥åºåˆ—ï¼Œå¹¶ä¸”è®°ä½åºåˆ—ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚

#### è§£ç å™¨

è§£ç å™¨çš„ä»»åŠ¡æ˜¯**æ¥æ”¶ç¼–ç å™¨ç”Ÿæˆçš„ä¸Šä¸‹æ–‡å‘é‡ï¼Œå¹¶åŸºäºè¿™ä¸ªå‘é‡ç”Ÿæˆç›®æ ‡åºåˆ—**ã€‚è§£ç è¿‡ç¨‹æ˜¯ä¸€æ­¥æ­¥è¿›è¡Œçš„ï¼Œæ¯ä¸€æ­¥ç”Ÿæˆç›®æ ‡åºåˆ—ä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼Œæ¯”å¦‚ä¸€ä¸ªè¯æˆ–å­—ç¬¦ï¼Œç›´åˆ°ç”Ÿæˆç‰¹æ®Šçš„ç»“æŸç¬¦å·ï¼Œè¡¨ç¤ºè¾“å‡ºåºåˆ—çš„ç»“æŸã€‚è§£ç å™¨é€šå¸¸ä¹Ÿæ˜¯ä¸€ä¸ªRNNã€LSTMæˆ–GRUï¼Œå®ƒä¸ä»…ä¾èµ–äºç¼–ç å™¨çš„ä¸Šä¸‹æ–‡å‘é‡ï¼Œè¿˜å¯èƒ½ä¾èµ–äºè‡ªå·±ä¹‹å‰çš„è¾“å‡ºï¼Œæ¥ç”Ÿæˆä¸‹ä¸€ä¸ªè¾“å‡ºå…ƒç´ ã€‚

#### æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¯é€‰ï¼‰

åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´ï¼Œå¯èƒ½è¿˜ä¼šæœ‰ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰ã€‚æ³¨æ„åŠ›æœºåˆ¶ä½¿è§£ç å™¨èƒ½å¤Ÿåœ¨ç”Ÿæˆæ¯ä¸ªè¾“å‡ºå…ƒç´ æ—¶â€œå…³æ³¨â€è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œæé«˜æ¨¡å‹å¤„ç†é•¿åºåˆ—å’Œæ•æ‰å¤æ‚ä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚ç¼–ç å™¨ã€è§£ç å™¨ã€æ³¨æ„åŠ›æœºåˆ¶ä¹‹é—´æ˜¯æ€æ ·åä½œçš„å‘¢ï¼Ÿä½ å¯ä»¥çœ‹ä¸€ä¸‹æˆ‘ç»™å‡ºçš„ç¤ºæ„å›¾ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/ef/3b/ef925bd2yyec5f51836262527e5fa03b.gif?wh=800x407)

ä¸‹é¢æˆ‘é€šè¿‡ä¸€ä¸ªç¿»è¯‘çš„ä¾‹å­ï¼Œæ¥è¯´æ˜Seq2Seqçš„å·¥ä½œåŸç†ã€‚

## å·¥ä½œåŸç†

æˆ‘ä»¬å…ˆä»æ¨¡å‹çš„è®­ç»ƒå¼€å§‹ï¼ŒSeq2Seqçš„è®­ç»ƒå’ŒWord2Vecä¸å¤ªä¸€æ ·ï¼Œå› ä¸ºæˆ‘ä»¬è®²è§£çš„æ˜¯ä¸­è‹±æ–‡ç¿»è¯‘åœºæ™¯ï¼Œæ‰€ä»¥è®­ç»ƒçš„æ—¶å€™ï¼Œè®­ç»ƒæ•°æ®æ˜¯ä¸­è‹±æ–‡æ•°æ®å¯¹ã€‚Seq2Seqçš„è®­ç»ƒä¼šæ¯”Word2Vecæ›´åŠ å¤æ‚ä¸€äº›ã€‚ä¸ŠèŠ‚è¯¾çš„Word2Vecï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯gensimåº“æä¾›çš„åŸºç¡€æ¨¡å‹ï¼Œç›´æ¥è¿›è¡Œè®­ç»ƒï¼Œè¿™èŠ‚è¯¾æˆ‘ä»¬å®Œå…¨ä»å¤´å†™èµ·ï¼Œè®­ç»ƒä¸€ä¸ªSeq2Seqæ¨¡å‹ã€‚

#### æ¨¡å‹è®­ç»ƒ

æˆ‘ä»¬å…ˆå‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥åœ¨ç½‘ä¸Šæ‰¾å…¬å¼€çš„ç¿»è¯‘æ•°æ®é›†ï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯ [AIchallenger 2017](https://pan.baidu.com/s/113_kXXdekw5IxtinuxBGug?pwd=qvpn)ï¼Œè¿™ä¸ªæ•°æ®é›†æœ‰1000ä¸‡å¯¹ä¸­è‹±æ–‡æ•°æ®ï¼Œä¸è¿‡å› ä¸ºç”µè„‘é…ç½®é—®é¢˜ï¼Œæˆ‘ç›´æ¥ä»é‡Œé¢ä¸­æ–‡å’Œè‹±æ–‡çš„éƒ¨åˆ†å„å–äº†10000æ¡è¿›è¡Œè®­ç»ƒã€‚æ•°æ®é›†åç§°æ˜¯train\_1w.zhå’Œtrain\_1w.enã€‚

```python
cn_sentences = []
zh_file_path = "train_1w.zh"
# ä½¿ç”¨Pythonçš„æ–‡ä»¶æ“ä½œé€è¡Œè¯»å–æ–‡ä»¶ï¼Œå¹¶å°†æ¯ä¸€è¡Œçš„å†…å®¹æ·»åŠ åˆ°åˆ—è¡¨ä¸­
with open(zh_file_path, "r", encoding="utf-8") as file:
    for line in file:
        # å»é™¤è¡Œæœ«çš„æ¢è¡Œç¬¦å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        cn_sentences.append(line.strip())

en_sentences = []
en_file_path = "train_1w.en"
# ä½¿ç”¨Pythonçš„æ–‡ä»¶æ“ä½œé€è¡Œè¯»å–æ–‡ä»¶ï¼Œå¹¶å°†æ¯ä¸€è¡Œçš„å†…å®¹æ·»åŠ åˆ°åˆ—è¡¨ä¸­
with open(en_file_path, "r", encoding="utf-8") as file:
    for line in file:
        # å»é™¤è¡Œæœ«çš„æ¢è¡Œç¬¦å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        en_sentences.append(line.strip())
```

æ¥ä¸‹æ¥ï¼ŒåŸºäºè®­ç»ƒæ•°æ®é›†æ„å»ºä¸­æ–‡å’Œè‹±æ–‡çš„è¯æ±‡è¡¨ï¼Œå°†æ¯ä¸ªè¯æ˜ å°„åˆ°ä¸€ä¸ªå”¯ä¸€çš„ç´¢å¼•ï¼ˆintegerï¼‰ã€‚

```python
# cn_sentences å’Œ en_sentences åˆ†åˆ«åŒ…å«äº†æ‰€æœ‰çš„ä¸­æ–‡å’Œè‹±æ–‡å¥å­
cn_vocab = build_vocab(cn_sentences, tokenize_cn, max_size=10000, min_freq=2)
en_vocab = build_vocab(en_sentences, tokenize_en, max_size=10000, min_freq=2)
```

æˆ‘ä»¬å†æ¥çœ‹ biild\_vocabçš„æºç ã€‚

```python
def build_vocab(sentences, tokenizer, max_size, min_freq):
    token_freqs = Counter()
    for sentence in sentences:
        tokens = tokenizer(sentence)
        token_freqs.update(tokens)
    vocab = {token: idx + 4 for idx, (token, freq) in enumerate(token_freqs.items()) if freq >= min_freq}
    vocab['<unk>'] = 0
    vocab['<pad>'] = 1
    vocab['<sos>'] = 2
    vocab['<eos>'] = 3
    return vocab

```

æ€è·¯å°±æ˜¯æŠŠæ‰€æœ‰çš„å¥å­è¯»è¿›å»ï¼Œå¾ªç¯åˆ†è¯ï¼Œæ”¾å…¥å­—å…¸ï¼Œæ”¾çš„æ—¶å€™è¦åˆ¤æ–­ä¸€ä¸‹æ˜¯å¦å¤§äºç­‰äºmin\_freqï¼Œç”¨æ¥è¿‡æ»¤æ‰å‡ºç°é¢‘ç‡è¾ƒä½çš„è¯æ±‡ï¼Œæœ€åæ„å»ºå‡ºæ¥çš„è¯æ±‡è¡¨å¦‚ä¸‹ï¼š

```python
vocab = {
Â  Â  '<unk>': 0,
Â  Â  '<pad>': 1,
Â  Â  '<sos>': 2,
Â  Â  '<eos>': 3,
Â  Â  'i': 4,
Â  Â  'like': 5,
Â  Â  'learning': 6,
Â  Â  'machine': 7,
Â  Â  'is': 8,
Â  Â  'very': 9,
Â  Â  'interesting': 10,
Â  Â  ...
}

```

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹é‡Œé¢æ¯”è¾ƒé‡è¦çš„å‡ ä¸ªéƒ¨åˆ†ã€‚

- `<unk>`ï¼šæœªçŸ¥å•è¯ï¼Œè¡¨ç¤ºåœ¨è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„å•è¯ã€‚å½“æ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ–‡æœ¬æ—¶é‡åˆ°æœªçŸ¥å•è¯æ—¶ï¼Œä¼šç”¨è¿™ä¸ªæ ‡è®°æ¥è¡¨ç¤ºã€‚
- `<pad>`ï¼šå¡«å……å•è¯ï¼Œç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ã€‚åœ¨å¤„ç†æ‰¹æ¬¡æ•°æ®æ—¶ï¼Œç”±äºä¸åŒåºåˆ—çš„é•¿åº¦å¯èƒ½ä¸åŒï¼Œå› æ­¤éœ€è¦ç”¨è¿™ä¸ªæ ‡è®°æŠŠçŸ­åºåˆ—å¡«å……åˆ°ä¸æœ€é•¿åºåˆ—ç›¸åŒçš„é•¿åº¦ï¼Œä»¥ä¾¿è¿›è¡Œæ‰¹æ¬¡å¤„ç†ã€‚
- `<sos>`ï¼šå¥å­èµ·å§‹æ ‡è®°ï¼Œè¡¨ç¤ºå¥å­çš„å¼€å§‹ä½ç½®ã€‚åœ¨Seq2Seqæ¨¡å‹ä¸­ï¼Œé€šå¸¸ä¼šåœ¨ç›®æ ‡å¥å­çš„å¼€å¤´æ·»åŠ è¿™ä¸ªæ ‡è®°ï¼Œä»¥æŒ‡ç¤ºè§£ç å™¨å¼€å§‹ç”Ÿæˆè¾“å‡ºã€‚
- `<eos>`ï¼šå¥å­ç»“æŸæ ‡è®°ï¼Œè¡¨ç¤ºå¥å­çš„ç»“æŸä½ç½®ã€‚åœ¨Seq2Seqæ¨¡å‹ä¸­ï¼Œé€šå¸¸ä¼šåœ¨ç›®æ ‡å¥å­çš„æœ«å°¾æ·»åŠ è¯¥æ ‡è®°ï¼Œä»¥æŒ‡ç¤ºè§£ç å™¨ç”Ÿæˆç»“æŸã€‚

åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼Œå°†æ•°æ®å¤„ç†æˆæ–¹ä¾¿è®­ç»ƒçš„æ ¼å¼ï¼šè¯­è¨€åºåˆ—ï¼Œæ¯”å¦‚ `[1,2,3,4]`ã€‚

```python
dataset = TranslationDataset(cn_sentences, en_sentences, cn_vocab, en_vocab, tokenize_cn, tokenize_en)
train_loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
```

ç„¶åæ£€æµ‹æ˜¯å¦æœ‰æ˜¾å¡ï¼š

```plain
# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPUï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("è®­ç»ƒè®¾å¤‡ä¸ºï¼š", device)
```

åˆ›å»ºæ¨¡å‹ï¼Œå‚æ•°çš„è§£é‡Šå¯ä»¥å‚è€ƒä»£ç æ³¨é‡Šã€‚

```python

# å®šä¹‰ä¸€äº›è¶…å‚æ•°
INPUT_DIM = 10000  # è¾“å…¥è¯­è¨€çš„è¯æ±‡é‡
OUTPUT_DIM = 10000  # è¾“å‡ºè¯­è¨€çš„è¯æ±‡é‡
ENC_EMB_DIM = 256  # ç¼–ç å™¨åµŒå…¥å±‚å¤§å°ï¼Œä¹Ÿå°±æ˜¯ç¼–ç å™¨è¯å‘é‡ç»´åº¦
DEC_EMB_DIM = 256  # è§£ç å™¨åµŒå…¥å±‚å¤§å°ï¼Œè§£ç å™¨è¯å‘é‡ç»´åº¦
HID_DIM = 512  # éšè—å±‚ç»´åº¦
N_LAYERS = 2  # RNNå±‚çš„æ•°é‡
ENC_DROPOUT = 0.5  # ç¼–ç å™¨ç¥ç»å…ƒè¾“å‡ºçš„æ•°æ®æœ‰50%ä¼šè¢«éšæœºä¸¢æ‰
DEC_DROPOUT = 0.5  # è§£ç å™¨åŒä¸Š

enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

model = Seq2Seq(enc, dec, device).to(device)
# å‡å®šæ¨¡å‹å·²ç»è¢«å®ä¾‹åŒ–å¹¶ç§»åˆ°äº†æ­£ç¡®çš„è®¾å¤‡ä¸Š
model.to(device)
# å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss(ignore_index=en_vocab['<pad>'])  # å¿½ç•¥<pad>æ ‡è®°çš„æŸå¤±
```

å¼€å§‹è®­ç»ƒï¼š

```python
num_epochs = 10  # è®­ç»ƒè½®æ•°
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for src, trg in train_loader:
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        output = model(src, trg[:-1])  # è¾“å…¥ç»™æ¨¡å‹çš„æ˜¯é™¤äº†æœ€åä¸€ä¸ªè¯çš„ç›®æ ‡å¥å­
        # Reshapeè¾“å‡ºä»¥åŒ¹é…æŸå¤±å‡½æ•°æœŸæœ›çš„è¾“å…¥
        output_dim = output.shape[-1]
        output = output.view(-1, output_dim)
        trg = trg[1:].view(-1)  # ä»ç¬¬ä¸€ä¸ªè¯å¼€å§‹çš„ç›®æ ‡å¥å­
        loss = criterion(output, trg) # è®¡ç®—æ¨¡å‹è¾“å‡ºå’Œå®é™…ç›®æ ‡åºåˆ—ä¹‹é—´çš„æŸå¤±
        loss.backward()  # é€šè¿‡åå‘ä¼ æ’­è®¡ç®—æŸå¤±ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦
        optimizer.step()  # æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œè¿™æ˜¯ä¼˜åŒ–å™¨çš„ä¸€ä¸ªæ­¥éª¤
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss}')
    
```

æˆ‘æ‹¿ä¸‹é¢çš„ç´ æä¸¾ä¾‹ï¼Œç®€å•è§£é‡Šä¸€ä¸‹è®­ç»ƒè¿‡ç¨‹ã€‚

```plain
æˆ‘ å–œæ¬¢ å­¦ä¹  æœºå™¨ å­¦ä¹ ã€‚
I like studying machine learning
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œå…ˆæŠŠåŸæ–‡æœ¬è½¬åŒ–æˆåœ¨å¯¹åº”è¯è¯­è¡¨é‡Œçš„è¯­è¨€åºåˆ—ï¼Œæ¯”å¦‚åœ¨ä¸­æ–‡è¯æ±‡è¡¨ä¸­ï¼Œ`æˆ‘ å–œæ¬¢ å­¦ä¹  æœºå™¨ å­¦ä¹ ` åˆ†åˆ«å¯¹åº”çš„æ˜¯ `1,2,3,4,5`ï¼Œé‚£ä¹ˆè½¬åŒ–æˆçš„è¯­è¨€åºåˆ—å°±æ˜¯ `[1,2,3,4,5]`ï¼Œä¹Ÿå°±æ˜¯å‰é¢è®²çš„train\_loaderé‡Œçš„æ ¼å¼ã€‚

ç¼–ç å™¨æ¥æ”¶åˆ°è¯­è¨€åºåˆ—ï¼Œç»è¿‡ç¥ç»ç½‘ç»œGRUå•å…ƒå¤„ç†åï¼Œç”Ÿæˆä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡ï¼Œè¿™ä¸ªä¸Šä¸‹æ–‡å‘é‡ä¼šä½œä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€ã€‚

è§£ç å™¨æ¥æ”¶ä¸Šä¸‹æ–‡å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡ä»¥åŠå·²ç”Ÿæˆçš„éƒ¨åˆ†ç›®æ ‡è¯­è¨€åºåˆ—ï¼Œè®¡ç®—ç›®æ ‡è¯æ±‡è¡¨ä¸­æ¯ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œåœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œè§£ç å™¨å¯èƒ½è®¡ç®—å‡ºç›®æ ‡è¯æ±‡è¡¨ä¸­æ¯ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¦‚ `"I": 0.3, "like": 0.1, "studying": 0.5, "machine": 0.05, "learning": 0.05`ï¼Œæ ¹æ®è§£ç å™¨ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯studyingä½œä¸ºå½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºã€‚

æ¨¡å‹å°†è§£ç å™¨ç”Ÿæˆçš„è¾“å‡ºè¯æ±‡ä¸ç›®æ ‡è¯­è¨€å¥å­ï¼ˆâ€œI like studying machine learning.â€ï¼‰ä¸­å½“å‰æ—¶é—´æ­¥å¯¹åº”çš„è¯æ±‡è¿›è¡Œå¯¹æ¯”ã€‚è¿™é‡Œè§£ç å™¨è¾“å‡ºçš„ `"studying"` ä¸ç›®æ ‡è¯­è¨€å¥å­ä¸­çš„ `"I"` è¿›è¡Œå¯¹æ¯”ï¼Œå‘ç°å®ƒä»¬ä¹‹é—´çš„å·®åˆ«è¾ƒå¤§ã€‚

æ ¹æ®è§£ç å™¨è¾“å‡º `"studying"` å’Œç›®æ ‡è¯­è¨€å¥å­ä¸­çš„çœŸå®è¯æ±‡ `"I"` è®¡ç®—æŸå¤±ï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­ç®—æ³•è®¡ç®—æ¢¯åº¦ã€‚æŸå¤±å€¼æ˜¯ä¸€ä¸ªè¡¡é‡æ¨¡å‹é¢„æµ‹è¾“å‡ºä¸çœŸå®ç›®æ ‡ä¹‹é—´å·®å¼‚çš„æŒ‡æ ‡ã€‚ç„¶åï¼Œæ ¹æ®æŸå¤±å€¼æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ±‡ã€‚

é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°æ¨¡å‹è¾¾åˆ°æŒ‡å®šçš„è®­ç»ƒè½®æ•°æˆ–è€…æ»¡è¶³å…¶ä»–åœæ­¢è®­ç»ƒçš„æ¡ä»¶ã€‚åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œæ¨¡å‹éƒ½åœ¨å°è¯•è°ƒæ•´è‡ªå·±çš„å‚æ•°ï¼Œä»¥ä½¿å…¶é¢„æµ‹è¾“å‡ºæ›´æ¥è¿‘çœŸå®çš„ç›®æ ‡è¯­è¨€åºåˆ—ï¼Œä»è€Œæé«˜ç¿»è¯‘è´¨é‡ã€‚

æ‰€ä»¥è¿™é‡Œå°±èƒ½çœ‹å‡ºï¼Œ**è®­ç»ƒè½®æ•°å°±éå¸¸å…³é”®ï¼Œä¸èƒ½å¤ªå°‘ï¼Œä¹Ÿä¸èƒ½å¤ªå¤šã€‚**

#### æ¨¡å‹éªŒè¯

```python
def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):
    # å°†è¾“å…¥å¥å­è¿›è¡Œåˆ†è¯å¹¶è½¬æ¢ä¸ºç´¢å¼•åºåˆ—
    src_tokens = ['<sos>'] + tokenize_cn(sentence) + ['<eos>']
    src_indices = [src_vocab[token] if token in src_vocab else src_vocab['<unk>'] for token in src_tokens]
    # å°†è¾“å…¥å¥å­è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
    src_tensor = torch.LongTensor(src_indices).unsqueeze(1).to(device)
    # å°†è¾“å…¥å¥å­ä¼ é€’ç»™ç¼–ç å™¨ä»¥è·å–ä¸Šä¸‹æ–‡å¼ é‡
    with torch.no_grad():
        encoder_hidden = model.encoder(src_tensor)
    # åˆå§‹åŒ–è§£ç å™¨è¾“å…¥ä¸º<sos>
    trg_token = '<sos>'
    trg_index = trg_vocab[trg_token]
    # å­˜å‚¨ç¿»è¯‘ç»“æœ
    translation = []
    # è§£ç è¿‡ç¨‹
    for _ in range(max_len):
        # å°†è§£ç å™¨è¾“å…¥ä¼ é€’ç»™è§£ç å™¨ï¼Œå¹¶è·å–è¾“å‡ºå’Œéšè—çŠ¶æ€
        with torch.no_grad():
            trg_tensor = torch.LongTensor([trg_index]).to(device)
            output, encoder_hidden = model.decoder(trg_tensor, encoder_hidden)
        # è·å–è§£ç å™¨è¾“å‡ºä¸­æ¦‚ç‡æœ€é«˜çš„å•è¯çš„ç´¢å¼•
        pred_token_index = output.argmax(dim=1).item()
        # å¦‚æœé¢„æµ‹çš„å•è¯æ˜¯å¥å­ç»“æŸç¬¦ï¼Œåˆ™åœæ­¢è§£ç 
        if pred_token_index == trg_vocab['<eos>']:
            break
        # å¦åˆ™ï¼Œå°†é¢„æµ‹çš„å•è¯æ·»åŠ åˆ°ç¿»è¯‘ç»“æœä¸­
        pred_token = list(trg_vocab.keys())[list(trg_vocab.values()).index(pred_token_index)]
        translation.append(pred_token)
        # æ›´æ–°è§£ç å™¨è¾“å…¥ä¸ºå½“å‰é¢„æµ‹çš„å•è¯
        trg_index = pred_token_index
    # å°†ç¿»è¯‘ç»“æœè½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è¿”å›
    translation = ' '.join(translation)
    return translation

sentence = "æˆ‘å–œæ¬¢å­¦ä¹ æœºå™¨å­¦ä¹ ã€‚"
translation = translate_sentence(sentence, cn_vocab, en_vocab, model, device)
print(f"Chinese: {sentence}")
print(f"Translation: {translation}")

```

ç¨‹åºè¾“å‡ºå¦‚ä¸‹ï¼š

```python
Chinese: æˆ‘å–œæ¬¢å­¦ä¹ æœºå™¨å­¦ä¹ ã€‚
Translation: I a a a . a . . . .
```

çœ‹ä¸Šå»åªç¿»è¯‘æˆåŠŸäº†â€œæˆ‘â€è¿™ä¸ªå­—ï¼Œå…¶ä»–éƒ½æ²¡å‡ºæ¥ï¼Œå¤§æ¦‚ç‡æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®å¤ªå°‘çš„åŸå› ã€‚

æ¨ç†è¿‡ç¨‹å’Œè®­ç»ƒè¿‡ç¨‹å¾ˆåƒï¼ŒåŒºåˆ«åœ¨äºï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹ä¼šè®°ä½å‚æ•°ï¼Œæ¨ç†çš„æ—¶å€™ç›´æ¥æ ¹æ®è¿™äº›å‚æ•°è®¡ç®—ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡å³å¯ã€‚

ç»“å°¾æ”¾ä¸€ä¸‹å®Œæ•´çš„ä»£ç ï¼š

```python
import torch
from torch.utils.data import Dataset, DataLoader
import spacy
import jieba
from collections import Counter
from torch.nn.utils.rnn import pad_sequence
import torch.nn as nn
import random
import torch.optim as optim

# åŠ è½½è‹±æ–‡çš„Spacyæ¨¡å‹
spacy_en = spacy.load('en_core_web_sm')

def tokenize_en(text):
    """
    Tokenizes English text from a string into a list of strings (tokens)
    """
    return [tok.text for tok in spacy_en.tokenizer(text)]

def tokenize_cn(text):
    """
    Tokenizes Chinese text from a string into a list of strings (tokens)
    """
    return list(jieba.cut(text))

def build_vocab(sentences, tokenizer, max_size, min_freq):
    token_freqs = Counter()
    for sentence in sentences:
        tokens = tokenizer(sentence)
        token_freqs.update(tokens)
    vocab = {token: idx + 4 for idx, (token, freq) in enumerate(token_freqs.items()) if freq >= min_freq}
    vocab['<unk>'] = 0
    vocab['<pad>'] = 1
    vocab['<sos>'] = 2
    vocab['<eos>'] = 3
    return vocab

class TranslationDataset(Dataset):
    def __init__(self, src_sentences, trg_sentences, src_vocab, trg_vocab, tokenize_src, tokenize_trg):
        self.src_sentences = src_sentences
        self.trg_sentences = trg_sentences
        self.src_vocab = src_vocab
        self.trg_vocab = trg_vocab
        self.tokenize_src = tokenize_src
        self.tokenize_trg = tokenize_trg
    def __len__(self):
        return len(self.src_sentences)
    def __getitem__(self, idx):
        src_sentence = self.src_sentences[idx]
        trg_sentence = self.trg_sentences[idx]
        src_indices = [self.src_vocab[token] if token in self.src_vocab else self.src_vocab['<unk>']
                       for token in ['<sos>'] + self.tokenize_src(src_sentence) + ['<eos>']]
        trg_indices = [self.trg_vocab[token] if token in self.trg_vocab else self.trg_vocab['<unk>']
                       for token in ['<sos>'] + self.tokenize_trg(trg_sentence) + ['<eos>']]
        return torch.tensor(src_indices), torch.tensor(trg_indices)

def collate_fn(batch):
    src_batch, trg_batch = zip(*batch)
    src_batch = pad_sequence(src_batch, padding_value=1)  # 1 is the index for <pad>
    trg_batch = pad_sequence(trg_batch, padding_value=1)  # 1 is the index for <pad>
    return src_batch, trg_batch

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
    def forward(self, src):
        # src: [src_len, batch_size]
        embedded = self.dropout(self.embedding(src))
        outputs, hidden = self.rnn(embedded)
        return hidden

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, input, hidden):
        input = input.unsqueeze(0)  # input: [1, batch_size]
        embedded = self.dropout(self.embedding(input))
        output, hidden = self.rnn(embedded, hidden)
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        # src: [src_len, batch_size]
        # trg: [trg_len, batch_size]
        # teacher_forcing_ratioæ˜¯ä½¿ç”¨çœŸå®æ ‡ç­¾çš„æ¦‚ç‡
        trg_len = trg.shape[0]
        batch_size = trg.shape[1]
        trg_vocab_size = self.decoder.output_dim
        # å­˜å‚¨è§£ç å™¨è¾“å‡º
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        # ç¼–ç å™¨çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ç”¨ä½œè§£ç å™¨çš„åˆå§‹éšè—çŠ¶æ€
        hidden = self.encoder(src)
        # è§£ç å™¨çš„ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯<sos> tokens
        input = trg[0, :]
        for t in range(1, trg_len):
            output, hidden = self.decoder(input, hidden)
            outputs[t] = output
            # å†³å®šæ˜¯å¦ä½¿ç”¨teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[t] if teacher_force else top1
        return outputs

cn_sentences = []
zh_file_path = "train_1w.zh"
# ä½¿ç”¨Pythonçš„æ–‡ä»¶æ“ä½œé€è¡Œè¯»å–æ–‡ä»¶ï¼Œå¹¶å°†æ¯ä¸€è¡Œçš„å†…å®¹æ·»åŠ åˆ°åˆ—è¡¨ä¸­
with open(zh_file_path, "r", encoding="utf-8") as file:
    for line in file:
        # å»é™¤è¡Œæœ«çš„æ¢è¡Œç¬¦å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        cn_sentences.append(line.strip())
en_sentences = []
en_file_path = "train_1w.en"
# ä½¿ç”¨Pythonçš„æ–‡ä»¶æ“ä½œé€è¡Œè¯»å–æ–‡ä»¶ï¼Œå¹¶å°†æ¯ä¸€è¡Œçš„å†…å®¹æ·»åŠ åˆ°åˆ—è¡¨ä¸­
with open(en_file_path, "r", encoding="utf-8") as file:
    for line in file:
        # å»é™¤è¡Œæœ«çš„æ¢è¡Œç¬¦å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        en_sentences.append(line.strip())
# cn_sentences å’Œ en_sentences åˆ†åˆ«åŒ…å«äº†æ‰€æœ‰çš„ä¸­æ–‡å’Œè‹±æ–‡å¥å­
cn_vocab = build_vocab(cn_sentences, tokenize_cn, max_size=10000, min_freq=2)
en_vocab = build_vocab(en_sentences, tokenize_en, max_size=10000, min_freq=2)

# cn_vocab å’Œ en_vocab æ˜¯å·²ç»åˆ›å»ºçš„è¯æ±‡è¡¨
dataset = TranslationDataset(cn_sentences, en_sentences, cn_vocab, en_vocab, tokenize_cn, tokenize_en)
train_loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPUï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("è®­ç»ƒè®¾å¤‡ä¸ºï¼š", device)
# å®šä¹‰ä¸€äº›è¶…å‚æ•°
INPUT_DIM = 10000  # è¾“å…¥è¯­è¨€çš„è¯æ±‡é‡
OUTPUT_DIM = 10000  # è¾“å‡ºè¯­è¨€çš„è¯æ±‡é‡
ENC_EMB_DIM = 256  # ç¼–ç å™¨åµŒå…¥å±‚å¤§å°
DEC_EMB_DIM = 256  # è§£ç å™¨åµŒå…¥å±‚å¤§å°
HID_DIM = 512  # éšè—å±‚ç»´åº¦
N_LAYERS = 2  # RNNå±‚çš„æ•°é‡
ENC_DROPOUT = 0.5  # ç¼–ç å™¨ä¸­dropoutçš„æ¯”ä¾‹
DEC_DROPOUT = 0.5  # è§£ç å™¨ä¸­dropoutçš„æ¯”ä¾‹
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)
model = Seq2Seq(enc, dec, device).to(device)
# å‡å®šæ¨¡å‹å·²ç»è¢«å®ä¾‹åŒ–å¹¶ç§»åˆ°äº†æ­£ç¡®çš„è®¾å¤‡ä¸Š
model.to(device)
# å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss(ignore_index=en_vocab['<pad>'])  # å¿½ç•¥<pad>æ ‡è®°çš„æŸå¤±
num_epochs = 10  # è®­ç»ƒè½®æ•°
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for src, trg in train_loader:
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        output = model(src, trg[:-1])  # è¾“å…¥ç»™æ¨¡å‹çš„æ˜¯é™¤äº†æœ€åä¸€ä¸ªè¯çš„ç›®æ ‡å¥å­
        # Reshapeè¾“å‡ºä»¥åŒ¹é…æŸå¤±å‡½æ•°æœŸæœ›çš„è¾“å…¥
        output_dim = output.shape[-1]
        output = output.view(-1, output_dim)
        trg = trg[1:].view(-1)  # ä»ç¬¬ä¸€ä¸ªè¯å¼€å§‹çš„ç›®æ ‡å¥å­
        loss = criterion(output, trg)
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # æ›´æ–°å‚æ•°
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss}')
    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ éªŒè¯æ­¥éª¤

def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):
    # å°†è¾“å…¥å¥å­è¿›è¡Œåˆ†è¯å¹¶è½¬æ¢ä¸ºç´¢å¼•åºåˆ—
    src_tokens = ['<sos>'] + tokenize_cn(sentence) + ['<eos>']
    src_indices = [src_vocab[token] if token in src_vocab else src_vocab['<unk>'] for token in src_tokens]
    # å°†è¾“å…¥å¥å­è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
    src_tensor = torch.LongTensor(src_indices).unsqueeze(1).to(device)
    # å°†è¾“å…¥å¥å­ä¼ é€’ç»™ç¼–ç å™¨ä»¥è·å–ä¸Šä¸‹æ–‡å¼ é‡
    with torch.no_grad():
        encoder_hidden = model.encoder(src_tensor)
    # åˆå§‹åŒ–è§£ç å™¨è¾“å…¥ä¸º<sos>
    trg_token = '<sos>'
    trg_index = trg_vocab[trg_token]
    # å­˜å‚¨ç¿»è¯‘ç»“æœ
    translation = []
    # è§£ç è¿‡ç¨‹
    for _ in range(max_len):
        # å°†è§£ç å™¨è¾“å…¥ä¼ é€’ç»™è§£ç å™¨ï¼Œå¹¶è·å–è¾“å‡ºå’Œéšè—çŠ¶æ€
        with torch.no_grad():
            trg_tensor = torch.LongTensor([trg_index]).to(device)
            output, encoder_hidden = model.decoder(trg_tensor, encoder_hidden)
        # è·å–è§£ç å™¨è¾“å‡ºä¸­æ¦‚ç‡æœ€é«˜çš„å•è¯çš„ç´¢å¼•
        pred_token_index = output.argmax(dim=1).item()
        # å¦‚æœé¢„æµ‹çš„å•è¯æ˜¯å¥å­ç»“æŸç¬¦ï¼Œåˆ™åœæ­¢è§£ç 
        if pred_token_index == trg_vocab['<eos>']:
            break
        # å¦åˆ™ï¼Œå°†é¢„æµ‹çš„å•è¯æ·»åŠ åˆ°ç¿»è¯‘ç»“æœä¸­
        pred_token = list(trg_vocab.keys())[list(trg_vocab.values()).index(pred_token_index)]
        translation.append(pred_token)
        # æ›´æ–°è§£ç å™¨è¾“å…¥ä¸ºå½“å‰é¢„æµ‹çš„å•è¯
        trg_index = pred_token_index
    # å°†ç¿»è¯‘ç»“æœè½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è¿”å›
    translation = ' '.join(translation)
    return translation

sentence = "æˆ‘å–œæ¬¢å­¦ä¹ æœºå™¨å­¦ä¹ ã€‚"
translation = translate_sentence(sentence, cn_vocab, en_vocab, model, device)
print(f"Chinese: {sentence}")
print(f"Translation: {translation}")

```

## å°ç»“

è¿™èŠ‚è¯¾æˆ‘ä»¬è‡ªå·±åŠ¨æ‰‹è®­ç»ƒäº†ä¸€ä¸ªSeq2Seqæ¨¡å‹ï¼ŒSeq2Seqå¯ä»¥ç®—æ˜¯ä¸€ç§é«˜çº§çš„ç¥ç»ç½‘ç»œæ¨¡å‹äº†ï¼Œé™¤äº†åšè¯­è¨€ç¿»è¯‘å¤–ï¼Œç”šè‡³å¯ä»¥åšåŸºæœ¬çš„é—®ç­”ç³»ç»Ÿäº†ã€‚ä½†æ˜¯ï¼ŒSeq2Seqç¼ºç‚¹ä¹Ÿæ¯”è¾ƒæ˜æ˜¾ï¼Œé¦–å…ˆSeq2Seqä½¿ç”¨å›ºå®šä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ‰€ä»¥é•¿è·ç¦»ä¾èµ–èƒ½åŠ›è¾ƒå¼±ã€‚æ­¤å¤–ï¼ŒSeq2Seqè®­ç»ƒå’Œæ¨ç†é€šå¸¸éœ€è¦é€æ­¥å¤„ç†è¾“å…¥å’Œè¾“å‡ºåºåˆ—ï¼Œæ‰€ä»¥å¤„ç†é•¿åºåˆ—å¯èƒ½ä¼šæœ‰é™åˆ¶ã€‚æœ€åSeq2Seqå‚æ•°é‡é€šå¸¸è¾ƒå°‘ï¼Œæ‰€ä»¥é¢å¯¹å¤æ‚åœºæ™¯ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¼šå—é™ã€‚

å¸¦ç€è¿™äº›é—®é¢˜ï¼Œä¸‹ä¸€èŠ‚è¯¾æˆ‘å°†ä¼šå‘ä½ ä»‹ç»ç»ˆæå¤§bossï¼š**Transformer**ï¼Œæˆ‘ä»¬å­¦ä¹ äº†è¿™ä¹ˆå¤šåŸºç¡€æ¦‚å¿µï¼Œå°±æ˜¯ä¸ºå­¦ä¹ Transformeråšé“ºå«ï¼Œä»ML-&gt;NLP-&gt;Word2Vec-&gt;Seq2Seq-&gt;Transformerä¸€æ­¥ä¸€æ­¥é€’è¿›ã€‚

æ³¨ï¼šen\_core\_web\_smã€train\_1w.zhã€train\_1w.en é“¾æ¥: [https://pan.baidu.com/s/1\_GG3bIAjqpPGLGugHEI5Dg?pwd=fm8j](https://pan.baidu.com/s/1_GG3bIAjqpPGLGugHEI5Dg?pwd=fm8j) æå–ç : fm8j

## æ€è€ƒé¢˜

æˆ‘åˆšåˆšè®²è¿‡ï¼Œæ¨ç†çš„æ—¶å€™æ¨¡å‹ä¼šä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­è®°ä½çš„å‚æ•°æ¥è¿›è¡Œæ¦‚ç‡é¢„æµ‹ï¼Œä½ å¯ä»¥æ€è€ƒä¸€ä¸‹ï¼Œæ¨¡å‹çš„å‚æ•°åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿæ¬¢è¿åœ¨è¯„è®ºåŒºç•™è¨€ï¼Œæˆ‘ä»¬ä¸€èµ·è®¨è®ºå­¦ä¹ ï¼Œå¦‚æœä½ è§‰å¾—è¿™èŠ‚è¯¾çš„å†…å®¹å¯¹ä½ æœ‰å¸®åŠ©çš„è¯ï¼Œä¹Ÿæ¬¢è¿ä½ åˆ†äº«ç»™éœ€è¦çš„æœ‹å‹ï¼Œé‚€TAä¸€èµ·å­¦ä¹ ï¼Œæˆ‘ä»¬ä¸‹èŠ‚è¯¾å†è§ï¼
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ8ï¼‰</strong></div><ul>
<li><span>æ–¹æ¢</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>train_1w.zh
train_1w.en
è¯·æä¾›ä¸€ä¸‹å“ˆï¼Œè°¢è°¢</p>2024-06-26</li><br/><li><span>å°æ¯›é©´</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆè¡¥å……ä¸€ä¸‹ï¼šOSError: [E053] Could not read config file from external\en_core_web_sm-2.3.0\config.cfg
ä»ç½‘ç›˜ä¸‹è½½çš„æ¨¡å‹åŠ è½½ä¼šæŠ¥é”™ï¼Œåœ¨huggingfaceä¸Šå¼•ç”¨çš„æ¨¡å‹æ¯æ¬¡æ‰§è¡Œpred_token_index = output.argmax(dim=1).item()è¿”å›éƒ½æ˜¯0ï¼Œè¿™æ˜¯ä¸ºå•¥ï¼Ÿ
</p>2024-09-12</li><br/><li><span>å°æ¯›é©´</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œè¯·æ•™ä¸€ä¸‹ä¸ºä»€ä¹ˆ pred_token_index = output.argmax(dim=1).item()è¿™æ®µä»£ç æ°¸è¿œè¿”å›éƒ½æ˜¯0ï¼Œæ˜¯æˆ‘å¼•ç”¨çš„æ¨¡å‹ä¸å¯¹å˜›ï¼Ÿ
</p>2024-09-12</li><br/><li><span>çŸ³äº‘å‡</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç¬¬ä¸‰ç« å¼€å§‹çš„æŠ€æœ¯åŸç†éƒ¨åˆ†è¶Šæ¥è¶Šéš¾äº†ã€‚</p>2024-09-03</li><br/><li><span>ç‹æ—§ä¸š</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆè¯·æ•™ä¸‹æ–‡ä¸­è¿™ç§åŠ¨å›¾å’‹åšçš„</p>2024-08-24</li><br/><li><span>æ–¹æ¢</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>en_core_web_sm
ç­‰æ–‡ä»¶åœ¨å“ªé‡Œä¸‹è½½ï¼Ÿ
</p>2024-06-26</li><br/><li><span>Geek_7df415</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æ¨¡å‹è®­ç»ƒéƒ¨åˆ†ï¼Œ AIchallenger2017 çš„é“¾æ¥ï¼ŒAccessDenied</p>2024-06-26</li><br/><li><span>kiikii</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>åå‘ç®—æ³•ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œä¼šè¢«æ›´æ–°çš„å‚æ•°ï¼Œæ˜¯æƒé‡å’Œåç½®ï¼Œweightså’Œbaisï¼›æƒé‡å³ä¸Šä¸‹æ–‡å‘é‡ä¸­çš„æ¯ä¸ªè¯å’Œå·²ç”Ÿæˆåºåˆ—ï¼Œå½±å“åˆ°å½“å‰è¦è¢«ç”Ÿæˆçš„è¯çš„æƒé‡ã€å½±å“åŠ›æœ‰å¤šå¤§ï¼›baisesæ˜¯æŒ‡ä¸€ä¸ªåŸºç¡€é˜ˆå€¼</p>2025-01-19</li><br/>
</ul>