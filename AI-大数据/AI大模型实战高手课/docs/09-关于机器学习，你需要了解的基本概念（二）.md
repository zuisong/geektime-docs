你好，我是独行。

上一节课我们了解了机器学习的基本概念，学习了线性回归和逻辑回归，相信你对机器学习有了初步理解，这节课我们继续讲解机器学习的经典算法，先从决策树开始。

## 经典算法

### 决策树

决策树是一种常用的机器学习算法，用于 **分类和回归任务**。通过从数据中学习决策规则来预测目标变量的值。想象你在玩一个“是或否”的猜谜游戏，每次你只能问一个问题，对方只能回答是或否，你的目标就是用最少的问题猜出对方心中的答案。在实际应用中，决策树有很多场景，比如客户分类、信用评分、医疗诊断等等，下面我举一个简单的例子。

我们要根据天气情况、温度和风速来决定进行什么活动，比如宅在家还是出去玩，我们准备一些简单的数据集。

- 天气状况：晴天（0）、阴天（1）、雨天（2）
- 温度：低（0）、中（1）、高（2）
- 风速：无风（0）、微风（1）、强风（2）

活动：

- 去野餐（0）
- 去博物馆（1）
- 在家看书（2）

我们看一下使用sklearn库提供的决策树算法和模型的示例代码。

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
import numpy as np

# 创建数据集
X = np.array([
    [0, 2, 0],  # 晴天，高温，无风
    [1, 1, 1],  # 阴天，中温，微风
    [2, 0, 2],  # 雨天，低温，强风
    # ... 添加更多样本以增加模型的准确性
])
y = np.array([0, 1, 2])  # 分别对应去野餐、去博物馆、在家看书

# 初始化决策树模型，设置最大深度为5
clf = DecisionTreeClassifier(max_depth=5, random_state=42)

# 训练模型
clf.fit(X, y)

# 可视化决策树
plt.figure(figsize=(20, 10))
plot_tree(clf, filled=True, feature_names=["天气状况", "温度", "风速"], class_names=["去野餐", "去博物馆", "在家看书"], rounded=True, fontsize=12)
plt.show()

```

程序运行结果如下：

![图片](https://static001.geekbang.org/resource/image/28/7b/282cfa32356179a9afb3aa75739e1b7b.png?wh=3358x2136)

我来简单解释下这几个信息指标。

- Gini指数：一个衡量节点纯度的指标，用于分类问题。一个节点的Gini指数越低，表示这个节点包含的样本越倾向于属于同一个类别，0表示所有样本都属于同一类别，即完全纯净。
- samples：表示到达这个节点的样本数量。这个数值提供了关于树的分支如何基于数据集进行分割的直观理解，也可以帮助我们评估决策树各个部分的数据覆盖范围。
- value：表示这个节点每个类别的样本数量。对于二分类问题，它可能显示为\[value1, value2\]，代表了属于第一类和第二类的样本数量。这个数组提供了节点分类分布的具体信息，有助于我们了解在每个决策节点处数据是如何被分割的。
- class：代表了在当前节点样本中占多数的类别。如果一个节点是叶节点，那么这个类别就是这个节点的预测类别。即使在非叶节点，class也可以显示该节点样本的主要类别。

通过这些信息，我们不仅可以理解决策树是如何根据特征分割数据的，还可以评估树的深度、每个节点的决策质量，以及模型是否可能出现过拟合，比如某些叶节点只有很少的样本，具体原因我一会儿就会讲到。这些指标也是调整模型参数来优化模型性能的重要依据，比如树的最大深度、节点分割的最小样本数等。

### 随机森林

随机森林是一种流行的机器学习算法，属于集成学习家族。简单来说，随机森林通过构建多个决策树来进行预测，其基本思想是 **集体智慧——单个模型可能有限，但多个模型集合起来可以作出更好的判断**。随机森林算法的关键在于它的随机性。

1. 样本随机性：每棵树训练的数据通过从原始数据中进行随机抽样得到，这种方法称为自助采样。
2. 特征随机性：在分裂决策树的节点时，算法会从所有特征中随机选取一部分特征，然后只在这些随机选取的特征中寻找最优分裂特征。

这种随机性使随机森林模型具有很高的准确性，同时也能防止模型过拟合。这里我解释下什么叫过拟合？过拟合是机器学习领域一个非常重要的概念，简单来讲就是 **因为训练数据或者模型参数导致模型缺乏泛化能力。**

这里泛化能力可以理解为通用能力。我们训练模型的目的是希望模型解决通用问题。比如训练一个模型，用来识别一个照片中有没有狗。喂了1000张狗的照片进行训练，结果识别的时候只有尾巴直直的狗可以被识别出来。原因是有一个参数描述尾巴是直的还是弯的，这个参数的存在使模型额外识别狗的某一个特征，失去了泛化能力，导致了过拟合问题。

前面我们讲的决策树为什么会出现过拟合呢？如果决策条件中有一个非常不通用的条件，分类后这个分支节点只有一个样本，说明这个决策没有通用性（泛化能力），那么这个决策树模型就会存在过拟合问题。

随机森林怎么解决决策树过拟合的问题呢？我们先来看下随机森林的工作原理。

1. 从原始数据集中随机抽样选取多个子集。
2. 对每个子集训练一个决策树。
3. 每棵树独立进行预测。
4. 最终预测结果是所有的树预测结果的投票或平均。

简易代码如下：

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# 加载数据集
iris = load_iris()
X, y = iris.data, iris.target

# 训练随机森林模型
rf = RandomForestClassifier(n_estimators=3, random_state=42) # 使用3棵树以便于可视化
rf.fit(X, y)

# 绘制随机森林中的决策树
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5), dpi=100)
for index in range(0, 3):
    plot_tree(rf.estimators_[index],
              feature_names=iris.feature_names,
              class_names=iris.target_names,
              filled=True,
              ax=axes[index])

    axes[index].set_title(f'Tree {index + 1}')

plt.tight_layout()
plt.show()

```

程序运行结果如下：

![图片](https://static001.geekbang.org/resource/image/93/dc/937357e901fd8350abe66dfcfa9183dc.png?wh=5600x2200)

这3棵树会通过投票的方式来配合使用。具体来说，对于分类问题，每棵树在接收到一个输入样本后都会独立做出一个预测。最终，整个随机森林的预测结果是基于所有树的预测结果进行多数投票得到的。也就是说，被最多棵树预测为某一类别的类别将成为随机森林的最终预测结果。

具体步骤如下：

1. 预测：对于每个输入样本，随机森林中的每棵树都会独立做出自己的预测。
2. 投票：收集所有树对每个样本的预测结果，并对这些结果进行计数。
3. 决定最终预测结果：对于每个样本，被预测次数最多的类别将成为随机森林的最终预测结果。

在实际应用过程中，我们可以构造更多的决策树进行预测，比如100，甚至更多，这样可以使模型更准确更稳定。

### 支持向量机

支持向量机（Support Vector Machine，简称SVM）是一种强大的分类算法，在数据科学和机器学习领域广泛应用。SVM的核心思想是， **找到一个最优的决策边界，或者称为“超平面”，这个边界能够以最大的间隔将不同类别的数据分开**。这里有几个关键点需要好好理解一下。

1. 超平面：在二维空间中，这个边界就是一条线；在三维空间中，是一个平面；而在更高维度的空间中，我们称之为“超平面”。这个超平面的任务就是尽可能准确地分隔开不同类别的数据点。
2. 最大间隔：SVM不仅仅寻找一个能够将数据分类的边界，它寻找的是能够以最大间隔分开数据的边界。这个间隔是指不同类别的数据点到这个边界的最近距离，SVM试图使这个距离尽可能大。直观上，这样的边界更能抵抗数据中的小变动，提高模型的泛化能力。
3. 支持向量：决定这个最优超平面位置的几个关键数据点被称为支持向量。它们是最靠近决策边界的点，实际上这个最大间隔的边界就是通过这些点来确定的。
4. 核技巧：当数据不是线性可分时，也就是说无法通过一个直线或平面来分隔，SVM可以利用所谓的核技巧将数据映射到一个更高维的空间，在这个空间中数据可能是线性可分的。这使得SVM在处理非线性数据时非常强大。

简单来说，你可以将SVM想象成一个尽可能在不同类别间画一条粗的、清晰的界线，而这条界线是由距离它最近的几个点（支持向量）决定的。这种方法使得分类决策不仅仅依赖于数据的分布，而且具有很好的泛化能力，能够应对未见过的新数据。

我们简单看下示例代码：

```python
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# 生成模拟数据
X, y = datasets.make_blobs(n_samples=50, centers=2, random_state=6)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建 SVM 模型
model = SVC(kernel='linear')
model.fit(X_train, y_train)

# 绘制数据点和分类边界
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

# 绘制决策边界
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# 创建网格点
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = model.decision_function(xy).reshape(XX.shape)

# 绘制决策边界和间隔
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])
plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k')
plt.title("支持向量机分类示例")
plt.xlabel("特征1")
plt.ylabel("特征2")
plt.show()

```

程序运行结果如下：

![图片](https://static001.geekbang.org/resource/image/87/e7/8765a4011edf491ed1e0706fa1265ee7.png?wh=1848x1374)

这个例子比较简单，我们使用一个线性核的SVM模型来分类模拟生成的数据点。数据点用橙色和黄色表示，分别代表两个不同的类别。

图中的黑色实线表示决策边界，这是SVM找到的最佳超平面，用于将两类数据分开。虚线表示决策边界的边缘，这些边缘之间的区域是模型的间隔。支持向量在图中已经用黑色圆圈圈出，它们是离决策边界最近的几个点，对于确定决策边界至关重要。

这个例子通过直线就可以完成分类，假设这些点在一个平面上混合在一起，那么通过直线是无法进行分类的，这时候我们需要进行升维，再加一个维度，在三维空间里，通过一个面进行分类。在实际应用中，SVM可以通过选择不同的核函数来处理更复杂的数据集。

### 神经网络

神经网络应该说是目前最火的机器学习算法模型，现在主流的大部分大模型都是基于深度神经网络的。主要设计思路是模仿人的大脑，由许多小的、处理信息的单位组成，这些单位就是神经元，各神经元之间彼此连接，每个神经元可以向其他神经元发送和接收信号。通过这种方式，神经网络能够执行各种复杂的计算任务，比如图像和语音识别、自然语言处理以及许多其他类型的机器学习任务。

一个简单的神经网络包含三层：输入层、隐藏层和输出层。一般情况下，一个神经网络输入层和输出层仅有一层，隐藏层可以有多层，不过也有特殊情况，我们后面再讲。

- 输入层：接收原始数据输入，例如图片的像素值或者一段文本的编码。
- 隐藏层：处理输入数据，可以有一个或多个隐藏层。隐藏层的神经元会对输入数据进行加权和，应用激活函数，这个过程可以捕捉输入数据中的复杂模式和关系。
- 输出层：根据隐藏层的处理结果，输出一个值或一组值，代表了神经网络的最终预测结果。

我们来看一个示例。

```python
import matplotlib.pyplot as plt

# 创建一个简单的神经网络图，并调整文字标签的位置
def plot_neural_network_adjusted():
    fig, ax = plt.subplots(figsize=(10, 6))  # 创建绘图对象

    # 输入层、隐藏层、输出层的神经元数量
    input_neurons = 3
    hidden_neurons = 4
    output_neurons = 2

    # 绘制神经元
    layer_names = ['输入层', '隐藏层', '输出层']
    for layer, neurons in enumerate([input_neurons, hidden_neurons, output_neurons]):
        for neuron in range(neurons):
            circle = plt.Circle((layer*2, neuron*1.5 - neurons*0.75 + 0.75), 0.5, color='skyblue', ec='black', lw=1.5, zorder=4)
            ax.add_artist(circle)

    # 绘制连接线
    for input_neuron in range(input_neurons):
        for hidden_neuron in range(hidden_neurons):
            line = plt.Line2D([0*2, 1*2], [input_neuron*1.5 - input_neurons*0.75 + 0.75, hidden_neuron*1.5 - hidden_neurons*0.75 + 0.75], c='gray', lw=1, zorder=1)
            ax.add_artist(line)
    for hidden_neuron in range(hidden_neurons):
        for output_neuron in range(output_neurons):
            line = plt.Line2D([1*2, 2*2], [hidden_neuron*1.5 - hidden_neurons*0.75 + 0.75, output_neuron*1.5 - output_neurons*0.75 + 0.75], c='gray', lw=1, zorder=1)
            ax.add_artist(line)

    # 设置图参数
    ax.set_xlim(-1, 5)
    ax.set_ylim(-2, max(input_neurons, hidden_neurons, output_neurons)*1.5)
    plt.axis('off')  # 不显示坐标轴

    # 调整层名称的绘制位置，确保不被遮挡
    for i, name in enumerate(layer_names):
        plt.text(i*2, max(input_neurons, hidden_neurons, output_neurons)*0.75 + 1, name, horizontalalignment='center', fontsize=14, zorder=5)

    plt.title("简单神经网络图解", fontsize=16)
    return fig

fig = plot_neural_network_adjusted()
plt.show()

```

![图片](https://static001.geekbang.org/resource/image/81/7a/8141a9yyb159dcd65dbc64058a9a777a.png?wh=2824x1656)

神经网络之所以强，是因为它是非线性的，它可以理解非常复杂的逻辑关系。另外，在深度神经网络中，不同的层可以学习不同的特征，较低的层可能学习简单的特征，较高的层则可以学到更复杂的概念。这种从简单到复杂的学习过程使得神经网络非常适合处理复杂的数据结构。当然，还有一些其他概念，如激活函数、前向传播、反向传播、梯度下降等，我们通过一个例子来说明这些概念。

假设你在做一道菜，而神经网络就像是你的厨房，厨房里有各种炊具和调料，代表着神经网络的各个组成部分。

1. 食材：输入数据，这是你要做菜所需的原材料，类比神经网络的输入数据。
2. 调料：激活函数，赋予了食材不同的特性和味道。
3. 烹饪过程：前向传播，这是你根据菜谱和经验，按照一定的步骤和方法进行的烹饪过程。在前向传播中，神经网络逐层处理输入数据，通过各种操作和激活函数的作用，逐渐提取并组合数据的特征，最终得到输出结果。
4. 味道调整：训练过程，在烹饪过程中，你会尝试不同的调料和烹饪技巧，不断调整菜的味道，直到达到满意的效果。而在神经网络的训练过程中，我们通过反向传播算法来调整网络参数，使得网络的输出尽可能接近真实标签，达到最佳的预测效果。
5. 品尝和反馈：反向传播，在烹饪中，你会尝试做好的菜品，并根据味道来调整调料的用量和烹饪方法。而在神经网络中，反向传播就像是品尝和反馈过程，通过计算模型输出与真实标签之间的差距（损失函数），并利用链式法则逆向传播这个误差，以调整每一层的参数，使得网络的输出更接近真实标签。
6. 调整火候：梯度下降，在烹饪过程中，你还会根据实际情况调整火候，使菜肴烹饪得更加均匀和完美。而在神经网络的训练中，梯度下降就像是调整火候，它是一种优化算法，通过不断沿着梯度的反方向调整参数，逐步降低损失函数，使得网络的预测效果逐渐提升，达到最优的训练效果。
7. 评价口感：损失函数，在烹饪过程中，你可能会根据菜肴的味道、口感等因素来评价菜品的好坏。而在神经网络中，损失函数就像是评价口感的标准，它衡量了模型的输出与真实标签之间的差距，即模型的预测效果，损失函数越小表示模型的预测越接近真实标签。

通过这个示例，我们基本把常见的几个关于神经网络的概念都解释清楚了，技术相关的细节我们会在后面的课程中继续学习。

## 小结

这节课我们只是进行概念性的介绍，目的在于认识基本的机器学习算法，对于初学者而言，理解这些基本的概念至关重要，通过这两节课的学习，我们了解了一些比较常见的机器学习算法，通过一些简单的代码示例，理解了各个算法的实际应用案例。虽然有些抽象，但是慢慢看还是能够理解的。建议你深入研究下sklearn这个库，里面包含这类场景的机器学习算法，然后自己动手敲一下这些示例代码感受一下。

## 思考题

实际上我们上面讲的神经网络之所以强大，就是因为有激活函数，使神经网络呈现为非线性的，那么你可以思考一下，为什么激活函数可以使神经网络呈非线性？如果没有激活函数，神经网络会出现什么问题？欢迎你把你思考后的结果分享到评论区，也欢迎你把这节课的内容分享给其他朋友，我们下节课再见！