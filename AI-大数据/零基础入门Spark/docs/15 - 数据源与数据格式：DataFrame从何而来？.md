ä½ å¥½ï¼Œæˆ‘æ˜¯å´ç£Šã€‚

åœ¨ä¸Šä¸€è®²ï¼Œæˆ‘ä»¬é‡ç‚¹è®²è§£äº†DataFrameä¸Spark SQLçš„æ¸Šæºï¼Œå¹¶æåˆ°ï¼ŒDataFrameæ˜¯Spark SQLçš„é‡è¦å…¥å£ã€‚æ¢å¥è¯è¯´ï¼Œé€šè¿‡åˆ›å»ºDataFrameå¹¶æ²¿ç”¨DataFrameå¼€å‘APIï¼Œæˆ‘ä»¬æ‰èƒ½å……åˆ†åˆ©ç”¨Spark SQLä¼˜åŒ–å¼•æ“æä¾›ç§ç§â€œæ€§èƒ½çº¢åˆ©â€ã€‚æ˜¾ç„¶ï¼Œå¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œç¬¬ä¸€æ­¥çš„åˆ›å»ºDataFrameå°±å˜å¾—è‡³å…³é‡è¦ã€‚

ä¹‹å‰ [ç¬¬13è®²](https://time.geekbang.org/column/article/424550)ï¼Œæˆ‘ä»¬åšå°æ±½è½¦æ‘‡å·å€ç‡åˆ†ææ—¶ï¼Œç”¨äº†SparkSessionçš„read APIä»Parquetæ–‡ä»¶åˆ›å»ºDataFrameï¼Œå…¶å®åˆ›å»ºDataFrameçš„æ–¹æ³•è¿˜æœ‰å¾ˆå¤šã€‚æ¯«ä¸å¤¸å¼ åœ°è¯´ï¼ŒDataFrameçš„åˆ›å»ºé€”å¾„å¼‚å¸¸ä¸°å¯Œï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿ

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒSparkæ”¯æŒå¤šç§æ•°æ®æºï¼ŒæŒ‰ç…§æ•°æ®æ¥æºè¿›è¡Œåˆ’åˆ†ï¼Œè¿™äº›æ•°æ®æºå¯ä»¥åˆ†ä¸ºå¦‚ä¸‹å‡ ä¸ªå¤§ç±»ï¼šDriverç«¯è‡ªå®šä¹‰çš„æ•°æ®ç»“æ„ã€ï¼ˆåˆ†å¸ƒå¼ï¼‰æ–‡ä»¶ç³»ç»Ÿã€å…³ç³»å‹æ•°æ®åº“RDBMSã€å…³ç³»å‹æ•°æ®ä»“åº“ã€NoSQLæ•°æ®åº“ï¼Œä»¥åŠå…¶ä»–çš„è®¡ç®—å¼•æ“ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/f9/f6/f99dae173ba0268a8bd486ab200ecdf6.jpg?wh=1920x696 "Sparkæ”¯æŒçš„æ•°æ®æº")

æ˜¾ç„¶ï¼Œè¦æ·±å…¥åœ°ä»‹ç»Sparkä¸æ¯ä¸€ç§æ•°æ®æºçš„é›†æˆå¹¶ä¸ç°å®ï¼Œä¹Ÿæ²¡å¿…è¦ï¼Œå’±ä»¬åªéœ€è¦æŠŠæ³¨æ„åŠ›æ”¾åœ¨é‚£äº›æœ€å¸¸ç”¨ã€æœ€å¸¸è§çš„é›†æˆæ–¹å¼å³å¯ã€‚

è¿™ä¸€è®²ï¼Œæˆ‘ä¼šä»Driverã€æ–‡ä»¶ç³»ç»Ÿä¸RDBMSä¸‰ä¸ªæ–¹é¢ï¼Œä¸ºä½ è®²è§£5ç§å¸¸è§çš„DataFrameåˆ›å»ºæ–¹å¼ï¼Œç„¶åå¸¦ä½ äº†è§£ä¸åŒæ–¹å¼çš„ä½¿ç”¨åœºæ™¯è·Ÿä¼˜åŠ£åˆ†æã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ16ï¼‰</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg" width="30px"><span>welldo</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆ,
å¯¹äº max=â€œmaleâ€åŒæ—¶ min=â€œmaleâ€çš„ gender æ–‡ä»¶æ¥è¯´
è¿™å¥è¯æ²¡æœ‰è¯»æ‡‚,æ±‚è§£é‡Š.</div>2021-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg" width="30px"><span>ç«ç‚ç„±ç‡š</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>Python ä»£ç ï¼š
# ä¸‹é¢æ˜¯æˆ‘åœ¨notebookä¸Šè¿è¡Œçš„ï¼Œå¯ä»¥å¾—åˆ°ç»“æœï¼Œæ‹¿å‡ºæ¥åˆ†äº«

from pyspark import SparkContext, SparkConf
from pyspark.sql.session import SparkSession
sc = SparkContext()
spark = SparkSession(sc)

seq=[(&#39;Bob&#39;,14),(&#39;Alice&#39;,18)]
rdd=sc.parallelize(seq)
column = [&#39;name&#39;,&#39;age&#39;]
dataframe=spark.createDataFrame(seq,column)


seq=[(&#39;Bob&#39;,14),(&#39;Alice&#39;,18)]
rdd=sc.parallelize(seq)
column = [&#39;name&#39;,&#39;age&#39;]
df2=rdd.toDF(column)  

csvFilePath=&#39;file:&#47;&#47;&#47;home&#47;ray&#47;DataSet&#47;learn-spark&#47;chapter15&#47;info.txt&#39;
df=spark.read.format(&#39;csv&#39;).option(&#39;header&#39;,True).load(csvFilePath)

from pyspark.sql.types import (StringType,StructField,StringType,IntegerType)
schema = StructType([
    StructField(&quot;name&quot;, StringType(), True),
    StructField(&quot;age&quot;, IntegerType(), True),
])

df2=spark.read.format(&#39;csv&#39;).option(&#39;header&#39;,True).load(csvFilePath)

df3=spark.read.format(&#39;csv&#39;).schema(schema).option(&#39;header&#39;,True).option(&#39;mode&#39;,&#39;dropMalformed&#39;).load(csvFilePath)

parquetFilePath=&#39;file:&#47;&#47;&#47;home&#47;RawData&#47;lucky&#47;batchNum=201705&#47;part-00000-a4ecpy.parquet&#39;
df1=spark.read.format(&#39;parquet&#39;).load(parquetFilePath)
# or
df2=spark.read.parquet(parquetFilePath)

from pyspark import SparkContext, SparkConf
from pyspark.sql.session import SparkSession
sc = SparkContext()
spark = SparkSession(sc)

df=spark.read.format(&quot;jdbc&quot;).option(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;).option(&quot;url&quot;, &quot;jdbc:mysql:&#47;&#47;localhost:3306&#47;DBName&quot;).option(&quot;user&quot;, &quot;username&quot;).option(&quot;password&quot;,&quot;password&quot;).option(&quot;numPartitions&quot;, 20).option(&quot;dbtable&quot;, &quot;tableName&quot;).load()

sqlQuery=&#39;(select * from django_content_type where app_label = &quot;auth&quot;)T&#39;
df2=spark.read.format(&quot;jdbc&quot;).option(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;).option(&quot;url&quot;, &quot;jdbc:mysql:&#47;&#47;localhost:3306&#47;DBName&quot;).option(&quot;user&quot;, &quot;username&quot;).option(&quot;password&quot;,&quot;password&quot;).option(&quot;numPartitions&quot;, 20).option(&quot;dbtable&quot;, sqlQuery).load()</div>2021-10-23</li><br/><li><img src="" width="30px"><span>å§šç¤¼åš</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼ŒRDD[Row]æ€ä¹ˆç†è§£ï¼Œå’ŒRDD[String]è¿™äº›æœ‰å•¥åŒºåˆ«å‘¢</div>2021-12-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg" width="30px"><span>GACÂ·DU</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼Œæ„Ÿè§‰æœ€è¿‘æ›´æ–°çš„Spark SQLæ²¡æœ‰Spark coreæœ‰æ·±åº¦äº†ï¼Ÿæ˜¯æˆ‘ç›´è§‰æ€§é”™è¯¯ï¼Œè¿˜æ˜¯æˆ‘æ²¡æœ‰getåˆ°è€å¸ˆèŠçš„ç‚¹å‘¢</div>2021-10-13</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/H8NxrljQXlibw5tznwNYgqp9WSJicDIB8Bn9MygzFD0jn6ycBfkPDnDEcoEbuh2C3N6fCSAlvWV9wuA5KFa5yMuQ/132" width="30px"><span>Geek_f09d5e</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>å‘ç°å°å¤±è¯¯ï¼ŒCSVæ–‡ä»¶åˆ†éš”åˆ—æ•°æ®çš„åˆ†éš”ç¬¦ä¸æ˜¯â€œseqâ€ï¼Œæ˜¯â€œsepâ€</div>2022-02-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/d3/0a/92640aae.jpg" width="30px"><span>æˆ‘çˆ±å¤œæ¥é¦™</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆ,æˆ‘è¿æ¥oracleæ—¶dbtableé€‰é¡¹æŒ‡å®šè¡¨åæˆåŠŸäº†,æŒ‡å®šçš„æ˜¯ä¸ªsqlqueryæŠ¥é”™:è¡¨åæ— æ•ˆ</div>2022-01-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/7d/57/c94b6a93.jpg" width="30px"><span>ç‹ç’€ç’¨</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆ sqlè¯­å¥æŸ¥è¯¢åé¢é»˜è®¤åŠ where 1=0çš„æŠ¥é”™:
java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;select * from HaoDF_article limit 2000 WHERE 1=0&#39; at line 1</div>2021-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/7d/57/c94b6a93.jpg" width="30px"><span>ç‹ç’€ç’¨</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>è€å¸ˆï¼Œsqlè¯­å¥åé¢é»˜è®¤åŠ where 1=0çš„æŠ¥é”™
java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;select * from HaoDF_article limit 2000 WHERE 1=0&#39; at line 1</div>2021-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/7d/57/c94b6a93.jpg" width="30px"><span>ç‹ç’€ç’¨</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆè¯·é—®ä¸€ä¸‹spark.default.parallelismå’ŒnumPartitionsåˆ†åˆ«æ˜¯ä»€ä¹ˆæ„æ€ï¼Œæˆ‘çœ‹éƒ½æœ‰åˆ†å—çš„æ„æ€ï¼Œä½†æ˜¯ä¸æ‡‚å…¶ä¸­çš„åŸç†ã€‚</div>2021-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/20/ba/c6/10448065.jpg" width="30px"><span>ä¸œå›´å±…å£«</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æˆ‘åœ¨ spark-shell ä¸­ä½¿ç”¨ failFast æ¨¡å¼çš„æ—¶å€™ï¼Œå¹¶ä¸ä¼šåœ¨ load è¯­å¥å¤„å‡ºé”™ï¼Œè€Œæ˜¯åœ¨æ‰§è¡Œ df.show çš„æ—¶å€™æ‰ä¼šæŠ¥é”™</div>2021-10-29</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/w74m73icotZZEiasC6VzRUytfkFkgyYCGAcz16oBWuMXueWOxxVuAnH6IHaZFXkj5OqwlVO1fnocvn9gGYh8gGcw/132" width="30px"><span>Geek_995b78</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼Œæˆ‘åœ¨ç”¨spark sql è¯»å–mysqlæ•°æ®çš„æ—¶å€™ï¼Œä¼šåœ¨sqlåé»˜è®¤åŠ  where 1=0è¯­å¥ï¼Œå¯¼è‡´ç¨‹åºæŠ¥é”™ï¼Œè¿™ä¸ªæ€ä¹ˆè§£å†³å‘¢ï¼Ÿ</div>2021-10-18</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaEIQpPwtelwwDve2HDXszyDMR4e9iaw9mVJ1EdQy6WQO9pQibZ43TO7KE6wdo5n6lMXNRxqLRAjgP1Ng/132" width="30px"><span>czuo03</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>permissiveä¼šå°†sixæ›¿æ¢ä¸ºnull
val df: DataFrame = spark.read.format(&quot;csv&quot;).schema(schema).option(&quot;header&quot;, true).option(&quot;mode&quot;, &quot;permissive&quot;).load(csvFilePath)
+------+----+
|  name| age|
+------+----+
| alice|  18|
|   bob|  14|
|cassie|null|
+------+----+

dropMalFormedè·³è¿‡cassie, sixè¿™æ¡è®°å½•
val df: DataFrame = spark.read.format(&quot;csv&quot;).schema(schema).option(&quot;header&quot;, true).option(&quot;mode&quot;, &quot;dropMalFormed&quot;).load(csvFilePath)
+-----+---+
| name|age|
+-----+---+
|alice| 18|
|  bob| 14|
+-----+---+

failFastæ— æ³•ç”ŸæˆDataFrameå¹¶æç¤ºSparkException
val df: DataFrame = spark.read.format(&quot;csv&quot;).schema(schema).option(&quot;header&quot;, true).option(&quot;mode&quot;, &quot;failFast&quot;).load(csvFilePath)
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option &#39;mode&#39; as &#39;PERMISSIVE&#39;.</div>2021-10-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f8/99/8e760987.jpg" width="30px"><span>è¨±æ•²æ•²</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>è€å¸ˆæœ‰æ²¡æœ‰ä»€ä¹ˆèµ„æ–™æˆ–è€…å­¦ä¹ æ–¹æ³•æ¥å­¦ä¹ å®ç°è‡ªå·±çš„æ•°æ®æºconnector</div>2024-11-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/d8/e5/bb87e21f.jpg" width="30px"><span>ç©Œæ»‚</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>è¯·é—®è€å¸ˆï¼Œreadæ˜¯actionsç®—å­è¿˜æ˜¯transformationsç®—å­ï¼Ÿ</div>2022-06-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2d/36/1c/adfeb6c4.jpg" width="30px"><span>çˆ±å­¦ä¹ çš„ç‹å‘±å‘±</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>è€å¸ˆæˆ‘æƒ³é—®ä¸‹ï¼Œæˆ‘çš„parquetæ–‡ä»¶è¿‡å¤§ï¼Œè¢«åˆ‡åˆ†æˆäº†è‹¥å¹²ä¸ªå°æ–‡ä»¶ï¼Œè¿™äº›å°æ–‡ä»¶éƒ½åœ¨åŒä¸€ä¸ªhdfsç›®å½•ä¸‹ã€‚è¿™ç§æƒ…å†µä¸‹æ€ä¹ˆè¯»å–å‘¢ï¼Ÿ</div>2022-05-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4c/d2/f12dd0ac.jpg" width="30px"><span>ç¿¡ç¿ å°å—ç“œ</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>jdbcçš„numPartitionsæœ‰ä»€ä¹ˆä½œç”¨ï¼Œæœ€å¤§åˆ†åŒºå•¥æ„æ€ï¼Ÿæ•°æ®åº“çš„è¡¨ä¸æ˜¯åˆ†åŒºè¡¨ï¼Œå®ƒæœ‰ç”¨å—</div>2022-04-14</li><br/>
</ul>