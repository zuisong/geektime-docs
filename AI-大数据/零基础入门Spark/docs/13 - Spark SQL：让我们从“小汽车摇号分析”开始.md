ä½ å¥½ï¼Œæˆ‘æ˜¯å´ç£Šã€‚

åœ¨å¼€ç¯‡è¯æˆ‘ä»¬æå‡ºâ€œå…¥é—¨Sparkéœ€è¦ä¸‰æ­¥èµ°â€ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æºæ‰‹å¹¶è‚©è·¨è¶Šäº†å‰é¢ä¸¤æ­¥ï¼Œé¦–å…ˆæ­å–œä½ å­¦åˆ°è¿™é‡Œï¼ç†Ÿç»ƒæŒæ¡äº†Sparkå¸¸ç”¨ç®—å­ä¸æ ¸å¿ƒåŸç†ä»¥åï¼Œä½ å·²ç»å¯ä»¥è½»æ¾åº”å¯¹å¤§éƒ¨åˆ†æ•°æ®å¤„ç†éœ€æ±‚äº†ã€‚

ä¸è¿‡ï¼Œæ•°æ®å¤„ç†æ¯•ç«Ÿæ˜¯æ¯”è¾ƒåŸºç¡€çš„æ•°æ®åº”ç”¨åœºæ™¯ï¼Œå°±åƒèµ›è½¦æœ‰ç€ä¸åŒçš„é©¾é©¶åœºæ™¯ï¼Œæƒ³æˆä¸ºSparkçš„èµ„æ·±èµ›è½¦æ‰‹ï¼Œæˆ‘ä»¬è¿˜è¦èµ°å‡ºç¬¬ä¸‰æ­¥â€”â€”å­¦ä¹ Sparkè®¡ç®—å­æ¡†æ¶ã€‚åªæœ‰å®Œæˆè¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬æ‰èƒ½æŒæ¡Spark SQLï¼ŒStructured Streamingå’ŒSpark MLlibçš„å¸¸è§„å¼€å‘æ–¹æ³•ï¼Œæ¸¸åˆƒæœ‰ä½™åœ°åº”å¯¹ä¸åŒçš„æ•°æ®åº”ç”¨åœºæ™¯ï¼Œå¦‚æ•°æ®åˆ†æã€æµè®¡ç®—å’Œæœºå™¨å­¦ä¹ ï¼Œç­‰ç­‰ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/6a/a3/6a56c520ab7666d1bb9dd1f0683346a3.jpg?wh=1920x915 "è¿˜å·®ç¬¬ä¸‰æ­¥")

é‚£è¿™ä¹ˆå¤šå­æ¡†æ¶ï¼Œä»å“ªé‡Œå…¥æ‰‹æ¯”è¾ƒå¥½å‘¢ï¼Ÿåœ¨æ‰€æœ‰çš„å­æ¡†æ¶ä¸­ï¼ŒSpark SQLæ˜¯ä»£ç é‡æœ€å¤šã€Sparkç¤¾åŒºæŠ•å…¥æœ€å¤§ã€åº”ç”¨èŒƒå›´æœ€å¹¿ã€å½±å“åŠ›æœ€æ·±è¿œçš„é‚£ä¸ªã€‚å°±å­æ¡†æ¶çš„å­¦ä¹ æ¥è¯´ï¼Œæˆ‘ä»¬è‡ªç„¶è¦ä»Spark SQLå¼€å§‹ã€‚

ä»Šå¤©æˆ‘ä»¬ä»ä¸€ä¸ªä¾‹å­å…¥æ‰‹ï¼Œåœ¨å®æˆ˜ä¸­å¸¦ä½ ç†Ÿæ‚‰æ•°æ®åˆ†æå¼€å‘çš„æ€è·¯å’Œå®ç°æ­¥éª¤ã€‚æœ‰äº†å¯¹Spark SQLçš„ç›´è§‚ä½“éªŒï¼Œæˆ‘ä»¬åé¢å‡ è®²è¿˜ä¼šæ·±å…¥æ¢è®¨Spark SQLçš„ç”¨æ³•ã€ç‰¹æ€§ä¸ä¼˜åŠ¿ï¼Œè®©ä½ é€æ­¥æŒæ¡Spark SQLçš„å…¨è²Œã€‚

## ä¸šåŠ¡éœ€æ±‚

ä»Šå¤©æˆ‘ä»¬è¦è®²çš„å°ä¾‹å­ï¼Œæ¥è‡ªäºåŒ—äº¬å¸‚å°æ±½è½¦æ‘‡å·ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œä¸ºäº†é™åˆ¶æœºåŠ¨è½¦ä¿æœ‰é‡ï¼Œä»2011å¹´å¼€å§‹ï¼ŒåŒ—äº¬å¸‚æ”¿åºœæ¨å‡ºäº†å°æ±½è½¦æ‘‡å·æ”¿ç­–ã€‚éšç€æ‘‡å·è¿›ç¨‹çš„æ¨è¿›ï¼Œåœ¨2016å¹´ï¼Œä¸ºäº†ç…§é¡¾é‚£äº›é•¿æ—¶é—´æ²¡æœ‰æ‘‡ä¸­å·ç ç‰Œçš„â€œå‡†å¸æœºâ€ï¼Œæ‘‡å·æ”¿ç­–åˆæ¨å‡ºäº†â€œå€ç‡â€åˆ¶åº¦ã€‚

æ‰€è°“å€ç‡åˆ¶åº¦ï¼Œå®ƒæŒ‡çš„æ˜¯ï¼Œç»“åˆå‚ä¸æ‘‡å·æ¬¡æ•°ï¼Œä¸ºæ¯ä¸ªäººèµ‹äºˆä¸åŒçš„å€ç‡ç³»æ•°ã€‚æœ‰äº†å€ç‡åŠ æŒï¼Œå¤§å®¶çš„ä¸­ç­¾ç‡å°±ç”±åŸæ¥æ•´é½åˆ’ä¸€çš„åŸºç¡€æ¦‚ç‡ï¼Œå˜ä¸ºâ€œ**åŸºç¡€æ¦‚ç‡ * å€ç‡ç³»æ•°**â€ã€‚å‚ä¸æ‘‡å·çš„æ¬¡æ•°è¶Šå¤šï¼Œå€ç‡ç³»æ•°è¶Šå¤§ï¼Œä¸­ç­¾ç‡ä¹Ÿä¼šç›¸åº”å¾—åˆ°æé«˜ã€‚

ä¸è¿‡ï¼Œèº«è¾¹æ— æ•°çš„â€œå‡†å¸æœºâ€æ€»æ˜¯è·Ÿæˆ‘è¯´ï¼Œå…¶å®å€ç‡è¿™ç©æ„æ²¡ä»€ä¹ˆç”¨ï¼ŒèƒŒäº†8å€ã€10å€çš„å€ç‡ï¼Œç…§æ ·æ‘‡ä¸ä¸Šï¼é‚£ä¹ˆä»Šå¤©è¿™ä¸€è®²ï¼Œå’±ä»¬å°±æ¥å€Ÿç€å­¦ä¹ Spark SQLçš„æœºä¼šï¼Œç”¨æ•°æ®æ¥ä¸ºè¿™äº›è¿˜æ²¡æ‘¸è¿‡è½¦çš„â€œè€å¸æœºâ€ç­”ç–‘è§£æƒ‘ï¼Œå¸®ä»–ä»¬å®šé‡åœ°åˆ†æä¸€ä¸‹ï¼Œå€ç‡ä¸ä¸­ç­¾ç‡ä¹‹é—´ï¼Œåˆ°åº•æœ‰æ²¡æœ‰å…³ç³»ï¼Ÿ

## å‡†å¤‡å·¥ä½œ

å·§å¦‡éš¾ä¸ºæ— ç±³ä¹‹ç‚Šï¼Œæ—¢ç„¶æ˜¯åšæ•°æ®åˆ†æï¼Œé‚£å’±ä»¬å¾—å…ˆæœ‰æ•°æ®æ‰è¡Œã€‚æˆ‘è¿™è¾¹ä¸ºä½ å‡†å¤‡äº†2011å¹´åˆ°2019å¹´åŒ—äº¬å¸‚å°æ±½è½¦çš„æ‘‡å·æ•°æ®ï¼Œä½ å¯ä»¥é€šè¿‡[è¿™ä¸ªåœ°å€](https://pan.baidu.com/s/1Vys1Z1mofQFoU52ye7SKuw)ï¼Œä»ç½‘ç›˜è¿›è¡Œä¸‹è½½ï¼Œæå–ç ä¸ºajs6ã€‚

è¿™ä»½æ•°æ®çš„æ–‡ä»¶åæ˜¯â€œ2011-2019 å°æ±½è½¦æ‘‡å·æ•°æ®.tar.gzâ€ï¼Œè§£å‹ä¹‹åçš„ç›®å½•ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

å¯ä»¥çœ‹åˆ°ï¼Œæ ¹ç›®å½•ä¸‹æœ‰applyå’Œluckyä¸¤ä¸ªå­ç›®å½•ï¼Œapplyç›®å½•çš„å†…å®¹æ˜¯ 2011-2019 å¹´å„ä¸ªæ‰¹æ¬¡å‚ä¸æ‘‡å·çš„ç”³è¯·å·ç ï¼Œè€Œluckyç›®å½•åŒ…å«çš„æ˜¯å„ä¸ªæ‰¹æ¬¡ä¸­ç­¾çš„ç”³è¯·å·ç ã€‚ä¸ºäº†å™è¿°æ–¹ä¾¿ï¼Œæˆ‘ä»¬æŠŠå‚ä¸è¿‡æ‘‡å·çš„äººå«â€œç”³è¯·è€…â€ï¼ŒæŠŠä¸­ç­¾çš„äººå«â€œä¸­ç­¾è€…â€ã€‚applyå’Œluckyçš„ä¸‹ä¸€çº§å­ç›®å½•æ˜¯å„ä¸ªæ‘‡å·æ‰¹æ¬¡ï¼Œè€Œæ‘‡å·æ‰¹æ¬¡ç›®å½•ä¸‹åŒ…å«çš„æ˜¯Parquetæ ¼å¼çš„æ•°æ®æ–‡ä»¶ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/65/93/659f18d2e1c851byye56553cbcff7b93.jpg?wh=1920x1389 "æ•°æ®çš„ç›®å½•ç»“æ„")

æ•°æ®ä¸‹è½½ã€è§£å‹å®Œæˆä¹‹åï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å†æ¥å‡†å¤‡è¿è¡Œç¯å¢ƒã€‚

å’±ä»¬çš„å°ä¾‹å­æ¯”è¾ƒè½»é‡ï¼ŒScalaç‰ˆæœ¬çš„ä»£ç å®ç°ä¸ä¼šè¶…è¿‡20è¡Œï¼Œå†è€…æ‘‡å·æ•°æ®ä½“é‡å¾ˆå°ï¼Œè§£å‹ä¹‹åçš„Parquetæ–‡ä»¶æ€»å¤§å°ä¹Ÿä¸è¶…è¿‡4Gã€‚

é€‰æ‹©è¿™æ ·çš„ä¾‹å­ä¹Ÿæ˜¯ä¸ºäº†è½»è£…ä¸Šé˜µï¼Œé¿å…ä½ å› ä¸ºç¡¬ä»¶é™åˆ¶è€Œéš¾ä»¥å®éªŒã€‚æƒ³è¦æŠŠç”¨äºåˆ†æå€ç‡çš„åº”ç”¨è·‘èµ·æ¥ï¼Œä½ åœ¨ç¬”è®°æœ¬æˆ–æ˜¯PCä¸Šï¼Œé€šè¿‡å¯åŠ¨æœ¬åœ°spark-shellç¯å¢ƒå°±å¯ä»¥ã€‚ä¸è¿‡ï¼Œå¦‚æœæ¡ä»¶å…è®¸çš„è¯ï¼Œæˆ‘è¿˜æ˜¯é¼“åŠ±ä½ æ­å»ºåˆ†å¸ƒå¼çš„ç‰©ç†é›†ç¾¤ã€‚å…³äºåˆ†å¸ƒå¼é›†ç¾¤çš„æ­å»ºç»†èŠ‚ï¼Œä½ å¯ä»¥å‚è€ƒ[ç¬¬4è®²](https://time.geekbang.org/column/article/419200)ã€‚

å¥½å•¦ï¼Œå‡†å¤‡å¥½æ•°æ®ä¸è¿è¡Œç¯å¢ƒä¹‹åï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ­¥å…¥æ­£é¢˜ï¼Œå»å¼€å‘æ¢ç´¢å€ç‡ä¸ä¸­ç­¾ç‡å…³ç³»çš„æ•°æ®åˆ†æåº”ç”¨å•¦ã€‚

## æ•°æ®æ¢ç´¢

ä¸è¿‡ï¼Œå…ˆåˆ«å¿™ç€ç›´æ¥ä¸Šæ‰‹æ•°æ®åˆ†æã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆè¦å¯¹æ•°æ®æ¨¡å¼ï¼ˆData Schemaï¼‰æœ‰æœ€åŸºæœ¬çš„è®¤çŸ¥ï¼Œä¹Ÿå°±æ˜¯æºæ•°æ®éƒ½æœ‰å“ªäº›å­—æ®µï¼Œè¿™äº›å­—æ®µçš„ç±»å‹å’Œå«ä¹‰åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Œè¿™ä¸€æ­¥å°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„æ•°æ®æ¢ç´¢ã€‚

æ•°æ®æ¢ç´¢çš„æ€è·¯æ˜¯è¿™æ ·çš„ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨SparkSessionçš„read APIè¯»å–æºæ•°æ®ã€åˆ›å»ºDataFrameã€‚ç„¶åï¼Œé€šè¿‡è°ƒç”¨DataFrameçš„showæ–¹æ³•ï¼Œæˆ‘ä»¬å°±å¯ä»¥è½»æ¾è·å–æºæ•°æ®çš„æ ·æœ¬æ•°æ®ï¼Œä»è€Œå®Œæˆæ•°æ®çš„åˆæ­¥æ¢ç´¢ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚

```scala
import org.apache.spark.sql.DataFrame
Â 
val rootPath: String = _
// ç”³è¯·è€…æ•°æ®
val hdfs_path_apply: String = s"${rootPath}/apply"
// sparkæ˜¯spark-shellä¸­é»˜è®¤çš„SparkSessionå®ä¾‹
// é€šè¿‡read APIè¯»å–æºæ–‡ä»¶
val applyNumbersDF: DataFrame = spark.read.parquet(hdfs_path_apply)
// æ•°æ®æ‰“å°
applyNumbersDF.show
Â 
// ä¸­ç­¾è€…æ•°æ®
val hdfs_path_lucky: String = s"${rootPath}/lucky"
// é€šè¿‡read APIè¯»å–æºæ–‡ä»¶
val luckyDogsDF: DataFrame = spark.read.parquet(hdfs_path_lucky)
// æ•°æ®æ‰“å°
luckyDogsDF.show
```

çœ‹åˆ°è¿™é‡Œï¼Œæƒ³å¿…ä½ å·²ç»çœ‰å¤´ç´§é”ï¼šâ€œSparkSessionï¼ŸDataFrameï¼Ÿè¿™äº›éƒ½æ˜¯ä»€ä¹ˆé¬¼ï¼Ÿä½ å¥½åƒå‹æ ¹å„¿ä¹Ÿæ²¡æœ‰æåˆ°è¿‡è¿™äº›æ¦‚å¿µå‘€ï¼â€åˆ«ç€æ€¥ï¼Œå¯¹äºè¿™äº›å…³é”®æ¦‚å¿µï¼Œæˆ‘ä»¬åœ¨åç»­çš„è¯¾ç¨‹ä¸­éƒ½ä¼šé™†ç»­å±•å¼€ï¼Œä»Šå¤©è¿™ä¸€è®²ï¼Œå’±ä»¬å…ˆæ¥â€œçŸ¥å…¶ç„¶â€ï¼Œâ€œçŸ¥å…¶æ‰€ä»¥ç„¶â€çš„éƒ¨åˆ†å’±ä»¬æ”¾åˆ°åé¢å»è®²ã€‚

å¯¹äºSparkSessionï¼Œä½ å¯ä»¥æŠŠå®ƒç†è§£ä¸ºæ˜¯SparkContextçš„è¿›é˜¶ç‰ˆï¼Œæ˜¯Sparkï¼ˆ2.0ç‰ˆæœ¬ä»¥åï¼‰æ–°ä¸€ä»£çš„å¼€å‘å…¥å£ã€‚SparkContexté€šè¿‡textFile APIæŠŠæºæ•°æ®è½¬æ¢ä¸ºRDDï¼Œè€ŒSparkSessioné€šè¿‡read APIæŠŠæºæ•°æ®è½¬æ¢ä¸ºDataFrameã€‚

è€ŒDataFrameï¼Œä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ç§ç‰¹æ®Šçš„RDDã€‚RDDæˆ‘ä»¬å·²ç»å¾ˆç†Ÿæ‚‰äº†ï¼Œç°åœ¨å°±æŠŠDataFrameè·ŸRDDåšä¸ªå¯¹æ¯”ï¼Œè®©ä½ å…ˆå¯¹DataFrameæœ‰ä¸ªæ„Ÿæ€§è®¤è¯†ã€‚

å…ˆä»åŠŸèƒ½åˆ†æï¼Œä¸RDDä¸€æ ·ï¼ŒDataFrameä¹Ÿç”¨æ¥å°è£…åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œå®ƒä¹Ÿæœ‰æ•°æ®åˆ†åŒºçš„æ¦‚å¿µï¼Œä¹Ÿæ˜¯é€šè¿‡ç®—å­æ¥å®ç°ä¸åŒDataFrameä¹‹é—´çš„è½¬æ¢ï¼Œåªä¸è¿‡DataFrameé‡‡ç”¨äº†ä¸€å¥—ä¸RDDç®—å­ä¸åŒçš„ç‹¬ç«‹ç®—å­é›†ã€‚

å†è€…ï¼Œåœ¨æ•°æ®å†…å®¹æ–¹é¢ï¼Œä¸RDDä¸åŒï¼ŒDataFrameæ˜¯ä¸€ç§å¸¦Schemaçš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œå› æ­¤ï¼Œä½ å¯ä»¥ç®€å•åœ°æŠŠDataFrameçœ‹ä½œæ˜¯æ•°æ®åº“ä¸­çš„ä¸€å¼ äºŒç»´è¡¨ã€‚

æœ€åï¼ŒDataFrameèƒŒåçš„è®¡ç®—å¼•æ“æ˜¯Spark SQLï¼Œè€ŒRDDçš„è®¡ç®—å¼•æ“æ˜¯Spark Coreï¼Œè¿™ä¸€ç‚¹è‡³å…³é‡è¦ã€‚ä¸è¿‡ï¼Œå…³äºè®¡ç®—å¼•æ“ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬ç•™åˆ°[ä¸‹ä¸€è®²](https://time.geekbang.org/column/article/425322)å†å»å±•å¼€ã€‚

å¥½å•¦ï¼Œè¨€å½’æ­£ä¼ ã€‚ç®€å•äº†è§£äº†SparkSessionä¸DataFrameçš„æ¦‚å¿µä¹‹åï¼Œæˆ‘ä»¬ç»§ç»­æ¥çœ‹æ•°æ®æ¢ç´¢ã€‚

æŠŠä¸Šè¿°ä»£ç ä¸¢è¿›spark-shellä¹‹åï¼Œåˆ†åˆ«åœ¨applyNumbersDFå’ŒluckyDogsDFè¿™ä¸¤ä¸ªDataFrameä¹‹ä¸Šè°ƒç”¨showå‡½æ•°ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°æ ·æœ¬æ•°æ®ã€‚å¯ä»¥çœ‹åˆ°ï¼Œâ€œè¿™ä¸¤å¼ è¡¨â€çš„Schemaæ˜¯ä¸€æ ·çš„ï¼Œå®ƒä»¬éƒ½åŒ…å«ä¸¤ä¸ªå­—æ®µï¼Œä¸€ä¸ªæ˜¯Stringç±»å‹çš„carNumï¼Œå¦ä¸€ä¸ªæ˜¯ç±»å‹ä¸ºIntçš„batchNumã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/b4/c5/b490801c4fd89yy7d3bab83539bb36c5.jpg?wh=1467x998 "æºæ•°æ®çš„æ ·æœ¬æ•°æ®")

å…¶ä¸­ï¼ŒcarNumçš„å«ä¹‰æ˜¯ç”³è¯·å·ç ã€æˆ–æ˜¯ä¸­ç­¾å·ç ï¼Œè€ŒbatchNumåˆ™ä»£è¡¨æ‘‡å·æ‰¹æ¬¡ï¼Œæ¯”å¦‚201906è¡¨ç¤º2019å¹´çš„æœ€åä¸€æ‰¹æ‘‡å·ï¼Œ201401è¡¨ç¤º2014å¹´çš„ç¬¬ä¸€æ¬¡æ‘‡å·ã€‚

å¥½å•¦ï¼Œè¿›è¡Œåˆ°è¿™é‡Œï¼Œåˆæ­¥çš„æ•°æ®æ¢ç´¢å·¥ä½œå°±å‘Šä¸€æ®µè½äº†ã€‚

## ä¸šåŠ¡éœ€æ±‚å®ç°

å®Œæˆåˆæ­¥çš„æ•°æ®æ¢ç´¢ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥ç»“åˆæ•°æ®ç‰¹ç‚¹ï¼ˆæ¯”å¦‚ä¸¤å¼ è¡¨çš„Schemaå®Œå…¨ä¸€è‡´ï¼Œä½†æ•°æ®å†…å®¹çš„èŒƒç•´ä¸åŒï¼‰ï¼Œæ¥å®ç°æœ€å¼€å§‹çš„ä¸šåŠ¡éœ€æ±‚ï¼šè®¡ç®—ä¸­ç­¾ç‡ä¸å€ç‡ä¹‹é—´çš„é‡åŒ–å…³ç³»ã€‚

é¦–å…ˆï¼Œæ—¢ç„¶æ˜¯è¦é‡åŒ–ä¸­ç­¾ç‡ä¸å€ç‡ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬åªéœ€è¦å…³æ³¨é‚£äº›ä¸­ç­¾è€…ï¼ˆluckyç›®å½•ä¸‹çš„æ•°æ®ï¼‰çš„å€ç‡å˜åŒ–å°±å¥½äº†ã€‚è€Œå€ç‡çš„è®¡ç®—ï¼Œè¦ä¾èµ–applyç›®å½•ä¸‹çš„æ‘‡å·æ•°æ®ã€‚å› æ­¤ï¼Œè¦åšåˆ°ä»…å…³æ³¨ä¸­ç­¾è€…çš„å€ç‡ï¼Œæˆ‘ä»¬å°±å¿…é¡»è¦ä½¿ç”¨æ•°æ®å…³è”è¿™ä¸ªåœ¨æ•°æ®åˆ†æé¢†åŸŸä¸­æœ€å¸¸è§çš„æ“ä½œã€‚æ­¤å¤–ï¼Œç”±äºå€ç‡åˆ¶åº¦è‡ª2016å¹´æ‰å¼€å§‹æ¨å‡ºï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦è®¿é—®2016å¹´ä»¥åçš„æ•°æ®å³å¯ã€‚

åŸºäºä»¥ä¸Šè¿™äº›åˆ†æï¼Œæˆ‘ä»¬å…ˆæŠŠæ•°æ®è¿‡æ»¤ä¸æ•°æ®å…³è”çš„ä»£ç å†™å‡ºæ¥ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```scala
// è¿‡æ»¤2016å¹´ä»¥åçš„ä¸­ç­¾æ•°æ®ï¼Œä¸”ä»…æŠ½å–ä¸­ç­¾å·ç carNumå­—æ®µ
val filteredLuckyDogs: DataFrame = luckyDogsDF.filter(col("batchNum") >= "201601").select("carNum")
Â 
// æ‘‡å·æ•°æ®ä¸ä¸­ç­¾æ•°æ®åšå†…å…³è”ï¼ŒJoin Keyä¸ºä¸­ç­¾å·ç carNum
val jointDF: DataFrame = applyNumbersDF.join(filteredLuckyDogs, Seq("carNum"), "inner")
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨filterç®—å­å¯¹luckyDogsDFåšè¿‡æ»¤ï¼Œç„¶åä½¿ç”¨selectç®—å­æå–carNumå­—æ®µã€‚

ç´§æ¥ç€ï¼Œæˆ‘ä»¬åœ¨applyNumbersDFä¹‹ä¸Šè°ƒç”¨joinç®—å­ï¼Œä»è€Œå®Œæˆä¸¤ä¸ªDataFrameçš„æ•°æ®å…³è”ã€‚joinç®—å­æœ‰3ä¸ªå‚æ•°ï¼Œä½ å¯ä»¥å¯¹ç…§å‰é¢ä»£ç çš„ç¬¬5è¡Œæ¥ç†è§£ï¼Œè¿™é‡Œç¬¬ä¸€ä¸ªå‚æ•°ç”¨äºæŒ‡å®šéœ€è¦å…³è”çš„DataFrameï¼Œç¬¬äºŒä¸ªå‚æ•°ä»£è¡¨Join Keyï¼Œä¹Ÿå°±æ˜¯ä¾æ®å“ªäº›å­—æ®µåšå…³è”ï¼Œè€Œç¬¬ä¸‰ä¸ªå‚æ•°æŒ‡å®šçš„æ˜¯å…³è”å½¢å¼ï¼Œæ¯”å¦‚innerè¡¨ç¤ºå†…å…³è”ï¼Œleftè¡¨ç¤ºå·¦å…³è”ï¼Œç­‰ç­‰ã€‚

åšå®Œæ•°æ®å…³è”ä¹‹åï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å†æ¥è¯´ä¸€è¯´ï¼Œå€ç‡åº”è¯¥æ€ä¹ˆç»Ÿè®¡ã€‚å¯¹äºå€ç‡è¿™ä¸ªæ•°å€¼ï¼Œå®˜æ–¹çš„å®ç°ç•¥æ˜¾ç²—æš´ï¼Œå¦‚æœå»è§‚å¯Ÿ apply ç›®å½•ä¸‹ 2016 å¹´ä»¥åå„ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶ï¼Œä½ å°±ä¼šå‘ç°ï¼Œæ‰€è°“çš„å€ç‡ï¼Œå®é™…ä¸Šå°±æ˜¯ç”³è¯·å·ç çš„å‰¯æœ¬æ•°é‡ã€‚

æ¯”å¦‚è¯´ï¼Œæˆ‘çš„å€ç‡æ˜¯8ï¼Œé‚£ä¹ˆåœ¨å„ä¸ªæ‰¹æ¬¡çš„æ‘‡å·æ–‡ä»¶ä¸­ï¼Œæˆ‘çš„ç”³è¯·å·ç å°±ä¼šå‡ºç°8æ¬¡ã€‚æ˜¯ä¸æ˜¯å¾ˆç²—æš´ï¼Ÿå› æ­¤ï¼Œè¦ç»Ÿè®¡æŸä¸ªç”³è¯·å·ç çš„å€ç‡ï¼Œæˆ‘ä»¬åªéœ€è¦ç»Ÿè®¡å®ƒåœ¨æ‰¹æ¬¡æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°å°±å¯ä»¥è¾¾åˆ°ç›®çš„ã€‚

æŒ‰ç…§æ‰¹æ¬¡ã€ç”³è¯·å·ç åšç»Ÿè®¡è®¡æ•°ï¼Œæ˜¯ä¸æ˜¯æœ‰ç§ç†Ÿæ‚‰çš„æ„Ÿè§‰ï¼Ÿæ²¡é”™ï¼Œè¿™ä¸å°±æ˜¯æˆ‘ä»¬ä¹‹å‰å­¦è¿‡çš„Word Countå—ï¼Ÿå®ƒæœ¬è´¨ä¸Šå…¶å®å°±æ˜¯ä¸€ä¸ªåˆ†ç»„è®¡æ•°çš„è¿‡ç¨‹ã€‚ä¸è¿‡ï¼Œè¿™ä¸€æ¬¡ï¼Œå’±ä»¬ä¸å†ä½¿ç”¨reduceByKeyè¿™ä¸ªRDDç®—å­äº†ï¼Œè€Œæ˜¯ä½¿ç”¨DataFrameçš„é‚£å¥—ç®—å­æ¥å®ç°ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä»£ç ã€‚

```scala
val multipliers: DataFrame = jointDF.groupBy(col("batchNum"),col("carNum"))
.agg(count(lit(1)).alias("multiplier"))
```

åˆ†ç»„è®¡æ•°

å¯¹ç…§ä»£ç æˆ‘ç»™ä½ åˆ†æä¸‹æ€è·¯ï¼Œæˆ‘ä»¬å…ˆæ˜¯ç”¨groupByç®—å­æ¥æŒ‰ç…§æ‘‡å·æ‰¹æ¬¡å’Œç”³è¯·å·ç åšåˆ†ç»„ï¼Œç„¶åé€šè¿‡aggå’Œcountç®—å­æŠŠï¼ˆbatchNumï¼ŒcarNumï¼‰å‡ºç°çš„æ¬¡æ•°ï¼Œä½œä¸ºcarNumåœ¨æ‘‡å·æ‰¹æ¬¡batchNumä¸­çš„å€ç‡ï¼Œå¹¶ä½¿ç”¨aliasç®—å­æŠŠå€ç‡é‡å‘½åä¸ºâ€œmultiplierâ€ã€‚

è¿™ä¹ˆè¯´å¯èƒ½æœ‰ç‚¹ç»•ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨multipliersä¹‹ä¸Šè°ƒç”¨showå‡½æ•°ï¼Œæ¥ç›´è§‚åœ°è§‚å¯Ÿè¿™ä¸€æ­¥çš„è®¡ç®—ç»“æœã€‚ä¸ºäº†æ–¹ä¾¿è¯´æ˜ï¼Œæˆ‘ç”¨è¡¨æ ¼çš„å½¢å¼æ¥è¿›è¡Œç¤ºæ„ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/73/dd/73735ac4ec0bc22f4d79153ae38954dd.jpg?wh=1878x1007 "multipliersè®¡ç®—ç»“æœç¤ºæ„å›¾")

å¯ä»¥çœ‹åˆ°ï¼ŒåŒä¸€ä¸ªç”³è¯·å·ç ï¼Œåœ¨ä¸åŒæ‰¹æ¬¡ä¸­çš„å€ç‡æ˜¯ä¸ä¸€æ ·çš„ã€‚å°±åƒæˆ‘ä»¬ä¹‹å‰è¯´çš„ï¼Œéšç€æ‘‡å·çš„æ¬¡æ•°å¢åŠ ï¼Œå€ç‡ä¹Ÿä¼šè·Ÿç€æå‡ã€‚ä¸è¿‡ï¼Œè¿™é‡Œå’±ä»¬è¦ç ”ç©¶çš„æ˜¯å€ç‡ä¸ä¸­ç­¾ç‡çš„å…³ç³»ï¼Œæ‰€ä»¥åªéœ€è¦å…³å¿ƒä¸­ç­¾è€…æ˜¯åœ¨å¤šå¤§çš„å€ç‡ä¸‹ä¸­ç­¾çš„å°±è¡Œã€‚å› æ­¤ï¼Œå¯¹äºåŒä¸€ä¸ªç”³è¯·å·ç ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿ç•™å…¶ä¸­æœ€å¤§çš„å€ç‡å°±å¯ä»¥äº†ã€‚

éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå–æœ€å¤§å€ç‡çš„åšæ³•ï¼Œä¼šæŠŠå€ç‡çš„ç»Ÿè®¡åŸºæ•°å˜å°ï¼Œä»è€Œå¼•å…¥å¹¸å­˜è€…åå·®ã€‚æ›´ä¸¥è°¨çš„åšæ³•ï¼Œåº”è¯¥æŠŠä¸­ç­¾è€…è¿‡å¾€çš„å€ç‡ä¹Ÿéƒ½ç»Ÿè®¡åœ¨å†…ï¼Œè¿™æ ·å€ç‡çš„åŸºæ•°æ‰æ˜¯å‡†ç¡®çš„ã€‚ä¸è¿‡å‘¢ï¼Œç»“åˆå®éªŒï¼Œå¹¸å­˜è€…åå·®å¹¶ä¸å½±å“â€œå€ç‡ä¸ä¸­ç­¾ç‡æ˜¯å¦æœ‰ç›´æ¥å…³ç³»â€è¿™ä¸€ç»“è®ºã€‚å› æ­¤ï¼Œå’±ä»¬ä¸å¦¨é‡‡ç”¨å–æœ€å¤§å€ç‡è¿™ç§æ›´åŠ ç®€ä¾¿çš„åšæ³•ã€‚æ¯•ç«Ÿï¼Œå­¦ä¹ Spark SQLï¼Œæ‰æ˜¯å’±ä»¬çš„é¦–è¦ç›®æ ‡ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦â€œæŠ¹å»â€batchNumè¿™ä¸ªç»´åº¦ï¼ŒæŒ‰ç…§carNumå¯¹multipliersåšåˆ†ç»„ï¼Œå¹¶æå–å€ç‡çš„æœ€å¤§å€¼ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚

```scala
val uniqueMultipliers: DataFrame = multipliers.groupBy("carNum")
.agg(max("multiplier").alias("multiplier"))
```

åˆ†ç»„èšåˆçš„æ–¹æ³•è·Ÿå‰é¢å·®ä¸å¤šï¼Œæˆ‘ä»¬è¿˜æ˜¯å…ˆç”¨groupByåšåˆ†ç»„ï¼Œä¸è¿‡è¿™æ¬¡ä»…ç”¨carNumä¸€ä¸ªå­—æ®µåšåˆ†ç»„ï¼Œç„¶åä½¿ç”¨aggå’Œmaxç®—å­æ¥ä¿ç•™å€ç‡æœ€å¤§å€¼ã€‚ç»è¿‡è¿™ä¸€æ­¥çš„è®¡ç®—ä¹‹åï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªç”³è¯·å·ç åœ¨ä¸­ç­¾ä¹‹å‰çš„å€ç‡ç³»æ•°ï¼š

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/63/d0/633fc65203b70b8528544a14a09633d0.jpg?wh=1920x506)

å¯ä»¥çœ‹åˆ°ï¼ŒuniqueMultipliersè¿™ä¸ªDataFrameä»…åŒ…å«ç”³è¯·å·ç carNumå’Œå€ç‡multiplierè¿™ä¸¤ä¸ªå­—æ®µï¼Œä¸”carNumå­—æ®µä¸å­˜åœ¨é‡å¤å€¼ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨è¿™ä»½æ•°æ®é›†ä¸­ï¼Œä¸€ä¸ªç”³è¯·å·ç ï¼Œåªæœ‰ä¸€ä¸ªæœ€å¤§å€ç‡ä¸ä¹‹å¯¹åº”ã€‚

å¥½å•¦ï¼Œåˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ‹¿åˆ°äº†æ¯ä¸€ä¸ªä¸­ç­¾è€…ï¼Œåœ¨ä¸­ç­¾ä¹‹å‰çš„å€ç‡ç³»æ•°ã€‚æ¥ä¸‹æ¥ï¼Œç»“åˆè¿™ä»½æ•°æ®ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç»Ÿè®¡å€ç‡æœ¬èº«çš„åˆ†å¸ƒæƒ…å†µã€‚

å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“çš„æ˜¯ï¼Œä¸åŒå€ç‡ä¹‹ä¸‹çš„äººæ•°åˆ†å¸ƒæ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚æ¢å¥è¯è¯´ï¼Œè¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬è¦**æŒ‰ç…§å€ç‡æ¥å¯¹æ•°æ®åšåˆ†ç»„**ï¼Œç„¶åè®¡ç®—ä¸åŒå€ç‡ä¸‹çš„ç»Ÿè®¡è®¡æ•°ã€‚ä¸ç”¨è¯´ï¼Œè¿™æ¬¡å’±ä»¬è¿˜æ˜¯å¾—ä»°ä»—groupByå’Œaggè¿™ä¸¤ä¸ªç®—å­ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚

```scala
val result: DataFrame = uniqueMultipliers.groupBy("multiplier")
.agg(count(lit(1)).alias("cnt"))
.orderBy("multiplier")
Â 
result.collect
```

åœ¨æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬ä¾ç„¶ä½¿ç”¨groupByå’Œaggç®—å­å¦‚æ³•ç‚®åˆ¶ï¼Œå¾—åˆ°æŒ‰ç…§å€ç‡ç»Ÿè®¡çš„äººæ•°åˆ†å¸ƒä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡collectç®—å­æ¥æ”¶é›†è®¡ç®—ç»“æœï¼Œå¹¶åŒæ—¶è§¦å‘ä¸Šè¿°çš„æ‰€æœ‰ä»£ç ä»å¤´è‡³å°¾äº¤ä»˜æ‰§è¡Œã€‚

è®¡ç®—ç»“æœresultåŒ…å«ä¸¤ä¸ªå­—æ®µï¼Œä¸€ä¸ªæ˜¯å€ç‡ï¼Œä¸€ä¸ªæ˜¯æŒæœ‰è¯¥å€ç‡çš„ç»Ÿè®¡äººæ•°ã€‚å¦‚æœæŠŠresultç»“æœæ•°æ®åšæˆæŸ±çŠ¶å›¾çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥æ›´åŠ ç›´è§‚åœ°è§‚å¯Ÿåˆ°ä¸­ç­¾ç‡ä¸å€ç‡ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/41/93/417b1430b64a7c305cb07fb49d3aa993.png?wh=1212x454 "å€ç‡åˆ†å¸ƒ")

ä¸éš¾å‘ç°ï¼Œä¸åŒå€ç‡ä¸‹çš„ä¸­ç­¾è€…äººæ•°ï¼Œå‘ˆç°å‡ºæ­£æ€åˆ†å¸ƒã€‚ä¹Ÿå³æ˜¯è¯´ï¼Œå¯¹äºä¸€ä¸ªç”³è¯·è€…æ¥è¯´ï¼Œä»–/å¥¹æœ‰å¹¸æ‘‡ä¸­çš„æ¦‚ç‡ï¼Œå¹¶ä¸ä¼šéšç€å€ç‡çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ã€‚ç”¨èº«è¾¹é‚£äº›â€œè€å¸æœºâ€çš„è¯è¯´ï¼Œä¸­ç­¾è¿™ä»¶äº‹ï¼Œç¡®å®è·Ÿå€ç‡çš„å…³ç³»ä¸å¤§ã€‚

## é‡ç‚¹å›é¡¾

ä»Šå¤©è¿™ä¸€è®²ï¼Œæˆ‘ä»¬ä¸€èµ·åŠ¨æ‰‹ï¼Œå¼€å‘äº†â€œå€ç‡çš„ç»Ÿè®¡åˆ†å¸ƒâ€è¿™ä¸ªæ•°æ®åˆ†æåº”ç”¨ï¼Œå¹¶è§£ç­”äº†ä¸­ç­¾ç‡ä¸å€ç‡ä¹‹é—´æ˜¯å¦å­˜åœ¨å…³è”å…³ç³»è¿™ä¸€éš¾é¢˜ã€‚

å°½ç®¡åœ¨å®ç°çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸€äº›æ–°æ¦‚å¿µå’Œæ–°çš„ç®—å­ï¼Œä½†ä½ ä¸å¿…æ‹…å¿ƒï¼Œæ›´ä¸å¿…ç€æ€¥ã€‚ä»Šå¤©è¿™èŠ‚è¯¾ï¼Œä½ åªéœ€è¦å¯¹Spark SQLæ¡†æ¶ä¸‹çš„åº”ç”¨å¼€å‘æœ‰ä¸€ä¸ªæ„Ÿæ€§çš„è®¤è¯†å°±å¯ä»¥äº†ã€‚

åœ¨Spark SQLçš„å¼€å‘æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬é€šå¸¸æ˜¯é€šè¿‡SparkSessionçš„read APIä»æºæ•°æ®åˆ›å»ºDataFrameã€‚ç„¶åï¼Œä»¥DataFrameä¸ºå…¥å£ï¼Œåœ¨DataFrameä¹‹ä¸Šè°ƒç”¨å„å¼å„æ ·çš„è½¬æ¢ç®—å­ï¼Œå¦‚aggã€groupByã€selectã€filterç­‰ç­‰ï¼Œå¯¹DataFrameè¿›è¡Œè½¬æ¢ï¼Œè¿›è€Œå®Œæˆç›¸åº”çš„æ•°æ®åˆ†æã€‚

ä¸ºäº†åç»­è¯•éªŒæ–¹ä¾¿ï¼Œæˆ‘æŠŠä»Šå¤©æ¶‰åŠçš„ä»£ç ç‰‡æ®µæ•´ç†åˆ°äº†ä¸€èµ·ï¼Œä½ å¯ä»¥æŠŠå®ƒä»¬ä¸¢è¿›spark-shellå»è¿è¡Œï¼Œè§‚å¯Ÿæ¯ä¸ªç¯èŠ‚çš„è®¡ç®—ç»“æœï¼Œä½“ä¼šä¸åŒç®—å­çš„è®¡ç®—é€»è¾‘ä¸æ‰§è¡Œç»“æœä¹‹é—´çš„å…³ç³»ã€‚åŠ æ²¹ï¼Œç¥ä½ å¥½è¿ï¼

```scala
import org.apache.spark.sql.DataFrame
Â 
val rootPath: String = _
// ç”³è¯·è€…æ•°æ®
val hdfs_path_apply: String = s"${rootPath}/apply"
// sparkæ˜¯spark-shellä¸­é»˜è®¤çš„SparkSessionå®ä¾‹
// é€šè¿‡read APIè¯»å–æºæ–‡ä»¶
val applyNumbersDF: DataFrame = spark.read.parquet(hdfs_path_apply)
Â 
// ä¸­ç­¾è€…æ•°æ®
val hdfs_path_lucky: String = s"${rootPath}/lucky"
// é€šè¿‡read APIè¯»å–æºæ–‡ä»¶
val luckyDogsDF: DataFrame = spark.read.parquet(hdfs_path_lucky)
Â 
// è¿‡æ»¤2016å¹´ä»¥åçš„ä¸­ç­¾æ•°æ®ï¼Œä¸”ä»…æŠ½å–ä¸­ç­¾å·ç carNumå­—æ®µ
val filteredLuckyDogs: DataFrame = luckyDogsDF.filter(col("batchNum") >= "201601").select("carNum")
Â 
// æ‘‡å·æ•°æ®ä¸ä¸­ç­¾æ•°æ®åšå†…å…³è”ï¼ŒJoin Keyä¸ºä¸­ç­¾å·ç carNum
val jointDF: DataFrame = applyNumbersDF.join(filteredLuckyDogs, Seq("carNum"), "inner")
Â 
// ä»¥batchNumã€carNumåšåˆ†ç»„ï¼Œç»Ÿè®¡å€ç‡ç³»æ•°
val multipliers: DataFrame = jointDF.groupBy(col("batchNum"),col("carNum"))
.agg(count(lit(1)).alias("multiplier"))
Â 
// ä»¥carNumåšåˆ†ç»„ï¼Œä¿ç•™æœ€å¤§çš„å€ç‡ç³»æ•°
val uniqueMultipliers: DataFrame = multipliers.groupBy("carNum")
.agg(max("multiplier").alias("multiplier"))
Â 
// ä»¥multiplierå€ç‡åšåˆ†ç»„ï¼Œç»Ÿè®¡äººæ•°
val result: DataFrame = uniqueMultipliers.groupBy("multiplier")
.agg(count(lit(1)).alias("cnt"))
.orderBy("multiplier")
Â 
result.collect
```

## æ¯è¯¾ä¸€ç»ƒ

1.è„‘æ´æ—¶é—´ï¼šä½ è§‰å¾—æ±½è½¦æ‘‡å·çš„å€ç‡åˆ¶åº¦åº”è¯¥æ€æ ·è®¾è®¡ï¼Œæ‰æ˜¯æœ€åˆç†çš„ï¼Ÿ

2.è¯·åœ¨ä½ çš„Sparkç¯å¢ƒä¸­æŠŠä»£ç è¿è¡Œèµ·æ¥ï¼Œå¹¶ç¡®è®¤æ‰§è¡Œç»“æœæ˜¯å¦ä¸resultä¸€è‡´ã€‚

æ¬¢è¿ä½ åœ¨ç•™è¨€åŒºè·Ÿæˆ‘äº¤æµäº’åŠ¨ï¼Œä¹Ÿæ¨èä½ æŠŠè¿™ä¸€è®²çš„å†…å®¹åˆ†äº«ç»™æ›´å¤šçš„æœ‹å‹ã€åŒäº‹ã€‚æˆ‘ä»¬ä¸‹ä¸€è®²è§ï¼
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>qinsi</span> ğŸ‘ï¼ˆ16ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>sparkæ— å…³ã€‚è®¨è®ºä¸‹æ‘‡å·ã€‚

è¯„è®ºåŒºæœ‰åŒ¿åè¯»è€…è´¨ç–‘æ–‡ä¸­çš„ç»“è®ºã€‚è¿™é‡Œå°è¯•æ¢ä¸ªè§’åº¦ä»£å…¥å…·ä½“çš„æ•°å­—åˆ†æä¸‹ã€‚

ç®€å•èµ·è§ï¼Œå‡è®¾æ¯è½®æ‘‡å·æœ‰1000äººä¸­ç­¾ï¼Œå¹¶ä¸”å€ç‡å’Œè½®æ¬¡ä¸€è‡´ï¼Œå³ç¬¬ä¸€è½®å¤§å®¶éƒ½æ˜¯1å€ï¼Œç¬¬ä¸€è½®æ²¡ä¸­çš„äººåœ¨ç¬¬äºŒè½®å˜ä¸º2å€ï¼Œç¬¬äºŒè½®åˆæ²¡ä¸­çš„äººåˆ°äº†ç¬¬ä¸‰è½®å°±å˜æˆ3å€ï¼Œä¾æ¬¡ç±»æ¨ã€‚

å…ˆçœ‹ç¬¬ä¸€è½®çš„1000ä¸ªä¸­ç­¾è€…ï¼Œæ˜¾ç„¶ä»–ä»¬çš„å€ç‡éƒ½æ˜¯1ï¼Œæ²¡æœ‰å…¶ä»–å€ç‡çš„ä¸­ç­¾è€…ï¼Œè®°ä¸º:

[1000, 0, 0, ...]

å†çœ‹ç¬¬äºŒè½®çš„1000ä¸ªä¸­ç­¾è€…ã€‚ç”±äºæ–°åŠ å…¥çš„ç”³è¯·è€…å€ç‡ä¸º1ï¼Œç¬¬ä¸€è½®æœªä¸­çš„äººå€ç‡ä¸º2ã€‚æŒ‰ç…§å€ç‡æ‘‡å·çš„è¯ï¼ŒæœŸæœ›çš„ç»“æœå°±æ˜¯ï¼Œå€ç‡ä¸º2çš„ä¸­ç­¾äººæ•°æ˜¯å€ç‡ä¸º1çš„ä¸­ç­¾äººæ•°çš„2å€ï¼Œè®°ä¸ºï¼š

[333, 667, 0, 0, ...]

ä»¥æ­¤ç±»æ¨ï¼Œç¬¬ä¸‰è½®å°±æ˜¯ï¼š

[167, 333, 500, 0, 0, ...]

å°è¯•æ‘‡ä¸ª10è½®ï¼Œå¯ä»¥å¾—åˆ°ä¸‹è¡¨ï¼š

[1000, 0, 0, ...]
[333, 667, 0, 0, ...]
[167, 333, 500, 0, 0, ...]
[100, 200, 300, 400, 0, 0, ...]
[67, 133, 200, 267, 333, 0, 0, ...]
[48, 95, 143, 190, 238, 286, 0, 0, ...]
[36, 71, 107, 143, 179, 214, 250, 0, 0, ...]
[28, 56, 83, 111, 139, 167, 194, 222, 0, 0, ...]
[22, 44, 67, 89, 111, 133, 156, 178, 200, 0, 0, ...]
[18, 36, 55, 73, 91, 109, 127, 145, 164, 182, 0, 0, ...]

å¯ä»¥çœ‹åˆ°åœ¨æ¯ä¸€è½®çš„ä¸­ç­¾è€…ä¸­ï¼Œç¡®å®æ˜¯å€ç‡è¶Šé«˜ä¸­ç­¾çš„äººæ•°è¶Šå¤šã€‚

è€Œæ–‡ä¸­çš„ç»Ÿè®¡æ–¹æ³•ï¼Œç›¸å½“äºæŠŠè¿™å¼ è¡¨æŒ‰åˆ—æ±‚å’Œï¼š

[1819, 1635, 1455, 1273, 1091, 909, 727, 545, 364, 182, 0, 0, ...]

å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€æ¡å•è°ƒé€’å‡çš„æ›²çº¿ã€‚ç„¶è€Œå´ä¸èƒ½åƒæ–‡ä¸­ä¸€æ ·å¾—å‡ºâ€œä¸­ç­¾ç‡æ²¡æœ‰éšç€å€ç‡å¢åŠ â€çš„ç»“è®ºã€‚é«˜å€ç‡çš„ä¸­ç­¾äººæ•°æ¯”ä½å€ç‡çš„äººæ•°å°‘ï¼Œæ˜¯å› ä¸ºèƒ½è¾¾åˆ°é«˜å€ç‡çš„äººæœ¬èº«å°±å°‘ã€‚æ¯”å¦‚ä¸Šé¢ä¾‹å­ä¸­ï¼Œ10è½®è¿‡å10å€ç‡çš„ä¸­ç­¾è€…åªæœ‰182äººï¼Œæ˜¯å› ä¸ºå‰9è½®æ²¡æœ‰äººèƒ½è¾¾åˆ°10å€ç‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨ç¬¬ä¸€è½®å°±æœ‰1000ä¸ª1å€ç‡çš„äººä¸­ç­¾ã€‚

è‡³äºæ–‡ä¸­é…å›¾ä¸ºä»€ä¹ˆä¼šæ˜¯ä¸€æ¡ç±»ä¼¼é’Ÿå‹çš„æ›²çº¿ï¼ŒçŒœæµ‹å¯èƒ½ç¬¬ä¸€æ¬¡å¼•å…¥å€ç‡æ‘‡å·çš„æ—¶å€™ï¼Œå°±å·²ç»ç»™ä¸åŒçš„äººåˆ†é…ä¸åŒçš„å€ç‡äº†ï¼Œè€Œä¸æ˜¯å¤§å®¶ä¸€å¼€å§‹éƒ½æ˜¯1å€ç‡ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œå¦‚æœåªå¯¹å5è½®æ±‚å’Œï¼Œå¯ä»¥å¾—åˆ°ï¼š

[152, 302, 455, 606, 758, 909, 727, 545, 364, 182]

è¿™æ ·å°±å’Œæ–‡ä¸­çš„é…å›¾æ¯”è¾ƒæ¥è¿‘äº†ã€‚

æ‰€ä»¥ç»“è®ºå°±æ˜¯è¦éªŒè¯ä¸­ç­¾ç‡å’Œå€ç‡çš„å…³ç³»ï¼Œä¸èƒ½æŒ‰ç…§å€ç‡å»ç´¯åŠ ä¸­ç­¾äººæ•°ï¼Œè€Œæ˜¯è¦çœ‹å•æ¬¡æ‘‡å·ä¸­ä¸åŒå€ç‡çš„ä¸­ç­¾è€…çš„åˆ†å¸ƒã€‚</p>2021-10-12</li><br/><li><span>Alvin-L</span> ğŸ‘ï¼ˆ6ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>```
import os
from pyspark import SparkContext, SparkConf
from pyspark.sql.session import SparkSession
from pyspark.sql.functions import first, collect_list, mean, count, max
import matplotlib.pyplot as plt

def plot(res):
    x = [x[&quot;multiplier&quot;] for x in res]
    y = [y[&quot;cnt&quot;] for y in res]
    plt.figure(figsize=(8, 5), dpi=100)
    plt.xlabel(&#39;å€ç‡&#39;)
    plt.ylabel(&#39;äººæ•°&#39;)
    plt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;] 
    plt.rcParams[&#39;axes.unicode_minus&#39;]=False
    plt.bar(x, y, width=0.5)
    plt.xticks(x)
    plt.show()

# pyæ–‡ä»¶å°±åœ¨é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹
rootPath = os.path.split(os.path.realpath(__file__))[0]

conf = SparkConf()
conf.set(&#39;spark.executor.memory&#39;, &#39;4g&#39;)
conf.set(&#39;spark.driver.memory&#39;, &#39;8g&#39;)
conf.set(&quot;spark.executor.cores&quot;, &#39;4&#39;)
conf.set(&#39;spark.cores.max&#39;, 16)
conf.set(&#39;spark.local.dir&#39;, rootPath)
spark = SparkSession(SparkContext(conf=conf))
# ç”³è¯·è€…æ•°æ®
# Windowsç¯å¢ƒ
# æ³¨æ„ç‚¹1ï¼šå¢åŠ  option(&quot;basePath&quot;, rootPath) é€‰é¡¹
# æ³¨æ„ç‚¹2ï¼šè·¯å¾„ hdfs_path_apply éœ€è¦è¿½åŠ  &#47;*&#47;*.parquet
hdfs_path_apply = rootPath + &quot;&#47;apply&quot;
applyNumbersDF = spark.read.option(&quot;basePath&quot;, rootPath).parquet(
    hdfs_path_apply + &quot;&#47;*&#47;*.parquet&quot;
)
# ä¸­ç­¾è€…æ•°æ®
hdfs_path_lucky = rootPath + &quot;&#47;lucky&quot;
luckyDogsDF = spark.read.option(&quot;basePath&quot;, rootPath).parquet(
    hdfs_path_lucky + &quot;&#47;*&#47;*.parquet&quot;
)
# è¿‡æ»¤2016å¹´ä»¥åçš„ä¸­ç­¾æ•°æ®ï¼Œä¸”ä»…æŠ½å–ä¸­ç­¾å·ç carNumå­—æ®µ
filteredLuckyDogs = (
    luckyDogsDF
    .filter(luckyDogsDF[&quot;batchNum&quot;] &gt;= &quot;201601&quot;)
    .select(&quot;carNum&quot;)
)
# æ‘‡å·æ•°æ®ä¸ä¸­ç­¾æ•°æ®åšå†…å…³è”ï¼ŒJoin Keyä¸ºä¸­ç­¾å·ç carNum
jointDF = applyNumbersDF.join(filteredLuckyDogs, &quot;carNum&quot;, &quot;inner&quot;)
# ä»¥batchNumã€carNumåšåˆ†ç»„ï¼Œç»Ÿè®¡å€ç‡ç³»æ•°
multipliers = (
    jointDF
    .groupBy([&quot;batchNum&quot;, &quot;carNum&quot;])
    .agg(count(&quot;batchNum&quot;).alias(&quot;multiplier&quot;))
)
# ä»¥carNumåšåˆ†ç»„ï¼Œä¿ç•™æœ€å¤§çš„å€ç‡ç³»æ•°
uniqueMultipliers = (
    multipliers
    .groupBy(&quot;carNum&quot;)
    .agg(max(&quot;multiplier&quot;).alias(&quot;multiplier&quot;))
)
# ä»¥multiplierå€ç‡åšåˆ†ç»„ï¼Œç»Ÿè®¡äººæ•°
result = (
    uniqueMultipliers
    .groupBy(&quot;multiplier&quot;)
    .agg(count(&quot;carNum&quot;).alias(&quot;cnt&quot;))
    .orderBy(&quot;multiplier&quot;)
)
result.show(40)
res = result.collect()
# ç”»å›¾
plot(res)
```</p>2021-10-26</li><br/><li><span>ä¸œå›´å±…å£«</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è¡¥ä¸€ä¸ªå®Œæ•´çš„ spark ä»£ç ï¼ˆwindowsç¯å¢ƒï¼‰ï¼š

package spark.basic

import org.apache.spark.sql.functions.{col,count, lit, max}
import org.apache.spark.sql.{DataFrame, SparkSession}

object Chapter13 {
    def main(args: Array[String]): Unit = {

        val spark: SparkSession = SparkSession.builder().master(&quot;local[*]&quot;).appName(&quot;Chapter13&quot;).getOrCreate()
        import spark.implicits._

        val rootPath: String = &quot;E:\\temp\\yaohao_home\\yaohao&quot;
        &#47;&#47; ç”³è¯·è€…æ•°æ®
        val hdfs_path_apply: String = s&quot;${rootPath}&#47;apply&quot;
        &#47;&#47; sparkæ˜¯spark-shellä¸­é»˜è®¤çš„SparkSessionå®ä¾‹
        &#47;&#47; é€šè¿‡read APIè¯»å–æºæ–‡ä»¶
        val applyNumbersDF: DataFrame = spark.read.option(&quot;basePath&quot;, rootPath).parquet(hdfs_path_apply + &quot;&#47;*&#47;*.parquet&quot;)

        &#47;&#47; ä¸­ç­¾è€…æ•°æ®
        val hdfs_path_lucky: String = s&quot;${rootPath}&#47;lucky&quot;
        &#47;&#47; é€šè¿‡read APIè¯»å–æºæ–‡ä»¶
        val luckyDogsDF: DataFrame = spark.read.option(&quot;basePath&quot;, rootPath).parquet(hdfs_path_lucky + &quot;&#47;*&#47;*.parquet&quot;)

        &#47;&#47; è¿‡æ»¤2016å¹´ä»¥åçš„ä¸­ç­¾æ•°æ®ï¼Œä¸”ä»…æŠ½å–ä¸­ç­¾å·ç carNumå­—æ®µ
        val filteredLuckyDogs: DataFrame = luckyDogsDF.filter(col(&quot;batchNum&quot;) &gt;= &quot;201601&quot;).select(&quot;carNum&quot;)

        &#47;&#47; æ‘‡å·æ•°æ®ä¸ä¸­ç­¾æ•°æ®åšå†…å…³è”ï¼ŒJoin Keyä¸ºä¸­ç­¾å·ç carNum
        val jointDF: DataFrame = applyNumbersDF.join(filteredLuckyDogs, Seq(&quot;carNum&quot;), &quot;inner&quot;)

        &#47;&#47; ä»¥batchNumã€carNumåšåˆ†ç»„ï¼Œç»Ÿè®¡å€ç‡ç³»æ•°
        val multipliers: DataFrame = jointDF.groupBy(col(&quot;batchNum&quot;),col(&quot;carNum&quot;))
            .agg(count(lit(1)).alias(&quot;multiplier&quot;))

        &#47;&#47; ä»¥carNumåšåˆ†ç»„ï¼Œä¿ç•™æœ€å¤§çš„å€ç‡ç³»æ•°
        val uniqueMultipliers: DataFrame = multipliers.groupBy(&quot;carNum&quot;)
            .agg(max(&quot;multiplier&quot;).alias(&quot;multiplier&quot;))

        &#47;&#47; ä»¥multiplierå€ç‡åšåˆ†ç»„ï¼Œç»Ÿè®¡äººæ•°
        val result: DataFrame = uniqueMultipliers.groupBy(&quot;multiplier&quot;)
            .agg(count(lit(1)).alias(&quot;cnt&quot;))
            .orderBy(&quot;multiplier&quot;)

        result.collect
        result.show()
    }
}
</p>2021-11-12</li><br/><li><span>ç«ç‚ç„±ç‡š</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>å¯¹åº”çš„pythonä»£ç ä¸ºï¼š

# åœ¨notebookä¸Šè¿è¡Œæ—¶ï¼ŒåŠ ä¸Šä¸‹é¢çš„é…ç½®
from pyspark import SparkContext, SparkConf
from pyspark.sql.session import SparkSession

sc_conf = SparkConf() # sparkå‚æ•°é…ç½®
# sc_conf.setMaster()
# sc_conf.setAppName(&#39;my-app&#39;)
sc_conf.set(&#39;spark.executor.memory&#39;, &#39;2g&#39;) 
sc_conf.set(&#39;spark.driver.memory&#39;, &#39;4g&#39;) 
sc_conf.set(&quot;spark.executor.cores&quot;, &#39;2&#39;) 
sc_conf.set(&#39;spark.cores.max&#39;, 20)    
sc = SparkContext(conf=sc_conf)

# åŠ è½½æ•°æ®ï¼Œè½¬æ¢æˆdataframe
rootPath=&#39;~~&#47;RawData&#39;
hdfs_path_apply=rootPath+&#39;&#47;apply&#39;
spark = SparkSession(sc)
applyNumbersDF=spark.read.parquet(hdfs_path_apply)
# applyNumbersDF.show() # æ‰“å°å‡ºå‰å‡ è¡Œæ•°æ®ï¼ŒæŸ¥çœ‹æ•°æ®ç»“æ„

hdfs_path_lucky=rootPath+&#39;&#47;lucky&#39;
luckyDogsDF=spark.read.parquet(hdfs_path_lucky)
# luckyDogsDF.show()

filteredLuckyDogs=luckyDogsDF.filter(luckyDogsDF[&#39;batchNum&#39;]&gt;=&#39;201601&#39;).select(&#39;carNum&#39;)
jointDF=applyNumbersDF.join(filteredLuckyDogs,&#39;carNum&#39;,&#39;inner&#39;)
# joinå‡½æ•°æ¶ˆè€—å†…å­˜è¾ƒå¤§ï¼Œå®¹æ˜“å‡ºç°OOMé”™è¯¯ï¼Œå¦‚æœå‡ºé”™ï¼Œè¦å°†spark.driver.memoryè°ƒå¤§
# jointDF.show() # æ‰“å°å‡ºjoinä¹‹åçš„dféƒ¨åˆ†æ•°æ®

# è¿›è¡Œå¤šç§groupByæ“ä½œ
from pyspark.sql import functions as f
multipliers=jointDF.groupBy([&#39;batchNum&#39;,&#39;carNum&#39;]).agg(f.count(&#39;batchNum&#39;).alias(&quot;multiplier&quot;))
# multipliers.show()

uniqueMultipliers=multipliers.groupBy(&#39;carNum&#39;).agg(f.max(&#39;multiplier&#39;).alias(&#39;multiplier&#39;))
# uniqueMultipliers.show()

result=uniqueMultipliers.groupBy(&#39;multiplier&#39;).agg(f.count(&#39;carNum&#39;).alias(&#39;cnt&#39;)).orderBy(&#39;multiplier&#39;)
result2=result.collect()

# ç»˜å›¾
import matplotlib.pyplot as plt
x=[i[&#39;multiplier&#39;] for i in result2]
y=[i[&#39;cnt&#39;] for i in result2]
plt.bar(x,y)</p>2021-10-23</li><br/><li><span>Geek_d447af</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>æ–‡ç« é‡Œçš„ä»£ç éœ€è¦åœ¨ Hadoop ç¯å¢ƒæ‰èƒ½è·‘èµ·æ¥ï¼Œspark æœ¬èº«ä¸æ”¯æŒè§£æ parquet æ–‡ä»¶</p>2021-10-09</li><br/><li><span>lightning_å¥³å·«</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æˆ‘åœ¨æœ¬åœ°è·‘è¿™ä¸ªä»£ç ç¢°åˆ°äº†å¦‚ä¸‹é”™è¯¯ï¼Œè¯·é—®å¦‚ä½•è§£å†³ï¼Ÿ
22&#47;01&#47;28 15:13:22 ERROR BypassMergeSortShuffleWriter: Error while deleting file &#47;private&#47;var&#47;folders&#47;hk&#47;7j9sqdtn55j3cq_gv5qvp5pm39d49n&#47;T&#47;blockmgr-88ef94e9-943a-4971-a3a8-33d25949886f&#47;1a&#47;temp_shuffle_e0e163fb-852c-4298-b08e-dc4989277ab3
22&#47;01&#47;28 15:13:22 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file &#47;private&#47;var&#47;folders&#47;hk&#47;7j9sqdtn55j3cq_gv5qvp5pm39d49n&#47;T&#47;blockmgr-88ef94e9-943a-4971-a3a8-33d25949886f&#47;08&#47;temp_shuffle_6c160c23-3395-445f-be03-b29a375e1139
java.io.FileNotFoundException: &#47;private&#47;var&#47;folders&#47;hk&#47;7j9sqdtn55j3cq_gv5qvp5pm39d49n&#47;T&#47;blockmgr-88ef94e9-943a-4971-a3a8-33d25949886f&#47;08&#47;temp_shuffle_6c160c23-3395-445f-be03-b29a375e1139 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:217)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1369)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:214)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:105)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)</p>2022-01-28</li><br/><li><span>ä¸œå›´å±…å£«</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>è€å¸ˆï¼Œæ•°æ®æ–‡ä»¶æ–¹ä¾¿å­˜ä¸€ä»½åˆ°åˆ«çš„åœ°æ–¹å—ï¼Œæ¯”å¦‚é©¬äº‘å®¶çš„ç½‘ç›˜ï¼Œæˆ–è€…åšä¸ªç§å­ä¸‹è½½ä»€ä¹ˆçš„ï¼Œç™¾åº¦ç½‘ç›˜é‚£é€Ÿåº¦çœŸçš„æ˜¯ï¼Œæˆ‘ä¸‹åˆ°ä¸‹åˆä¸‹ç­è¿‡å‘¨æœ«éƒ½ä¸‹ä¸å®Œ</p>2021-10-22</li><br/><li><span>Geek_995b78</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>ç”¨scalaå®ç°ï¼Œlit(1)æ˜¯ä»€ä¹ˆæ„æ€å‘€</p>2021-10-11</li><br/><li><span>GACÂ·DU</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>resultå…·ä½“æ•°å€¼ï¼š
scala&gt; result.collect
res7: Array[org.apache.spark.sql.Row] = Array([1,8967], [2,19174], [3,26952], [4,29755], [5,32988], [6,34119], [7,29707], [8,26123], [9,19476], [10,9616], [11,3930], [12,1212])</p>2021-10-08</li><br/><li><span>Neo-dqy</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ã€.agg(count(lit(1)).alias(&quot;cnt&quot;))ã€‘é—®ä¸‹è€å¸ˆï¼Œè¿™é‡Œcountä¸­çš„lit(1)æ˜¯ä»€ä¹ˆæ„æ€å•Šï¼Ÿ
å¯¹äºæ±½è½¦æ‘‡å·çš„å€ç‡åˆ¶åº¦ï¼Œå¦‚æœä¸ºäº†ä¼˜å…ˆè®©å€ç‡é«˜çš„äººæ‘‡åˆ°å·ï¼Œå¯ä»¥æŠŠæ¯ä¸€æœŸçš„èµ„æ ¼åˆ†å¤šæ¬¡æŠ½å–ã€‚å°±æ˜¯è¯´ï¼Œå…ˆæ„å»ºä¸€ä¸ªæ‰€æœ‰äººéƒ½åœ¨é‡Œé¢çš„æ ·æœ¬ï¼ŒæŠ½éƒ¨åˆ†äººï¼›å†å°†å€ç‡é«˜äºæŸä¸ªé˜ˆå€¼çš„äººéƒ½å–å‡ºæ¥ï¼Œæ„å»ºä¸€ä¸ªæ–°çš„æ ·æœ¬ï¼Œå†æŠ½å–éƒ¨åˆ†äººã€‚ï¼ˆå…·ä½“åˆ’åˆ†æˆå‡ ä¸ªæ ·æœ¬å¯ä»¥æŒ‰å€ç‡çš„äººæ•°åˆ†å¸ƒæ¥åˆ’åˆ†ï¼‰å½“ç„¶è¿™æ ·åˆä¼šå¯¹æ–°æ¥çš„äººä¸å…¬å¹³ï¼Œæ‰€ä»¥å¤§å®¶è¿˜æ˜¯æŒ¤åœ°é“å§~~</p>2021-10-08</li><br/><li><span>Spoon</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>Javaå®ç°
https:&#47;&#47;github.com&#47;Spoon94&#47;spark-practice&#47;blob&#47;master&#47;src&#47;main&#47;java&#47;com&#47;spoon&#47;spark&#47;sql&#47;CarNumAnalyseJob.java</p>2022-04-05</li><br/><li><span>æœªæ¥å·²æ¥</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>å¤§æ¦‚çœ‹äº†ä¸‹è¯„è®ºï¼Œå‘ç°ä¸€æ¬¡æ‘‡å·ä¸€ä¸ªå·ç ä¼šå‡ºç°å¤šæ¬¡ï¼Œæ˜¯ä¸ºäº†å¢åŠ næ¬¡å‚ä¸çš„äººè¢«æ‘‡åˆ°çš„æ¦‚ç‡ã€‚ç›¸å½“äºåœ¨ä¸€ä¸ªå°é—­çš„ç®±å­é‡Œæ‘‡çƒï¼Œä¸€ä¸ªå·ç çš„çƒå¤šæ”¾äº†å‡ ä¸ªï¼Œæ‘‡ç®±å­åä¸ªæ•°å¤šçš„å·ç è¢«æŠ½åˆ°çš„æ¦‚ç‡æ›´é«˜ï¼ˆn&#47;Nï¼ŒNä¸ºç®±å­å†…çƒçš„æ€»æ•°ï¼‰</p>2023-02-10</li><br/><li><span>ç¿¡ç¿ å°å—ç“œ</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>ä¸æ‡‚åŒ—äº¬çš„æ‘‡å·è§„åˆ™ï¼Œä¹Ÿæ²¡å†™æ¸…æ¥šï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªæ‰¹æ¬¡å·é‡Œé¢ï¼Œä¸€ä¸ªç”³è¯·å·å¯ä»¥æœ‰å¤šæ¬¡ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ</p>2022-04-12</li><br/><li><span>Each</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<p>è€å¸«æ‚¨å¥½, ç„¡æ³•ä¸‹è¼‰ dataset, å¯ä»¥æä¾›æµ·å¤–è¼‰é»å—? è¬è¬.</p>2024-10-15</li><br/><li><span>é£ä¸€æ ·</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ä¸Šé¢çš„ç»“è®ºæ„Ÿè§‰ä¸å¤ªæ­£ç¡®ï¼Œåç¦»äº†æ¯ä¸ªæ¯ä¸ªå€ç‡ä¸‹çš„åŸºæ•°ï¼Œå€ç‡è¶Šé«˜ä¸­ç­¾æ¦‚ç‡è‚¯å®šè¶Šå¤§å•Šï¼Œå¯¹ä¸ªäººè€Œè¨€ï¼Œå¦‚æœæˆ‘ä»¬æ¯æ¬¡å‚ä¸æŠ½ç­¾çš„äººæ•°å¤§åŸºæ•°ä¸å˜çš„æƒ…å†µä¸‹(åŸºæ•°1000)ï¼Œå€ç‡è¶Šå¤§ï¼Œç›¸å½“äºå¾€é‡Œé¢æ·»åŠ äº†å¤šä¸ªæ ·æœ¬ï¼ˆ8å€ç‡ï¼‰ï¼Œæœ¬æ¥æ˜¯1&#47;1000çš„æ¦‚ç‡å˜ä¸ºäº†8&#47;1000ã€‚ä½†æ˜¯è€ƒè™‘åˆ°æ¯æ¬¡å€ç‡è¶Šå¤§æ²¡ä¸­ç­¾çš„ç”¨æˆ·é‡å¤çš„æ¬¡æ•°è¶Šå¤šï¼ŒåŸºæ•°ä¹Ÿä¼šè·Ÿç€å˜å¤§ï¼Œç›¸å¯¹äºæ–°åŠ å…¥çš„äººæ¥è¯´å…¶å®æ˜¯æœ‰ä¼˜åŠ¿çš„ï¼Œä½†æ˜¯æ–°å¢çš„äººå…¶å®è¿œå°äºå·²ç»å‚ä¸è¿‡æŠ½ç­¾çš„äººï¼Œæ‰€ä»¥æ‰å¯¼è‡´äº†æ„Ÿè§‰ä¸Šå˜åŒ–ä¸å¤§</p>2022-12-01</li><br/>
</ul>