ä½ å¥½ï¼Œæˆ‘æ˜¯å´ç£Šã€‚

åœ¨å¼€ç¯‡è¯æˆ‘ä»¬æå‡ºâ€œå…¥é—¨Sparkéœ€è¦ä¸‰æ­¥èµ°â€ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æºæ‰‹å¹¶è‚©è·¨è¶Šäº†å‰é¢ä¸¤æ­¥ï¼Œé¦–å…ˆæ­å–œä½ å­¦åˆ°è¿™é‡Œï¼ç†Ÿç»ƒæŒæ¡äº†Sparkå¸¸ç”¨ç®—å­ä¸æ ¸å¿ƒåŸç†ä»¥åï¼Œä½ å·²ç»å¯ä»¥è½»æ¾åº”å¯¹å¤§éƒ¨åˆ†æ•°æ®å¤„ç†éœ€æ±‚äº†ã€‚

ä¸è¿‡ï¼Œæ•°æ®å¤„ç†æ¯•ç«Ÿæ˜¯æ¯”è¾ƒåŸºç¡€çš„æ•°æ®åº”ç”¨åœºæ™¯ï¼Œå°±åƒèµ›è½¦æœ‰ç€ä¸åŒçš„é©¾é©¶åœºæ™¯ï¼Œæƒ³æˆä¸ºSparkçš„èµ„æ·±èµ›è½¦æ‰‹ï¼Œæˆ‘ä»¬è¿˜è¦èµ°å‡ºç¬¬ä¸‰æ­¥â€”â€”å­¦ä¹ Sparkè®¡ç®—å­æ¡†æ¶ã€‚åªæœ‰å®Œæˆè¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬æ‰èƒ½æŒæ¡Spark SQLï¼ŒStructured Streamingå’ŒSpark MLlibçš„å¸¸è§„å¼€å‘æ–¹æ³•ï¼Œæ¸¸åˆƒæœ‰ä½™åœ°åº”å¯¹ä¸åŒçš„æ•°æ®åº”ç”¨åœºæ™¯ï¼Œå¦‚æ•°æ®åˆ†æã€æµè®¡ç®—å’Œæœºå™¨å­¦ä¹ ï¼Œç­‰ç­‰ã€‚

![å›¾ç‰‡](https://static001.geekbang.org/resource/image/6a/a3/6a56c520ab7666d1bb9dd1f0683346a3.jpg?wh=1920x915 "è¿˜å·®ç¬¬ä¸‰æ­¥")

é‚£è¿™ä¹ˆå¤šå­æ¡†æ¶ï¼Œä»å“ªé‡Œå…¥æ‰‹æ¯”è¾ƒå¥½å‘¢ï¼Ÿåœ¨æ‰€æœ‰çš„å­æ¡†æ¶ä¸­ï¼ŒSpark SQLæ˜¯ä»£ç é‡æœ€å¤šã€Sparkç¤¾åŒºæŠ•å…¥æœ€å¤§ã€åº”ç”¨èŒƒå›´æœ€å¹¿ã€å½±å“åŠ›æœ€æ·±è¿œçš„é‚£ä¸ªã€‚å°±å­æ¡†æ¶çš„å­¦ä¹ æ¥è¯´ï¼Œæˆ‘ä»¬è‡ªç„¶è¦ä»Spark SQLå¼€å§‹ã€‚

ä»Šå¤©æˆ‘ä»¬ä»ä¸€ä¸ªä¾‹å­å…¥æ‰‹ï¼Œåœ¨å®æˆ˜ä¸­å¸¦ä½ ç†Ÿæ‚‰æ•°æ®åˆ†æå¼€å‘çš„æ€è·¯å’Œå®ç°æ­¥éª¤ã€‚æœ‰äº†å¯¹Spark SQLçš„ç›´è§‚ä½“éªŒï¼Œæˆ‘ä»¬åé¢å‡ è®²è¿˜ä¼šæ·±å…¥æ¢è®¨Spark SQLçš„ç”¨æ³•ã€ç‰¹æ€§ä¸ä¼˜åŠ¿ï¼Œè®©ä½ é€æ­¥æŒæ¡Spark SQLçš„å…¨è²Œã€‚

## ä¸šåŠ¡éœ€æ±‚

ä»Šå¤©æˆ‘ä»¬è¦è®²çš„å°ä¾‹å­ï¼Œæ¥è‡ªäºåŒ—äº¬å¸‚å°æ±½è½¦æ‘‡å·ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œä¸ºäº†é™åˆ¶æœºåŠ¨è½¦ä¿æœ‰é‡ï¼Œä»2011å¹´å¼€å§‹ï¼ŒåŒ—äº¬å¸‚æ”¿åºœæ¨å‡ºäº†å°æ±½è½¦æ‘‡å·æ”¿ç­–ã€‚éšç€æ‘‡å·è¿›ç¨‹çš„æ¨è¿›ï¼Œåœ¨2016å¹´ï¼Œä¸ºäº†ç…§é¡¾é‚£äº›é•¿æ—¶é—´æ²¡æœ‰æ‘‡ä¸­å·ç ç‰Œçš„â€œå‡†å¸æœºâ€ï¼Œæ‘‡å·æ”¿ç­–åˆæ¨å‡ºäº†â€œå€ç‡â€åˆ¶åº¦ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ19ï¼‰</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg" width="30px"><span>qinsi</span> ğŸ‘ï¼ˆ16ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>sparkæ— å…³ã€‚è®¨è®ºä¸‹æ‘‡å·ã€‚

è¯„è®ºåŒºæœ‰åŒ¿åè¯»è€…è´¨ç–‘æ–‡ä¸­çš„ç»“è®ºã€‚è¿™é‡Œå°è¯•æ¢ä¸ªè§’åº¦ä»£å…¥å…·ä½“çš„æ•°å­—åˆ†æä¸‹ã€‚

ç®€å•èµ·è§ï¼Œå‡è®¾æ¯è½®æ‘‡å·æœ‰1000äººä¸­ç­¾ï¼Œå¹¶ä¸”å€ç‡å’Œè½®æ¬¡ä¸€è‡´ï¼Œå³ç¬¬ä¸€è½®å¤§å®¶éƒ½æ˜¯1å€ï¼Œç¬¬ä¸€è½®æ²¡ä¸­çš„äººåœ¨ç¬¬äºŒè½®å˜ä¸º2å€ï¼Œç¬¬äºŒè½®åˆæ²¡ä¸­çš„äººåˆ°äº†ç¬¬ä¸‰è½®å°±å˜æˆ3å€ï¼Œä¾æ¬¡ç±»æ¨ã€‚

å…ˆçœ‹ç¬¬ä¸€è½®çš„1000ä¸ªä¸­ç­¾è€…ï¼Œæ˜¾ç„¶ä»–ä»¬çš„å€ç‡éƒ½æ˜¯1ï¼Œæ²¡æœ‰å…¶ä»–å€ç‡çš„ä¸­ç­¾è€…ï¼Œè®°ä¸º:

[1000, 0, 0, ...]

å†çœ‹ç¬¬äºŒè½®çš„1000ä¸ªä¸­ç­¾è€…ã€‚ç”±äºæ–°åŠ å…¥çš„ç”³è¯·è€…å€ç‡ä¸º1ï¼Œç¬¬ä¸€è½®æœªä¸­çš„äººå€ç‡ä¸º2ã€‚æŒ‰ç…§å€ç‡æ‘‡å·çš„è¯ï¼ŒæœŸæœ›çš„ç»“æœå°±æ˜¯ï¼Œå€ç‡ä¸º2çš„ä¸­ç­¾äººæ•°æ˜¯å€ç‡ä¸º1çš„ä¸­ç­¾äººæ•°çš„2å€ï¼Œè®°ä¸ºï¼š

[333, 667, 0, 0, ...]

ä»¥æ­¤ç±»æ¨ï¼Œç¬¬ä¸‰è½®å°±æ˜¯ï¼š

[167, 333, 500, 0, 0, ...]

å°è¯•æ‘‡ä¸ª10è½®ï¼Œå¯ä»¥å¾—åˆ°ä¸‹è¡¨ï¼š

[1000, 0, 0, ...]
[333, 667, 0, 0, ...]
[167, 333, 500, 0, 0, ...]
[100, 200, 300, 400, 0, 0, ...]
[67, 133, 200, 267, 333, 0, 0, ...]
[48, 95, 143, 190, 238, 286, 0, 0, ...]
[36, 71, 107, 143, 179, 214, 250, 0, 0, ...]
[28, 56, 83, 111, 139, 167, 194, 222, 0, 0, ...]
[22, 44, 67, 89, 111, 133, 156, 178, 200, 0, 0, ...]
[18, 36, 55, 73, 91, 109, 127, 145, 164, 182, 0, 0, ...]

å¯ä»¥çœ‹åˆ°åœ¨æ¯ä¸€è½®çš„ä¸­ç­¾è€…ä¸­ï¼Œç¡®å®æ˜¯å€ç‡è¶Šé«˜ä¸­ç­¾çš„äººæ•°è¶Šå¤šã€‚

è€Œæ–‡ä¸­çš„ç»Ÿè®¡æ–¹æ³•ï¼Œç›¸å½“äºæŠŠè¿™å¼ è¡¨æŒ‰åˆ—æ±‚å’Œï¼š

[1819, 1635, 1455, 1273, 1091, 909, 727, 545, 364, 182, 0, 0, ...]

å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€æ¡å•è°ƒé€’å‡çš„æ›²çº¿ã€‚ç„¶è€Œå´ä¸èƒ½åƒæ–‡ä¸­ä¸€æ ·å¾—å‡ºâ€œä¸­ç­¾ç‡æ²¡æœ‰éšç€å€ç‡å¢åŠ â€çš„ç»“è®ºã€‚é«˜å€ç‡çš„ä¸­ç­¾äººæ•°æ¯”ä½å€ç‡çš„äººæ•°å°‘ï¼Œæ˜¯å› ä¸ºèƒ½è¾¾åˆ°é«˜å€ç‡çš„äººæœ¬èº«å°±å°‘ã€‚æ¯”å¦‚ä¸Šé¢ä¾‹å­ä¸­ï¼Œ10è½®è¿‡å10å€ç‡çš„ä¸­ç­¾è€…åªæœ‰182äººï¼Œæ˜¯å› ä¸ºå‰9è½®æ²¡æœ‰äººèƒ½è¾¾åˆ°10å€ç‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨ç¬¬ä¸€è½®å°±æœ‰1000ä¸ª1å€ç‡çš„äººä¸­ç­¾ã€‚

è‡³äºæ–‡ä¸­é…å›¾ä¸ºä»€ä¹ˆä¼šæ˜¯ä¸€æ¡ç±»ä¼¼é’Ÿå‹çš„æ›²çº¿ï¼ŒçŒœæµ‹å¯èƒ½ç¬¬ä¸€æ¬¡å¼•å…¥å€ç‡æ‘‡å·çš„æ—¶å€™ï¼Œå°±å·²ç»ç»™ä¸åŒçš„äººåˆ†é…ä¸åŒçš„å€ç‡äº†ï¼Œè€Œä¸æ˜¯å¤§å®¶ä¸€å¼€å§‹éƒ½æ˜¯1å€ç‡ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œå¦‚æœåªå¯¹å5è½®æ±‚å’Œï¼Œå¯ä»¥å¾—åˆ°ï¼š

[152, 302, 455, 606, 758, 909, 727, 545, 364, 182]

è¿™æ ·å°±å’Œæ–‡ä¸­çš„é…å›¾æ¯”è¾ƒæ¥è¿‘äº†ã€‚

æ‰€ä»¥ç»“è®ºå°±æ˜¯è¦éªŒè¯ä¸­ç­¾ç‡å’Œå€ç‡çš„å…³ç³»ï¼Œä¸èƒ½æŒ‰ç…§å€ç‡å»ç´¯åŠ ä¸­ç­¾äººæ•°ï¼Œè€Œæ˜¯è¦çœ‹å•æ¬¡æ‘‡å·ä¸­ä¸åŒå€ç‡çš„ä¸­ç­¾è€…çš„åˆ†å¸ƒã€‚</div>2021-10-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/75/ec/c60b29f5.jpg" width="30px"><span>Alvin-L</span> ğŸ‘ï¼ˆ6ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>```
import os
from pyspark import SparkContext, SparkConf
from pyspark.sql.session import SparkSession
from pyspark.sql.functions import first, collect_list, mean, count, max
import matplotlib.pyplot as plt

def plot(res):
    x = [x[&quot;multiplier&quot;] for x in res]
    y = [y[&quot;cnt&quot;] for y in res]
    plt.figure(figsize=(8, 5), dpi=100)
    plt.xlabel(&#39;å€ç‡&#39;)
    plt.ylabel(&#39;äººæ•°&#39;)
    plt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;] 
    plt.rcParams[&#39;axes.unicode_minus&#39;]=False
    plt.bar(x, y, width=0.5)
    plt.xticks(x)
    plt.show()

# pyæ–‡ä»¶å°±åœ¨é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹
rootPath = os.path.split(os.path.realpath(__file__))[0]

conf = SparkConf()
conf.set(&#39;spark.executor.memory&#39;, &#39;4g&#39;)
conf.set(&#39;spark.driver.memory&#39;, &#39;8g&#39;)
conf.set(&quot;spark.executor.cores&quot;, &#39;4&#39;)
conf.set(&#39;spark.cores.max&#39;, 16)
conf.set(&#39;spark.local.dir&#39;, rootPath)
spark = SparkSession(SparkContext(conf=conf))
# ç”³è¯·è€…æ•°æ®
# Windowsç¯å¢ƒ
# æ³¨æ„ç‚¹1ï¼šå¢åŠ  option(&quot;basePath&quot;, rootPath) é€‰é¡¹
# æ³¨æ„ç‚¹2ï¼šè·¯å¾„ hdfs_path_apply éœ€è¦è¿½åŠ  &#47;*&#47;*.parquet
hdfs_path_apply = rootPath + &quot;&#47;apply&quot;
applyNumbersDF = spark.read.option(&quot;basePath&quot;, rootPath).parquet(
    hdfs_path_apply + &quot;&#47;*&#47;*.parquet&quot;
)
# ä¸­ç­¾è€…æ•°æ®
hdfs_path_lucky = rootPath + &quot;&#47;lucky&quot;
luckyDogsDF = spark.read.option(&quot;basePath&quot;, rootPath).parquet(
    hdfs_path_lucky + &quot;&#47;*&#47;*.parquet&quot;
)
# è¿‡æ»¤2016å¹´ä»¥åçš„ä¸­ç­¾æ•°æ®ï¼Œä¸”ä»…æŠ½å–ä¸­ç­¾å·ç carNumå­—æ®µ
filteredLuckyDogs = (
    luckyDogsDF
    .filter(luckyDogsDF[&quot;batchNum&quot;] &gt;= &quot;201601&quot;)
    .select(&quot;carNum&quot;)
)
# æ‘‡å·æ•°æ®ä¸ä¸­ç­¾æ•°æ®åšå†…å…³è”ï¼ŒJoin Keyä¸ºä¸­ç­¾å·ç carNum
jointDF = applyNumbersDF.join(filteredLuckyDogs, &quot;carNum&quot;, &quot;inner&quot;)
# ä»¥batchNumã€carNumåšåˆ†ç»„ï¼Œç»Ÿè®¡å€ç‡ç³»æ•°
multipliers = (
    jointDF
    .groupBy([&quot;batchNum&quot;, &quot;carNum&quot;])
    .agg(count(&quot;batchNum&quot;).alias(&quot;multiplier&quot;))
)
# ä»¥carNumåšåˆ†ç»„ï¼Œä¿ç•™æœ€å¤§çš„å€ç‡ç³»æ•°
uniqueMultipliers = (
    multipliers
    .groupBy(&quot;carNum&quot;)
    .agg(max(&quot;multiplier&quot;).alias(&quot;multiplier&quot;))
)
# ä»¥multiplierå€ç‡åšåˆ†ç»„ï¼Œç»Ÿè®¡äººæ•°
result = (
    uniqueMultipliers
    .groupBy(&quot;multiplier&quot;)
    .agg(count(&quot;carNum&quot;).alias(&quot;cnt&quot;))
    .orderBy(&quot;multiplier&quot;)
)
result.show(40)
res = result.collect()
# ç”»å›¾
plot(res)
```</div>2021-10-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/20/ba/c6/10448065.jpg" width="30px"><span>ä¸œå›´å±…å£«</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>è¡¥ä¸€ä¸ªå®Œæ•´çš„ spark ä»£ç ï¼ˆwindowsç¯å¢ƒï¼‰ï¼š

package spark.basic

import org.apache.spark.sql.functions.{col,count, lit, max}
import org.apache.spark.sql.{DataFrame, SparkSession}

object Chapter13 {
    def main(args: Array[String]): Unit = {

        val spark: SparkSession = SparkSession.builder().master(&quot;local[*]&quot;).appName(&quot;Chapter13&quot;).getOrCreate()
        import spark.implicits._

        val rootPath: String = &quot;E:\\temp\\yaohao_home\\yaohao&quot;
        &#47;&#47; ç”³è¯·è€…æ•°æ®
        val hdfs_path_apply: String = s&quot;${rootPath}&#47;apply&quot;
        &#47;&#47; sparkæ˜¯spark-shellä¸­é»˜è®¤çš„SparkSessionå®ä¾‹
        &#47;&#47; é€šè¿‡read APIè¯»å–æºæ–‡ä»¶
        val applyNumbersDF: DataFrame = spark.read.option(&quot;basePath&quot;, rootPath).parquet(hdfs_path_apply + &quot;&#47;*&#47;*.parquet&quot;)

        &#47;&#47; ä¸­ç­¾è€…æ•°æ®
        val hdfs_path_lucky: String = s&quot;${rootPath}&#47;lucky&quot;
        &#47;&#47; é€šè¿‡read APIè¯»å–æºæ–‡ä»¶
        val luckyDogsDF: DataFrame = spark.read.option(&quot;basePath&quot;, rootPath).parquet(hdfs_path_lucky + &quot;&#47;*&#47;*.parquet&quot;)

        &#47;&#47; è¿‡æ»¤2016å¹´ä»¥åçš„ä¸­ç­¾æ•°æ®ï¼Œä¸”ä»…æŠ½å–ä¸­ç­¾å·ç carNumå­—æ®µ
        val filteredLuckyDogs: DataFrame = luckyDogsDF.filter(col(&quot;batchNum&quot;) &gt;= &quot;201601&quot;).select(&quot;carNum&quot;)

        &#47;&#47; æ‘‡å·æ•°æ®ä¸ä¸­ç­¾æ•°æ®åšå†…å…³è”ï¼ŒJoin Keyä¸ºä¸­ç­¾å·ç carNum
        val jointDF: DataFrame = applyNumbersDF.join(filteredLuckyDogs, Seq(&quot;carNum&quot;), &quot;inner&quot;)

        &#47;&#47; ä»¥batchNumã€carNumåšåˆ†ç»„ï¼Œç»Ÿè®¡å€ç‡ç³»æ•°
        val multipliers: DataFrame = jointDF.groupBy(col(&quot;batchNum&quot;),col(&quot;carNum&quot;))
            .agg(count(lit(1)).alias(&quot;multiplier&quot;))

        &#47;&#47; ä»¥carNumåšåˆ†ç»„ï¼Œä¿ç•™æœ€å¤§çš„å€ç‡ç³»æ•°
        val uniqueMultipliers: DataFrame = multipliers.groupBy(&quot;carNum&quot;)
            .agg(max(&quot;multiplier&quot;).alias(&quot;multiplier&quot;))

        &#47;&#47; ä»¥multiplierå€ç‡åšåˆ†ç»„ï¼Œç»Ÿè®¡äººæ•°
        val result: DataFrame = uniqueMultipliers.groupBy(&quot;multiplier&quot;)
            .agg(count(lit(1)).alias(&quot;cnt&quot;))
            .orderBy(&quot;multiplier&quot;)

        result.collect
        result.show()
    }
}
</div>2021-11-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg" width="30px"><span>ç«ç‚ç„±ç‡š</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>å¯¹åº”çš„pythonä»£ç ä¸ºï¼š

# åœ¨notebookä¸Šè¿è¡Œæ—¶ï¼ŒåŠ ä¸Šä¸‹é¢çš„é…ç½®
from pyspark import SparkContext, SparkConf
from pyspark.sql.session import SparkSession

sc_conf = SparkConf() # sparkå‚æ•°é…ç½®
# sc_conf.setMaster()
# sc_conf.setAppName(&#39;my-app&#39;)
sc_conf.set(&#39;spark.executor.memory&#39;, &#39;2g&#39;) 
sc_conf.set(&#39;spark.driver.memory&#39;, &#39;4g&#39;) 
sc_conf.set(&quot;spark.executor.cores&quot;, &#39;2&#39;) 
sc_conf.set(&#39;spark.cores.max&#39;, 20)    
sc = SparkContext(conf=sc_conf)

# åŠ è½½æ•°æ®ï¼Œè½¬æ¢æˆdataframe
rootPath=&#39;~~&#47;RawData&#39;
hdfs_path_apply=rootPath+&#39;&#47;apply&#39;
spark = SparkSession(sc)
applyNumbersDF=spark.read.parquet(hdfs_path_apply)
# applyNumbersDF.show() # æ‰“å°å‡ºå‰å‡ è¡Œæ•°æ®ï¼ŒæŸ¥çœ‹æ•°æ®ç»“æ„

hdfs_path_lucky=rootPath+&#39;&#47;lucky&#39;
luckyDogsDF=spark.read.parquet(hdfs_path_lucky)
# luckyDogsDF.show()

filteredLuckyDogs=luckyDogsDF.filter(luckyDogsDF[&#39;batchNum&#39;]&gt;=&#39;201601&#39;).select(&#39;carNum&#39;)
jointDF=applyNumbersDF.join(filteredLuckyDogs,&#39;carNum&#39;,&#39;inner&#39;)
# joinå‡½æ•°æ¶ˆè€—å†…å­˜è¾ƒå¤§ï¼Œå®¹æ˜“å‡ºç°OOMé”™è¯¯ï¼Œå¦‚æœå‡ºé”™ï¼Œè¦å°†spark.driver.memoryè°ƒå¤§
# jointDF.show() # æ‰“å°å‡ºjoinä¹‹åçš„dféƒ¨åˆ†æ•°æ®

# è¿›è¡Œå¤šç§groupByæ“ä½œ
from pyspark.sql import functions as f
multipliers=jointDF.groupBy([&#39;batchNum&#39;,&#39;carNum&#39;]).agg(f.count(&#39;batchNum&#39;).alias(&quot;multiplier&quot;))
# multipliers.show()

uniqueMultipliers=multipliers.groupBy(&#39;carNum&#39;).agg(f.max(&#39;multiplier&#39;).alias(&#39;multiplier&#39;))
# uniqueMultipliers.show()

result=uniqueMultipliers.groupBy(&#39;multiplier&#39;).agg(f.count(&#39;carNum&#39;).alias(&#39;cnt&#39;)).orderBy(&#39;multiplier&#39;)
result2=result.collect()

# ç»˜å›¾
import matplotlib.pyplot as plt
x=[i[&#39;multiplier&#39;] for i in result2]
y=[i[&#39;cnt&#39;] for i in result2]
plt.bar(x,y)</div>2021-10-23</li><br/><li><img src="" width="30px"><span>Geek_d447af</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>æ–‡ç« é‡Œçš„ä»£ç éœ€è¦åœ¨ Hadoop ç¯å¢ƒæ‰èƒ½è·‘èµ·æ¥ï¼Œspark æœ¬èº«ä¸æ”¯æŒè§£æ parquet æ–‡ä»¶</div>2021-10-09</li><br/><li><img src="" width="30px"><span>lightning_å¥³å·«</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æˆ‘åœ¨æœ¬åœ°è·‘è¿™ä¸ªä»£ç ç¢°åˆ°äº†å¦‚ä¸‹é”™è¯¯ï¼Œè¯·é—®å¦‚ä½•è§£å†³ï¼Ÿ
22&#47;01&#47;28 15:13:22 ERROR BypassMergeSortShuffleWriter: Error while deleting file &#47;private&#47;var&#47;folders&#47;hk&#47;7j9sqdtn55j3cq_gv5qvp5pm39d49n&#47;T&#47;blockmgr-88ef94e9-943a-4971-a3a8-33d25949886f&#47;1a&#47;temp_shuffle_e0e163fb-852c-4298-b08e-dc4989277ab3
22&#47;01&#47;28 15:13:22 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file &#47;private&#47;var&#47;folders&#47;hk&#47;7j9sqdtn55j3cq_gv5qvp5pm39d49n&#47;T&#47;blockmgr-88ef94e9-943a-4971-a3a8-33d25949886f&#47;08&#47;temp_shuffle_6c160c23-3395-445f-be03-b29a375e1139
java.io.FileNotFoundException: &#47;private&#47;var&#47;folders&#47;hk&#47;7j9sqdtn55j3cq_gv5qvp5pm39d49n&#47;T&#47;blockmgr-88ef94e9-943a-4971-a3a8-33d25949886f&#47;08&#47;temp_shuffle_6c160c23-3395-445f-be03-b29a375e1139 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:217)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1369)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:214)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:105)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)</div>2022-01-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/20/ba/c6/10448065.jpg" width="30px"><span>ä¸œå›´å±…å£«</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>è€å¸ˆï¼Œæ•°æ®æ–‡ä»¶æ–¹ä¾¿å­˜ä¸€ä»½åˆ°åˆ«çš„åœ°æ–¹å—ï¼Œæ¯”å¦‚é©¬äº‘å®¶çš„ç½‘ç›˜ï¼Œæˆ–è€…åšä¸ªç§å­ä¸‹è½½ä»€ä¹ˆçš„ï¼Œç™¾åº¦ç½‘ç›˜é‚£é€Ÿåº¦çœŸçš„æ˜¯ï¼Œæˆ‘ä¸‹åˆ°ä¸‹åˆä¸‹ç­è¿‡å‘¨æœ«éƒ½ä¸‹ä¸å®Œ</div>2021-10-22</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/w74m73icotZZEiasC6VzRUytfkFkgyYCGAcz16oBWuMXueWOxxVuAnH6IHaZFXkj5OqwlVO1fnocvn9gGYh8gGcw/132" width="30px"><span>Geek_995b78</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>ç”¨scalaå®ç°ï¼Œlit(1)æ˜¯ä»€ä¹ˆæ„æ€å‘€</div>2021-10-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg" width="30px"><span>GACÂ·DU</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>resultå…·ä½“æ•°å€¼ï¼š
scala&gt; result.collect
res7: Array[org.apache.spark.sql.Row] = Array([1,8967], [2,19174], [3,26952], [4,29755], [5,32988], [6,34119], [7,29707], [8,26123], [9,19476], [10,9616], [11,3930], [12,1212])</div>2021-10-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/5e/b2/aceb3e41.jpg" width="30px"><span>Neo-dqy</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ã€.agg(count(lit(1)).alias(&quot;cnt&quot;))ã€‘é—®ä¸‹è€å¸ˆï¼Œè¿™é‡Œcountä¸­çš„lit(1)æ˜¯ä»€ä¹ˆæ„æ€å•Šï¼Ÿ
å¯¹äºæ±½è½¦æ‘‡å·çš„å€ç‡åˆ¶åº¦ï¼Œå¦‚æœä¸ºäº†ä¼˜å…ˆè®©å€ç‡é«˜çš„äººæ‘‡åˆ°å·ï¼Œå¯ä»¥æŠŠæ¯ä¸€æœŸçš„èµ„æ ¼åˆ†å¤šæ¬¡æŠ½å–ã€‚å°±æ˜¯è¯´ï¼Œå…ˆæ„å»ºä¸€ä¸ªæ‰€æœ‰äººéƒ½åœ¨é‡Œé¢çš„æ ·æœ¬ï¼ŒæŠ½éƒ¨åˆ†äººï¼›å†å°†å€ç‡é«˜äºæŸä¸ªé˜ˆå€¼çš„äººéƒ½å–å‡ºæ¥ï¼Œæ„å»ºä¸€ä¸ªæ–°çš„æ ·æœ¬ï¼Œå†æŠ½å–éƒ¨åˆ†äººã€‚ï¼ˆå…·ä½“åˆ’åˆ†æˆå‡ ä¸ªæ ·æœ¬å¯ä»¥æŒ‰å€ç‡çš„äººæ•°åˆ†å¸ƒæ¥åˆ’åˆ†ï¼‰å½“ç„¶è¿™æ ·åˆä¼šå¯¹æ–°æ¥çš„äººä¸å…¬å¹³ï¼Œæ‰€ä»¥å¤§å®¶è¿˜æ˜¯æŒ¤åœ°é“å§~~</div>2021-10-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/e7/8e/318cfde0.jpg" width="30px"><span>Spoon</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>Javaå®ç°
https:&#47;&#47;github.com&#47;Spoon94&#47;spark-practice&#47;blob&#47;master&#47;src&#47;main&#47;java&#47;com&#47;spoon&#47;spark&#47;sql&#47;CarNumAnalyseJob.java</div>2022-04-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/1d/3a/cdf9c55f.jpg" width="30px"><span>æœªæ¥å·²æ¥</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>å¤§æ¦‚çœ‹äº†ä¸‹è¯„è®ºï¼Œå‘ç°ä¸€æ¬¡æ‘‡å·ä¸€ä¸ªå·ç ä¼šå‡ºç°å¤šæ¬¡ï¼Œæ˜¯ä¸ºäº†å¢åŠ næ¬¡å‚ä¸çš„äººè¢«æ‘‡åˆ°çš„æ¦‚ç‡ã€‚ç›¸å½“äºåœ¨ä¸€ä¸ªå°é—­çš„ç®±å­é‡Œæ‘‡çƒï¼Œä¸€ä¸ªå·ç çš„çƒå¤šæ”¾äº†å‡ ä¸ªï¼Œæ‘‡ç®±å­åä¸ªæ•°å¤šçš„å·ç è¢«æŠ½åˆ°çš„æ¦‚ç‡æ›´é«˜ï¼ˆn&#47;Nï¼ŒNä¸ºç®±å­å†…çƒçš„æ€»æ•°ï¼‰</div>2023-02-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4c/d2/f12dd0ac.jpg" width="30px"><span>ç¿¡ç¿ å°å—ç“œ</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>ä¸æ‡‚åŒ—äº¬çš„æ‘‡å·è§„åˆ™ï¼Œä¹Ÿæ²¡å†™æ¸…æ¥šï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªæ‰¹æ¬¡å·é‡Œé¢ï¼Œä¸€ä¸ªç”³è¯·å·å¯ä»¥æœ‰å¤šæ¬¡ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ</div>2022-04-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/3c/9f/d7/e393d0ec.jpg" width="30px"><span>Each</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>è€å¸«æ‚¨å¥½, ç„¡æ³•ä¸‹è¼‰ dataset, å¯ä»¥æä¾›æµ·å¤–è¼‰é»å—? è¬è¬.</div>2024-10-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/a3/80/02f4da95.jpg" width="30px"><span>é£ä¸€æ ·</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ä¸Šé¢çš„ç»“è®ºæ„Ÿè§‰ä¸å¤ªæ­£ç¡®ï¼Œåç¦»äº†æ¯ä¸ªæ¯ä¸ªå€ç‡ä¸‹çš„åŸºæ•°ï¼Œå€ç‡è¶Šé«˜ä¸­ç­¾æ¦‚ç‡è‚¯å®šè¶Šå¤§å•Šï¼Œå¯¹ä¸ªäººè€Œè¨€ï¼Œå¦‚æœæˆ‘ä»¬æ¯æ¬¡å‚ä¸æŠ½ç­¾çš„äººæ•°å¤§åŸºæ•°ä¸å˜çš„æƒ…å†µä¸‹(åŸºæ•°1000)ï¼Œå€ç‡è¶Šå¤§ï¼Œç›¸å½“äºå¾€é‡Œé¢æ·»åŠ äº†å¤šä¸ªæ ·æœ¬ï¼ˆ8å€ç‡ï¼‰ï¼Œæœ¬æ¥æ˜¯1&#47;1000çš„æ¦‚ç‡å˜ä¸ºäº†8&#47;1000ã€‚ä½†æ˜¯è€ƒè™‘åˆ°æ¯æ¬¡å€ç‡è¶Šå¤§æ²¡ä¸­ç­¾çš„ç”¨æˆ·é‡å¤çš„æ¬¡æ•°è¶Šå¤šï¼ŒåŸºæ•°ä¹Ÿä¼šè·Ÿç€å˜å¤§ï¼Œç›¸å¯¹äºæ–°åŠ å…¥çš„äººæ¥è¯´å…¶å®æ˜¯æœ‰ä¼˜åŠ¿çš„ï¼Œä½†æ˜¯æ–°å¢çš„äººå…¶å®è¿œå°äºå·²ç»å‚ä¸è¿‡æŠ½ç­¾çš„äººï¼Œæ‰€ä»¥æ‰å¯¼è‡´äº†æ„Ÿè§‰ä¸Šå˜åŒ–ä¸å¤§</div>2022-12-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/3d/6f/3a97712e.jpg" width="30px"><span>æ—Curry</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>val multipliers: DataFrame = jointDF.groupBy(col(&quot;batchNum&quot;),col(&quot;carNum&quot;)).agg(count(lit(1)).alias(&quot;multiplier&quot;))

è€å¸ˆï¼Œæœ‰ç‚¹æ²¡æœ‰çœ‹æ‡‚å€ç‡çš„è®¡ç®—æ–¹å¼ï¼Œä»¥batchNumå’ŒcarNumä¸¤ä¸ªå­—æ®µgroupbyçš„è¯ï¼Œè®¡ç®—çš„ä¸å°±æ˜¯ç”³è¯·å·åœ¨å„ä¸ªæ‰¹æ¬¡ä¸­å‡ºç°çš„æ¬¡æ•°ä¹ˆï¼Ÿ éš¾é“ä¸€ä¸ªç”³è¯·å·è¿˜ä¼šåœ¨åŒä¸ªæ‰¹æ¬¡ä¸­å‡ºç°å¤šæ¬¡å—</div>2022-07-16</li><br/><li><img src="" width="30px"><span>æ¨å¸…</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>è€å¸ˆï¼Œæœ¬åœ°æŠ¥é”™ ç±»ä¼¼Caused by: java.io.IOException: Failed to connect to &#47;192.168.1.3:50561ï¼Œè¿™ä¸ªä¸€èˆ¬æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ</div>2022-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2d/36/1c/adfeb6c4.jpg" width="30px"><span>çˆ±å­¦ä¹ çš„ç‹å‘±å‘±</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>joinçš„æ—¶å€™ä¸€ç›´æç¤ºï¼šã€No offset index for column carNum is available; Unable to do filteringã€‘ç½‘ä¸Šæ²¡æœåˆ°ï¼Œæ±‚å¤§ä½¬ä»¬å¸®å¿™çœ‹çœ‹åŸå› 

sc = SparkContext()
spark = SparkSession(sc)
root_path = &quot;&#47;Users&#47;admin&#47;Documents&#47;2011-2019å°æ±½è½¦æ‘‡å·æ•°æ®&quot;

hdfs_path_apply = root_path + &quot;&#47;apply&quot;
applyDF = spark.read.parquet(hdfs_path_apply).select(&quot;carNum&quot;)
# applyDF.show()

hdfs_path_lucky = root_path + &quot;&#47;lucky&quot;
lucyDF = spark.read.parquet(hdfs_path_lucky)
# lucyDF.show()


lucy_car_num = lucyDF.filter(lucyDF[&#39;batchNum&#39;] &gt;= &#39;201601&#39;).select(&#39;carNum&#39;)
all_car_num = applyDF.join(lucy_car_num, &quot;carNum&quot;, &quot;inner&quot;)
all_car_num.show()</div>2022-04-17</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIfxu4yQEXdQ3icro8QEb0W3Rk014YXqibgw28kAcezVGy0DJzkERd1gXh5uEKL42VnnwomelvgPMDA/132" width="30px"><span>Geek_9d1801</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>apply&#47;batchNum=201905&#47;part-00001-f8bb8e7b-904f-42a7-a616-a413406f06fb.c000.snappy.parquet is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [14, 14, 14, 14]
æ²¡æœ‰äººæŠ¥è¿™ä¸ªé”™è¯¯å—ï¼Ÿæ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ</div>2022-04-07</li><br/>
</ul>