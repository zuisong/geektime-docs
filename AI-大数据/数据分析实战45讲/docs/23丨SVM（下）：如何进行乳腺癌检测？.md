è®²å®Œäº†SVMçš„åŸç†ä¹‹åï¼Œä»Šå¤©æˆ‘æ¥å¸¦ä½ è¿›è¡ŒSVMçš„å®æˆ˜ã€‚

åœ¨æ­¤ä¹‹å‰æˆ‘ä»¬å…ˆæ¥å›é¡¾ä¸€ä¸‹SVMçš„ç›¸å…³çŸ¥è¯†ç‚¹ã€‚SVMæ˜¯æœ‰ç›‘ç£çš„å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦äº‹å…ˆå¯¹æ•°æ®æ‰“ä¸Šåˆ†ç±»æ ‡ç­¾ï¼Œé€šè¿‡æ±‚è§£æœ€å¤§åˆ†ç±»é—´éš”æ¥æ±‚è§£äºŒåˆ†ç±»é—®é¢˜ã€‚å¦‚æœè¦æ±‚è§£å¤šåˆ†ç±»é—®é¢˜ï¼Œå¯ä»¥å°†å¤šä¸ªäºŒåˆ†ç±»å™¨ç»„åˆèµ·æ¥å½¢æˆä¸€ä¸ªå¤šåˆ†ç±»å™¨ã€‚

ä¸Šä¸€èŠ‚ä¸­è®²åˆ°äº†ç¡¬é—´éš”ã€è½¯é—´éš”ã€éçº¿æ€§SVMï¼Œä»¥åŠåˆ†ç±»é—´éš”çš„å…¬å¼ï¼Œä½ å¯èƒ½ä¼šè§‰å¾—æ¯”è¾ƒæŠ½è±¡ã€‚è¿™èŠ‚è¯¾ï¼Œæˆ‘ä»¬ä¼šåœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè®²è§£å¯¹å·¥å…·çš„ä½¿ç”¨ï¼Œä»¥åŠç›¸å…³å‚æ•°çš„å«ä¹‰ã€‚

## å¦‚ä½•åœ¨sklearnä¸­ä½¿ç”¨SVM

åœ¨Pythonçš„sklearnå·¥å…·åŒ…ä¸­æœ‰SVMç®—æ³•ï¼Œé¦–å…ˆéœ€è¦å¼•ç”¨å·¥å…·åŒ…ï¼š

```
from sklearn import svm
```

SVMæ—¢å¯ä»¥åšå›å½’ï¼Œä¹Ÿå¯ä»¥åšåˆ†ç±»å™¨ã€‚

å½“ç”¨SVMåšå›å½’çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨SVRæˆ–LinearSVRã€‚SVRçš„è‹±æ–‡æ˜¯Support Vector Regressionã€‚è¿™ç¯‡æ–‡ç« åªè®²åˆ†ç±»ï¼Œè¿™é‡Œåªæ˜¯ç®€å•åœ°æä¸€ä¸‹ã€‚

å½“åšåˆ†ç±»å™¨çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯SVCæˆ–è€…LinearSVCã€‚SVCçš„è‹±æ–‡æ˜¯Support Vector Classificationã€‚

æˆ‘ç®€å•è¯´ä¸€ä¸‹è¿™ä¸¤è€…ä¹‹å‰çš„å·®åˆ«ã€‚

ä»åå­—ä¸Šä½ èƒ½çœ‹å‡ºLinearSVCæ˜¯ä¸ªçº¿æ€§åˆ†ç±»å™¨ï¼Œç”¨äºå¤„ç†çº¿æ€§å¯åˆ†çš„æ•°æ®ï¼Œåªèƒ½ä½¿ç”¨çº¿æ€§æ ¸å‡½æ•°ã€‚ä¸Šä¸€èŠ‚ï¼Œæˆ‘è®²åˆ°SVMæ˜¯é€šè¿‡æ ¸å‡½æ•°å°†æ ·æœ¬ä»åŸå§‹ç©ºé—´æ˜ å°„åˆ°ä¸€ä¸ªæ›´é«˜ç»´çš„ç‰¹è´¨ç©ºé—´ä¸­ï¼Œè¿™æ ·å°±ä½¿å¾—æ ·æœ¬åœ¨æ–°çš„ç©ºé—´ä¸­çº¿æ€§å¯åˆ†ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ30ï¼‰</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/15/fb/31/f0a884a3.jpg" width="30px"><span>Geek_dancer</span> ğŸ‘ï¼ˆ46ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>é»˜è®¤SVCè®­ç»ƒæ¨¡å‹ï¼Œ6ä¸ªç‰¹å¾å˜é‡ï¼Œè®­ç»ƒé›†å‡†ç¡®ç‡ï¼š96.0%ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡ï¼š92.4%
é»˜è®¤SVCè®­ç»ƒæ¨¡å‹ï¼Œ10ä¸ªç‰¹å¾å˜é‡ï¼Œè®­ç»ƒé›†å‡†ç¡®ç‡ï¼š98.7% ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡ï¼š98.2%
LinearSVCè®­ç»ƒæ¨¡å‹ï¼Œ 6ä¸ªç‰¹å¾å˜é‡ï¼Œ è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š93.9%ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡ï¼š92.3%
LinearSVCè®­ç»ƒæ¨¡å‹ï¼Œ 10ä¸ªç‰¹å¾å˜é‡ï¼Œ è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š99.4%ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡ï¼š96.0%

ç»“è®ºï¼š
1. å¢åŠ ç‰¹å¾å˜é‡å¯ä»¥æé«˜å‡†ç¡®ç‡ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹ç»´åº¦å˜é«˜ï¼Œæ¨¡å‹å˜å¾—æ›´åŠ å¤æ‚ã€‚å¯ä»¥çœ‹å‡ºç‰¹å¾å˜é‡çš„é€‰å–å¾ˆé‡è¦ã€‚
2. è®­ç»ƒé›†æ‹Ÿåˆéƒ½æ¯”è¾ƒå¥½ï¼Œä½†æ˜¯æµ‹è¯•é›†å‡†ç¡®ç‡å‡ºç°ä¸åŒç¨‹åº¦çš„ä¸‹é™ã€‚
3. æ¨¡å‹è®­ç»ƒçš„å‡†ç¡®ç‡ä¸äººç±»æ°´å¹³ä¹‹é—´åå·®å¯ä»¥é€šè¿‡å¢åŠ ç‰¹å¾å˜é‡æˆ–é‡‡ç”¨æ–°çš„è®­ç»ƒæ¨¡å‹æ¥é™ä½ï¼›æ¨¡å‹è®­ç»ƒçš„å‡†ç¡®ç‡ä¸æµ‹è¯•é›†æµ‹è¯•çš„å‡†ç¡®ç‡ä¹‹é—´çš„æ–¹å·®å¯ä»¥é€šè¿‡æ­£åˆ™åŒ–ï¼Œæé«˜æ³›åŒ–æ€§èƒ½ç­‰æ–¹å¼æ¥é™ä½ã€‚</div>2019-02-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg" width="30px"><span>æ»¢</span> ğŸ‘ï¼ˆ14ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>åˆ©ç”¨SVMåšåˆ†ç±»ï¼Œç‰¹å¾é€‰æ‹©å½±å“åº¦å¤§ï¼Œè¦æƒ³SVMåˆ†ç±»å‡†ç¡®ï¼Œäººå·¥å¤„ç†æ•°æ®è¿™ä¸€æ­¥å¾ˆé‡è¦</div>2019-04-18</li><br/><li><img src="" width="30px"><span>hlz-123</span> ğŸ‘ï¼ˆ11ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>é¦–å…ˆè¦è¯´ï¼Œè€å¸ˆçš„è¯¾è®²å¾—éå¸¸å¥½ï¼Œæ·±å¥¥çš„ç®—æ³•å’Œç†è®ºé€šè¿‡ç”ŸåŠ¨æœ‰è¶£çš„ä¾‹å­è®©äººé€šä¿—æ˜“æ‡‚ï¼Œå…´è¶£ç›ç„¶ã€‚
è€å¸ˆçš„æœ¬è¯¾æ¡ˆä¾‹ä¸­ï¼Œå¯¹ç‰¹å¾æ•°æ®éƒ½åšäº†Z-Scoreè§„èŒƒåŒ–å¤„ç†ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ï¼Œå‡†ç¡®ç‡åœ¨90%ä»¥ä¸Šï¼Œå¦‚æœæ•°æ®ä¸åšè§„èŒƒåŒ–å¤„ç†ï¼Œå‡†ç¡®ç‡åœ¨88%å·¦å³ï¼Œæˆ‘çš„é—®é¢˜ï¼š
1ã€æ•°æ®è§„èŒƒåŒ–å¤„ç†ï¼Œæ˜¯ä¸æ˜¯äººä¸ºåœ°æä¾›äº†å‡†ç¡®ç‡ï¼Ÿå®é™…æƒ…å†µï¼Œæ•°æ®ä¸ä¸€å®šæ˜¯æ­£æ€åˆ†å¸ƒã€‚
2ã€æ¨¡å‹å»ºå¥½åï¼Œåœ¨å®é™…åº”ç”¨ä¸­å»è¯„ä¼°æŸä¸ªæ¡ˆä¾‹æ—¶ï¼Œè¯¥æ¡ˆä¾‹æ•°æ®æ˜¯ä¸æ˜¯ä¹Ÿè¦è§„èŒƒåŒ–ï¼Œè¿™æ ·åšæ˜¯ä¸æ˜¯å¾ˆéº»çƒ¦å¹¶ä¸”æ•°æ®å¯¹æ¯”ä¸æ˜¯å¾ˆç›´è§‚å‘¢ï¼Ÿ</div>2019-03-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/a1/74/3dfa4436.jpg" width="30px"><span>Rickie</span> ğŸ‘ï¼ˆ7ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æ€è€ƒé¢˜ï¼š
ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒå¾—åˆ°çš„å‡†ç¡®ç‡ä¸º0.9766ï¼Œé«˜äºç¤ºä¾‹ä¸­çš„å‡†ç¡®ç‡ã€‚æ˜¯å¦æ˜¯ç”±äºå¤šé‡å…±çº¿æ€§ï¼Œä½¿å¾—æµ‹è¯•ç»“æœåé«˜ï¼Ÿ</div>2019-02-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg" width="30px"><span>æ˜ç¿¼</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>è€å¸ˆæˆ‘åˆ©ç”¨äº†ç»“æœå’Œç‰¹å¾çš„ç›¸å…³æ€§ï¼Œé€‰æ‹©ç‰¹å¾ï¼Œå‘ç°ç»“æœæ›´å¥½ï¼š
# ç‰¹å¾é€‰æ‹© æŒ‰ç…§ç»“æœå’Œæ•°æ®ç›¸å…³æ€§é€‰æ‹©ç‰¹å¾å‡†ç¡®ç‡0.9707602339181286
features_remain = [&#39;radius_mean&#39;,&#39;perimeter_mean&#39;,&#39;area_mean&#39;,&#39;concave points_mean&#39;,&#39;radius_worst&#39;,&#39;perimeter_worst&#39;,&#39;area_worst&#39;,&#39;concave points_worst&#39;]</div>2019-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/9d/eb/2c7f3d3b.jpg" width="30px"><span>Ricky</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è°¢è°¢ï¼Œæ2ä¸ªé—®é¢˜ï¼Œ
1ï¼‰åœ¨å®é™…åº”ç”¨ä¸­å¦‚ä½•å¹³è¡¡ç‰¹å¾å˜é‡å’Œå‡†ç¡®ç‡çš„å…³ç³»ï¼Ÿæœ‰æ²¡æœ‰æ–¹æ³•è®ºï¼Ÿ
å¢åŠ ç‰¹å¾å˜é‡æ„å‘³ç€å¢åŠ è¿ç®—æ—¶é—´ï¼Œæé«˜å‡†ç¡®ç‡ï¼Œä½†æ˜¯è¿™ä¸ªå¾—å¤±æ€ä¹ˆæŠŠæ¡ï¼ŸåŒæ—¶å¦‚ä½•è¯„ä¼°ä¼šå¢åŠ å¤šå°‘è¿ç®—æ—¶é—´ï¼Œä¸€ä¸ªä¸€ä¸ªå°è¯•ä¼¼ä¹æ¯”è¾ƒè´¹åŠ²å§
2ï¼‰æ­¤æ–‡çš„æ¡ˆä¾‹æ˜¯é€‰ç”¨å¹³å‡å€¼ï¼Œä¸¢å¼ƒäº†æœ€å¤§å€¼å’Œæ ‡å‡†å·®ï¼Œè¿™ä¸ªæ˜¯å¤šå°‘æ¡ˆä¾‹çš„é€šç”¨åšæ³•ä¹ˆï¼Ÿ

è°¢è°¢</div>2020-04-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/e3/d9/bda5e991.jpg" width="30px"><span>æ¬æ¬</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>å¯¹æ¯”å‡ ç»„featureåï¼Œå‘ç°ç”¨feature_worstè¿›è¡Œè®­ç»ƒï¼Œæ•ˆæœæ›´å¥½ã€‚
1ï¼‰SVC(kernel=&#39;linear&#39;)çš„æµ‹è¯•é›†å‡†ç¡®ç‡ä¸ºï¼š99.42%ï¼›
2)  LinearSVC()çš„æµ‹è¯•é›†å‡†ç¡®ç‡ä¸ºï¼š97.07%
2ï¼‰SVC()çš„æµ‹è¯•é›†å‡†ç¡®ç‡ä¸ºï¼š96.49%
è§‰å¾—å»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œç‰¹å¾é€‰æ‹©å¾ˆé‡è¦ï¼Œä¸åŒçš„æ•°æ®é›†åˆ’åˆ†ï¼Œæ­£è´Ÿæ ·æœ¬æ˜¯å¦å¹³è¡¡ä¹Ÿä¼šå¯¹ç»“æœæœ‰ä¸€å®šçš„å½±å“ï¼Œæ‰€ä»¥æœ€å¥½æ˜¯å¯ä»¥é‡‡ç”¨äº¤å‰éªŒè¯æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ä¸ªåœ°æ–¹å¤šæ¬¡æµ‹è¯•SVC(kernel=&#39;linear&#39;ï¼‰å’ŒLinearSVC()ï¼Œæ„Ÿè§‰è¿˜æ˜¯ä¼šå­˜åœ¨2ä¸ªç™¾åˆ†ç‚¹å·¦å³çš„å·®å¼‚ï¼Œè¿™ä¸¤ä¸ªéƒ½ç®—æ˜¯çº¿æ€§åˆ†ç±»ï¼Œæ˜¯å› ä¸ºé‡‡ç”¨äº†ä¸åŒçš„çº¿æ€§æ ¸å‡½æ•°å—ï¼Ÿè¿˜æ˜¯å…¶ä»–å‚æ•°æˆ–æ˜¯æ–¹æ³•å·®å¼‚çš„åŸå› å‘¢ï¼Ÿ</div>2020-03-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg" width="30px"><span>æ»¢</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è¯­è¨€Python3.6  æ²¡æœ‰z-scoreè§„èŒƒåŒ–æ•°æ®ä»¥åŠè§„èŒƒåŒ–åä¸¤ç§æƒ…å†µå‰æé¢„æµ‹å‡†ç¡®ç‡ï¼Œä½¿ç”¨LinearSVCï¼Œé€‰å–æ‰€æœ‰meanå±æ€§

import  pandas as  pd
import  matplotlib.pyplot as  plt
import  seaborn as  sns
from sklearn.model_selection import  train_test_split
from sklearn import  svm
from sklearn import  metrics
from sklearn.preprocessing import  StandardScaler

#å¯¼å…¥æ•°æ®
path = &#39;&#47;Users&#47;apple&#47;Desktop&#47;GitHubProject&#47;Read mark&#47;æ•°æ®åˆ†æ&#47;geekTime&#47;data&#47;&#39;
data = pd.read_csv(path + &#39;breast_cancer&#47;data.csv&#39;)

#æ•°æ®æ¢ç´¢
pd.set_option(&#39;display.max_columns&#39;, None)
print(data.columns)
print(data.head(5))
print(data.describe())

#å°†ç‰¹å¾å­—æ®µè¿›è¡Œåˆ†ç»„
features_mean = list(data.columns[2:12])
features_se = list(data.columns[12:22])
features_worst = list(data.columns[22:32])

#æ•°æ®æ¸…æ´—
#åˆ é™¤IDåˆ—
data.drop(&#39;id&#39;,axis=1,inplace=True)
#å°†è‰¯æ€§Bæ›¿æ¢ä¸º0ï¼Œå°†æ¶æ€§æ›¿æ¢ä¸º1
data[&#39;diagnosis&#39;] = data[&#39;diagnosis&#39;].map({&#39;B&#39;:0,&#39;M&#39;:1})

#å°†è‚¿ç˜¤è¯Šæ–­ç»“æœå¯è§†åŒ–
sns.countplot(data[&#39;diagnosis&#39;],label=&#39;count&#39;)
plt.show()
#è®¡ç®—ç›¸å…³ç³»æ•°
corr = data[features_mean].corr()
plt.figure(figsize=(14,14))

#ç”¨çƒ­åŠ›å›¾å‘ˆç°ç›¸å…³æ€§ï¼Œæ˜¾ç¤ºæ¯ä¸ªæ–¹æ ¼çš„æ•°æ®
sns.heatmap(corr,annot=True)
plt.show()

#ç‰¹å¾é€‰æ‹©ï¼Œé€‰æ‹©æ‰€æœ‰çš„meanæ•°æ®
feature_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,
       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,
       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;]

#æŠ½å–30%ç‰¹å¾é€‰æ‹©ä½œä¸ºæµ‹è¯•æ•°æ®ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
train,test = train_test_split(data,test_size=0.3)
#æŠ½å–ç‰¹å¾é€‰æ‹©ä½œä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
train_data = train[feature_remain]
train_result = train[&#39;diagnosis&#39;]
test_data = test[feature_remain]
test_result = test[&#39;diagnosis&#39;]

#åˆ›å»ºSVMåˆ†ç±»å™¨
model = svm.LinearSVC()
#ç”¨è®­ç»ƒé›†åšè®­ç»ƒ
model.fit(train_data,train_result)
#ç”¨æµ‹è¯•é›†åšé¢„æµ‹
prediction = model.predict(test_data)
#å‡†ç¡®ç‡
print(&#39;å‡†ç¡®ç‡:&#39;, metrics.accuracy_score(prediction,test_result))

#è§„èŒƒåŒ–æ•°æ®ï¼Œå†é¢„ä¼°å‡†ç¡®ç‡
z_score = StandardScaler()
train_data = z_score.fit_transform(train_data)
test_data = z_score.transform(test_data)
#ç”¨æ–°æ•°æ®åšè®­ç»ƒ
new_model = svm.LinearSVC()
new_model.fit(train_data,train_result)
#é‡æ–°é¢„æµ‹
new_prediction = new_model.predict(test_data)
#å‡†ç¡®ç‡
print(&#39;å‡†ç¡®ç‡:&#39;,metrics.accuracy_score(new_prediction,test_result))</div>2019-04-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg" width="30px"><span>Ronnyz</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>é€‰å–å…¨éƒ¨ç‰¹å¾ï¼š
SVMåˆ†ç±»å™¨å‡†ç¡®ç‡ï¼š 0.9824561403508771
cross_val_scoreçš„å‡†ç¡®ç‡ä¸ºï¼š0.9727
linearSVMåˆ†ç±»å™¨çš„å‡†ç¡®ç‡ï¼š 0.9766081871345029
cross_val_scoreçš„å‡†ç¡®ç‡ä¸ºï¼š0.9652

é€‰å–meanç›¸å…³ç‰¹å¾ï¼š
SVMåˆ†ç±»å™¨å‡†ç¡®ç‡ï¼š 0.9239766081871345
cross_val_scoreçš„å‡†ç¡®ç‡ä¸ºï¼š0.9321
linearSVMåˆ†ç±»å™¨çš„å‡†ç¡®ç‡ï¼š 0.9298245614035088
cross_val_scoreçš„å‡†ç¡®ç‡ä¸ºï¼š0.9247

æ•°æ®ç»“æœä¸Šçœ‹ï¼š
SVMçš„ç»“æœè¦å¥½äºlinearSVM;
é€‰å–å¤šç‰¹å¾çš„ç»“æœè¦å¥½äºé€‰å–å°‘ç‰¹å¾çš„ç»“æœ</div>2019-11-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a4/5a/e708e423.jpg" width="30px"><span>third</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ç¬¬äºŒä¸ªï¼Œå‡†ç¡®ç‡ 0.935672514619883ã€‚

æ„Ÿè§‰è¿˜è›®å¥½ç”¨çš„ï¼Œåªæ˜¯ä¸æ˜¯å¾ˆç†Ÿç»ƒçš„ä½¿ç”¨å„ä¸ªç®—æ³•åšåˆ†ç±»å’Œå›å½’</div>2019-02-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg" width="30px"><span>Python</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆå¯ä»¥ç”¨PCAè¿›è¡Œç‰¹å¾é€‰æ‹©å—ï¼Ÿå¦‚æœå¯ä»¥ï¼Œé‚£å’Œä½ è¿™ç§æ‰‹åŠ¨çš„æ–¹æ³•æ¯”æœ‰ä»€ä¹ˆå·®åˆ«</div>2019-02-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/8f/9e/a0a36b1c.jpg" width="30px"><span>æ™¨æ›¦</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼Œå¦‚æœè¿™ä¸ªç—…ç—‡æ”¹ä¸ºä¸å¾—ç—…ï¼Œè½»ç—‡ï¼Œé‡ç—‡ä¸‰ä¸ªåˆ†ç±»ï¼Œä¸æ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œå¯¹åº”æ”¹æˆåˆ†ç±»åºå·ï¼Œ0,1,2ã€‚é‚£ä¹ˆè¿™å¥—ç®—æ³•æ˜¯ä¸æ˜¯ä¹Ÿä¸ç®—é”™</div>2021-01-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/20/a9/67/b53898d7.jpg" width="30px"><span>é‚¹æ´²</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div># coding=utf-8
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
#å¯¼å…¥sklearnä¸­çš„SVMåº“
from sklearn import svm, metrics

#æ„é€ çº¿æ€§åˆ†ç±»æ¨¡å‹
model = svm.SVC(kernel=&quot;rbf&quot;,C=1.0,gamma=&quot;auto&quot;)
model2 = svm.LinearSVC()
&quot;&quot;&quot;
kernel :
    linear:çº¿æ€§æ¨¡å‹ï¼Œå½“æ¨¡å‹ä¸ºlinearsvmæ—¶ï¼Œå°±è¡¨æ˜æ²¡æœ‰kernelå‚æ•°
    polyï¼šå¤šé¡¹å¼æ¨¡å‹
    rbf:é«˜æ–¯å‡½æ•°
    sigmoid:sigmoidæ ¸å‡½æ•°
Cï¼šç›®æ ‡å‡½æ•°çš„æƒ©ç½šç³»æ•°
gamma :æ ¸å‡½æ•°ç³»æ•°
&quot;&quot;&quot;

#å®æˆ˜å¼€å§‹--åŠ è½½æ•°æ®
data = pd.read_csv(&#39;data.csv&#39;,engine=&#39;python&#39;)
# pd.set_option(&#39;display.max_columns&#39;,None)
#æŸ¥çœ‹ä¸€ä¸‹æ•°æ®ä¿¡æ¯
# print(data.columns)
# print(data.info())
# print(data.head(5))
# print(data[&#39;diagnosis&#39;].value_counts())

#æ•°æ®å¤„ç†
#idå­—æ®µå¯¹äºåˆ†ç±»æ— ç”¨ï¼Œåˆ é™¤å³å¯
data.drop(&#39;id&#39;,axis=1,inplace=True)
#diagnosiså­—æ®µä¸ºå­—ç¬¦å‹ï¼Œç»„éœ€è½¬æ¢ä¸ºæ•°å€¼å‹0 1
data[&#39;diagnosis&#39;] = data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})

feature_mean = data.columns[1:11]
feature_se = data.columns[11:21]
feature_max = data.columns[21:31]


#ç»Ÿè®¡ä¸€ä¸‹è‚¿ç˜¤äººæ•°æƒ…å†µ
sns.countplot(data[&#39;diagnosis&#39;],label=&#39;Count&#39;)
# plt.show()

#æŸ¥çœ‹å„ä¸ªç‰¹å¾çš„ç›¸å…³åº¦
corr = data[feature_mean].corr()
plt.figure(figsize=(14,14))
sns.heatmap(corr,annot=True)
# plt.show()

feature_sel = [&#39;radius_mean&#39;,&#39;texture_mean&#39;,&#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;,&#39;fractal_dimension_mean&#39;]


train_data,test_data = train_test_split(data,test_size=0.3)

train_feature = train_data[feature_sel]
train_label = train_data[&#39;diagnosis&#39;]
test_feature = test_data[feature_sel]
test_label = test_data[&#39;diagnosis&#39;]

#å¼€å§‹è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
#é‡‡ç”¨Z-Scoreè§„èŒƒåŒ–å¤„ç†ï¼Œä¿è¯æ¯ä¸€ä¸ªç‰¹å¾ç»´åº¦çš„æ•°æ®å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
ss = StandardScaler()
train_feature = ss.fit_transform(train_feature)
test_feature = ss.transform(test_feature)

#å¼€å§‹é¢„æµ‹
model.fit(train_feature,train_label)
model2.fit(train_feature,train_label)
predict_label = model.predict(test_feature)
predict_label2 = model2.predict(test_feature)
#å‡†ç¡®åº¦ -- æ¯æ¬¡è¿è¡Œä¸ä¸€æ ·
print(&quot;é«˜æ–¯å‡†ç¡®ç‡ï¼š&quot;,metrics.accuracy_score(test_label,predict_label))
print(&quot;çº¿æ€§å‡†ç¡®åº¦ï¼š&quot;,metrics.accuracy_score(test_label,predict_label2))


</div>2020-08-26</li><br/><li><img src="" width="30px"><span>æœ±ä¸€æ±Ÿ</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æˆ‘ä»¬æ‰€æµ‹çš„å‡†ç¡®ç‡æ˜¯ä¸train_yè¿›è¡Œæ¯”è¾ƒçš„å—
</div>2020-08-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/bf/e3/2aa8ec84.jpg" width="30px"><span>é±¼éå­</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn import metrics


data = pd.read_csv(&quot;.&#47;data.csv&quot;)
pd.set_option(&#39;display.max_columns&#39;, None)
# print(data.head(5))
# print(data.columns)
# print(data.describe())

features_mean = list(data.columns[2:12])
features_se = list(data.columns[12:22])
features_worst = list(data.columns[22:32])

data.drop(&quot;id&quot;,axis=1,inplace=True)
data[&#39;diagnosis&#39;] = data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})
# print(data.head(5))

sns.countplot(data[&#39;diagnosis&#39;],label=&quot;Count&quot;)
plt.show()

corr = data[features_mean].corr()
plt.figure(figsize=(14,14))
sns.heatmap(corr,annot=True)
plt.show()

features_remain = [&#39;radius_mean&#39;,&#39;texture_mean&#39;, &#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;]
X_train, X_test, y_train, y_test = train_test_split(data[features_remain], data[&#39;diagnosis&#39;], test_size=0.3,random_state=0)

# é‡‡ç”¨Z-Scoreè§„èŒƒåŒ–æ•°æ®ï¼Œä¿è¯æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ•°æ®å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)

model = svm.SVC()
model.fit(X_train,y_train)

prediction = model.predict(X_test)
print(&quot;å‡†ç¡®ç‡ï¼š&quot;,metrics.accuracy_score(prediction,y_test))

å‡†ç¡®ç‡ï¼š 0.9239766081871345
</div>2020-03-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg" width="30px"><span>è¿½æ¢¦</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼Œè¯·é—®ç”¨ä»€ä¹ˆæ–¹æ³•å¯ä»¥åˆ¤å®šæ•°æ®é›†æ˜¯å¦ä¸ºçº¿æ€§å¯åˆ†çš„å‘¢</div>2019-10-14</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/3hZfficKPGCq2kjFBu9SgaMjibJTEl7iaW1ta6pZNyiaWP8XEsNpunlnsiaOtBpWTXfT5BvRP3qNByml6p9rtBvqewg/132" width="30px"><span>å¤œè·¯ç ´æ™“</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>features_mean=list(data.columns[2:12]) 
è¿™è¡ŒæŠ¥é”™:
TypeError: &#39;DataFrame&#39; object is not callable
å¦‚ä½•æ”¹å–„?</div>2019-07-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/8e/76/6d55e26f.jpg" width="30px"><span>å¼ æ™“è¾‰</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>é‡‡ç”¨linearSVC, é¢„æµ‹å‡†ç¡®ç‡æ›´é«˜ã€‚
import pandas as pd 
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

data = pd.read_csv(&#39;data.csv&#39;)
data.drop(&#39;id&#39;, axis=1, inplace=True)

feature_names = list(data.columns)
feature_names.remove(&#39;diagnosis&#39;)

traindata, testdata = train_test_split(data, test_size=0.3)
train_x = traindata[feature_names]
train_y = traindata[&#39;diagnosis&#39;]
test_x = testdata[feature_names]
test_y = testdata[&#39;diagnosis&#39;]

ss = StandardScaler()
train_x = ss.fit_transform(train_x)
test_x = ss.transform(test_x)
                    
model = svm.LinearSVC()
model.fit(train_x, train_y)
prediction = model.predict(test_x)
print(&quot;The accuracy is %f&quot; % accuracy_score(prediction, test_y))</div>2019-05-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg" width="30px"><span>Geek_hve78z</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn import metrics

# åŠ è½½æ•°æ®é›†ï¼Œä½ éœ€è¦æŠŠæ•°æ®æ”¾åˆ°ç›®å½•ä¸­
data = pd.read_csv(&quot;.&#47;breast_cancer_data-master&#47;data.csv&quot;)

# æ•°æ®æ¢ç´¢
# å› ä¸ºæ•°æ®é›†ä¸­åˆ—æ¯”è¾ƒå¤šï¼Œæˆ‘ä»¬éœ€è¦æŠŠ dataframe ä¸­çš„åˆ—å…¨éƒ¨æ˜¾ç¤ºå‡ºæ¥
pd.set_option(&#39;display.max_columns&#39;, None)
print(data.columns)
print(data.head(5))
print(data.describe())

# å°†ç‰¹å¾å­—æ®µåˆ†æˆ 3 ç»„
features_mean= list(data.columns[2:12])
features_se= list(data.columns[12:22])
features_worst=list(data.columns[22:32])

# æ•°æ®æ¸…æ´—
# ID åˆ—æ²¡æœ‰ç”¨ï¼Œåˆ é™¤è¯¥åˆ—
data.drop(&quot;id&quot;,axis=1,inplace=True)
# å°† B è‰¯æ€§æ›¿æ¢ä¸º 0ï¼ŒM æ¶æ€§æ›¿æ¢ä¸º 1
data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})

# å°†è‚¿ç˜¤è¯Šæ–­ç»“æœå¯è§†åŒ–
sns.countplot(data[&#39;diagnosis&#39;],label=&quot;Count&quot;)
plt.show()
# ç”¨çƒ­åŠ›å›¾å‘ˆç° features_mean å­—æ®µä¹‹é—´çš„ç›¸å…³æ€§
corr = data[features_mean].corr()
plt.figure(figsize=(14,14))
# annot=True æ˜¾ç¤ºæ¯ä¸ªæ–¹æ ¼çš„æ•°æ®
sns.heatmap(corr, annot=True)
plt.show()

# ç‰¹å¾é€‰æ‹©
#features_remain = [&#39;radius_mean&#39;,&#39;texture_mean&#39;, &#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;] 
features_remain = data.columns[1:31]
print(features_remain)
print(&#39;-&#39;*100)

# æŠ½å– 30% çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test
# æŠ½å–ç‰¹å¾é€‰æ‹©çš„æ•°å€¼ä½œä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
train_X = train[features_remain]
train_y=train[&#39;diagnosis&#39;]
test_X= test[features_remain]
test_y =test[&#39;diagnosis&#39;]

# é‡‡ç”¨ Z-Score è§„èŒƒåŒ–æ•°æ®ï¼Œä¿è¯æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ•°æ®å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1
ss = StandardScaler()
train_X = ss.fit_transform(train_X)
test_X = ss.transform(test_X)


# åˆ›å»º SVM åˆ†ç±»å™¨
model = svm.LinearSVC()
# ç”¨è®­ç»ƒé›†åšè®­ç»ƒ
model.fit(train_X,train_y)
# ç”¨æµ‹è¯•é›†åšé¢„æµ‹
prediction=model.predict(test_X)
print(&#39;å‡†ç¡®ç‡: &#39;, metrics.accuracy_score(prediction,test_y))

------
å‡†ç¡®ç‡:  0.9649122807017544</div>2019-04-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/c7/a3/1e2f9f5a.jpg" width="30px"><span>åœ†åœ†çš„å¤§é£Ÿå®¢</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Sun Mar 17 23:18:31 2019

@author: xcma1
&quot;&quot;&quot;
 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# åŠ è½½æ•°æ®é›†ï¼Œä½ éœ€è¦æŠŠæ•°æ®æ”¾åˆ°ç›®å½•ä¸­
data = pd.read_csv(&quot;.&#47;data.csv&quot;)
# æ•°æ®æ¢ç´¢
# å› ä¸ºæ•°æ®é›†ä¸­åˆ—æ¯”è¾ƒå¤šï¼Œæˆ‘ä»¬éœ€è¦æŠŠ dataframe ä¸­çš„åˆ—å…¨éƒ¨æ˜¾ç¤ºå‡ºæ¥
pd.set_option(&#39;display.max_columns&#39;, None)
# å°†ç‰¹å¾å­—æ®µåˆ†æˆ 3 ç»„
features_mean= list(data.columns[2:12])
features_se= list(data.columns[12:22])
features_worst=list(data.columns[22:32])
# æ•°æ®æ¸…æ´—

# ID åˆ—æ²¡æœ‰ç”¨ï¼Œåˆ é™¤è¯¥åˆ—
data.drop(&quot;id&quot;,axis=1,inplace=True)

# å°† B è‰¯æ€§æ›¿æ¢ä¸º 0ï¼ŒM æ¶æ€§æ›¿æ¢ä¸º 1
data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})

# ç‰¹å¾é€‰æ‹©
features_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,
       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,
       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,
       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,
       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,
       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,
       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,
       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,
       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;] 

# æŠ½å– 30% çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test

# æŠ½å–ç‰¹å¾é€‰æ‹©çš„æ•°å€¼ä½œä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
train_X = train[features_remain]
train_y=train[&#39;diagnosis&#39;]
test_X= test[features_remain]
test_y =test[&#39;diagnosis&#39;]

# é‡‡ç”¨ Z-Score è§„èŒƒåŒ–æ•°æ®ï¼Œä¿è¯æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ•°æ®å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1
ss = StandardScaler()
train_X = ss.fit_transform(train_X)
test_X = ss.transform(test_X)

# åˆ›å»º SVM åˆ†ç±»å™¨
model = svm.SVC()
# ç”¨è®­ç»ƒé›†åšè®­ç»ƒ
model.fit(train_X,train_y)
# ç”¨æµ‹è¯•é›†åšé¢„æµ‹
prediction=model.predict(test_X)
print(&#39;å‡†ç¡®ç‡: &#39;, metrics.accuracy_score(prediction,test_y))


</div>2019-03-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/70/eb/e7a2dba9.jpg" width="30px"><span>ldw</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>é™ˆè€å¸ˆï¼Œè¿™å ‚è¯¾ç•™çš„è¯¾åä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯èƒ½ä½¿ç”¨çš„æ•°æ®æ¸…æ´—ï¼Œæ‚¨ä¼šæœŸæœ›æ‚¨å›¢é˜Ÿçš„äººç”¨å¤šé•¿æ—¶é—´å®Œæˆï¼Ÿè¶…è¿‡å¤šé•¿æ—¶é—´ä»¥ä¸Šï¼Œå°±æ˜¯ä¸åˆæ ¼çš„ï¼Ÿè°¢è°¢ğŸ™</div>2019-02-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg" width="30px"><span>mickey</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>å‹˜è¯¯ï¼šçƒ­åŠ›å­¦å›¾ä¸­çš„ç¬¬ä¸€ä¸ªè“è‰²æ¡†æ¡†åº”è¯¥æ˜¯æ ‡è®°åœ¨ç¬¬1åˆ—ç¬¬3-4è¡Œä¸Šï¼Œè€Œä¸æ˜¯ç¬¬1åˆ—ç¬¬1è¡Œã€‚</div>2019-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/62/a5/43aa0c27.jpg" width="30px"><span>TKbook</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div># ç‰¹å¾é€‰æ‹©
features_all = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,
       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,
       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,
       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,
       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,
       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,
       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,
       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,
       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;]
from sklearn.model_selection import train_test_split
# æŠ½å–30%çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
train, test = train_test_split(data, test_size=0.3)
# æŠ½å–ç‰¹å¾é€‰æ‹©çš„çš„æ•°æ®ä½œä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
train_X = train[features_all]
train_y = train[&#39;diagnosis&#39;]
test_X = test[features_all]
test_y = test[&#39;diagnosis&#39;]
from sklearn.preprocessing import StandardScaler
# é‡‡ç”¨Z-Scoreè§„èŒƒåŒ–æ•°æ®ï¼Œä¿è¯æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ•°æ®å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
ss = StandardScaler()
train_X = ss.fit_transform(train_X)
test_X = ss.transform(test_X)
from sklearn import svm
from sklearn.metrics import accuracy_score
# åˆ›å»ºSVMåˆ†ç±»å™¨
model = svm.LinearSVC()
# ç”¨è®­ç»ƒé›†åšè®­ç»ƒ
model.fit(train_X, train_y)
# ç”¨æµ‹è¯•é›†åšé¢„æµ‹
prediction = model.predict(test_X)
print(&#39;å‡†ç¡®ç‡ï¼š&#39;, accuracy_score(prediction, test_y))
å‡†ç¡®ç‡ï¼š 0.9824561403508771</div>2019-02-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/26/53/60fe31fb.jpg" width="30px"><span>æ·±ç™½æµ…é»‘</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼šï¼ˆç›¸åŒè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰
LinearSVCå‡†ç¡®ç‡:  0.9298245614035088
SVCé«˜æ–¯æ ¸å‡†ç¡®ç‡: 0.9415204678362573
SVMé¦–å…ˆæ˜¯æœ‰ç›‘ç£çš„å­¦ä¹ æ¨¡å‹ï¼Œéœ€è¦æ•°æ®æœ‰è¾ƒå¥½çš„åˆ†ç±»å±æ€§ã€‚å…¶æ¬¡ä¾æ®ç¡¬é—´éš”ã€è½¯é—´éš”å’Œæ ¸å‡½æ•°çš„åº”ç”¨ï¼Œå¯ä»¥è§£å†³çº¿æ€§åˆ†ç±»å’Œéçº¿æ€§åˆ†ç±»çš„é—®é¢˜ã€‚æœ€ååœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¯¹æ•°æ®çš„ç‰¹å¾è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„é™ç»´ï¼Œåˆ©ç”¨æ•°æ®çš„ç›¸å…³æ€§ï¼Œå¯¹ç›¸å…³æ€§è¾ƒå¤§çš„ç±»åˆ«å±æ€§é€‰æ‹©å…¶ä¸­ä¸€ä¸ªä½œä¸ºç‰¹å¾ï¼Œåœ¨ç‰¹å¾é€‰å–åï¼Œè¦è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚</div>2019-02-15</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/wJphZ3HcvhjVUyTWCIsCugzfQY5NAy6VJ0XoPLibDlcHWMswFmFe678zd0lUjFETia80NQhyQcVnGDlKgKPcRGyw/132" width="30px"><span>JingZ</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>#svm ä½¿ç”¨è¿˜æ˜¯è›®æ–¹ä¾¿çš„ï¼Œå®Œå…¨ç‰¹å¾ï¼Œå‡†ç¡®ç‡è¾¾åˆ°97%ä»¥ä¸Š

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn import metrics

#åŠ è½½æ•°æ®
data = pd.read_csv(â€˜.&#47;data.csv&#39;)

#æ•°æ®æ¢ç´¢
pd.set_option(&#39;display.max_columns&#39;, None)
print(data.columns)
print(data.head(5))
print(data.describe())

#æ•°æ®æ¸…æ´—
data.drop(&#39;id&#39;, axis=1, inplace=True)
data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;:1, &#39;B&#39;:0})

#ç‰¹å¾é€‰æ‹©
features_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,
       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,
       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,
       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,
       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,
       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,
       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,
       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,
       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;]

#30% æµ‹è¯•é›†ã€è®­ç»ƒé›†
train, test = train_test_split(data, test_size = 0.3)

#æŠ½å–ç‰¹å¾é€‰æ‹©çš„æ•°å€¼ä½œä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
train_X = train[features_remain]
train_y = train[&#39;diagnosis&#39;]
test_X = test[features_remain]
test_y = test[&#39;diagnosis&#39;]

#é‡‡ç”¨ Z-score è§„èŒƒåŒ–æ•°æ®
ss = StandardScaler()
train_X = ss.fit_transform(train_X)
test_X = ss.transform(test_X)

#åˆ›å»º LinearSVC åˆ†ç±»å™¨
model = svm.LinearSVC()

#ç”¨è®­ç»ƒé›†åšè®­ç»ƒ
model.fit(train_X, train_y)

#ç”¨æµ‹è¯•é›†åšé¢„æµ‹
prediction = model.predict(test_X)
print(&#39;å‡†ç¡®ç‡ï¼š&#39;, metrics.accuracy_score(prediction, test_y))</div>2019-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ec/4a/40a2ba79.jpg" width="30px"><span>reverse</span> ğŸ‘ï¼ˆ32ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>æå®¢æ—¶é—´æ•°æ®åˆ†æå®æˆ˜45è®²çš„è¯¦ç»†ç¬”è®°(åŒ…å«markdownã€å›¾ç‰‡ã€æ€ç»´å¯¼å›¾ ä»£ç ) githubåœ°å€ï¼š https:&#47;&#47;github.com&#47;xiaomiwujiecao&#47;DataAnalysisInAction</div>2019-03-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg" width="30px"><span>mickey</span> ğŸ‘ï¼ˆ7ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div># encoding=utf-8
from sklearn import svm
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import seaborn  as sns
import matplotlib.pyplot as plt

# åŠ è½½æ•°æ®é›†ï¼Œä½ éœ€è¦æŠŠæ•°æ®æ”¾åˆ°ç›®å½•ä¸­
data = pd.read_csv(&quot;.&#47;data.csv&quot;)
# æ•°æ®æ¢ç´¢
# å› ä¸ºæ•°æ®é›†ä¸­åˆ—æ¯”è¾ƒå¤šï¼Œæˆ‘ä»¬éœ€è¦æŠŠdataframeä¸­çš„åˆ—å…¨éƒ¨æ˜¾ç¤ºå‡ºæ¥
pd.set_option(&#39;display.max_columns&#39;, None)
#print(data.columns)
#print(data.head(5))
#print(data.describe())

# å°†ç‰¹å¾å­—æ®µåˆ†æˆ3ç»„
features_mean= list(data.columns[2:12])
features_se= list(data.columns[12:22])
features_worst=list(data.columns[22:32])
# æ•°æ®æ¸…æ´—
# IDåˆ—æ²¡æœ‰ç”¨ï¼Œåˆ é™¤è¯¥åˆ—
data.drop(&quot;id&quot;,axis=1,inplace=True)
# å°†Bè‰¯æ€§æ›¿æ¢ä¸º0ï¼ŒMæ¶æ€§æ›¿æ¢ä¸º1
data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;: 1, &#39;B&#39;: 0})

# ç‰¹å¾é€‰æ‹©
features_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,
       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,
       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,
       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,
       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,
       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,
       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,
       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,
       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;]

# æŠ½å–30%çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test
# æŠ½å–ç‰¹å¾é€‰æ‹©çš„æ•°å€¼ä½œä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
train_X = train[features_remain]
train_y=train[&#39;diagnosis&#39;]
test_X= test[features_remain]
test_y =test[&#39;diagnosis&#39;]

# é‡‡ç”¨Z-Scoreè§„èŒƒåŒ–æ•°æ®ï¼Œä¿è¯æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ•°æ®å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
ss = StandardScaler()
train_X = ss.fit_transform(train_X)
test_X = ss.transform(test_X)

# åˆ›å»ºSVMåˆ†ç±»å™¨
model = svm.LinearSVC()
# ç”¨è®­ç»ƒé›†åšè®­ç»ƒ
model.fit(train_X,train_y)
# ç”¨æµ‹è¯•é›†åšé¢„æµ‹
prediction=model.predict(test_X)
print(&#39;å‡†ç¡®ç‡: &#39;, metrics.accuracy_score(prediction,test_y))

å‡†ç¡®ç‡: 0.9707602339181286</div>2019-02-26</li><br/><li><img src="" width="30px"><span>yanyu-xin</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>å°†è‚¿ç˜¤è¯Šæ–­ç»“æœå¯è§†åŒ–çš„æŸ±çŠ¶å›¾å‡ºç°æ˜¾ç¤ºé”™è¯¯ã€‚åº”è¯¥æ˜¯seabornæ–°æ—§ç‰ˆæœ¬å¯¼è‡´çš„ã€‚
å°†ï¼šsns.countplot(data[&#39;diagnosis&#39;],label=&quot;Count&quot;)
æ”¹ä¸ºï¼šsns.countplot(x=&#39;diagnosis&#39;,data=data,label=&quot;Count&quot;)  
æŸ±çŠ¶å›¾æ˜¾ç¤ºæ­£ç¡®æŸ±çŠ¶å›¾ã€‚</div>2024-04-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/35/7d/c1/1e6158f6.jpg" width="30px"><span>ç§‰å…¨</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>æ€ä¹ˆçœ‹åˆ†ç±»ç»“æœ</div>2023-08-21</li><br/><li><img src="" width="30px"><span>ä¸‰ç¡åŸºç”²è‹¯</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div>ç”¨KæŠ˜äº¤å‰éªŒè¯ï¼ŒLinearSVCçš„å‡†ç¡®ç‡æ˜¯92.64% SVCæ˜¯92.98% 
è‡³äºSVCçš„ä½¿ç”¨ï¼Œæˆ‘ä¸€å¼€å§‹ç›´æ¥æŒ‰ç…§è‡ªå·±çš„æƒ³æ³•å†™å®Œä»¥åï¼Œä¼šæœ‰èšåˆè­¦å‘Šï¼Œç„¶åçœ‹äº†ä¸€ä¸‹ï¼Œæ˜¯æ•°æ®æ²¡æœ‰è¿›è¡ŒStandardScalerï¼Œæˆ‘è§‰å¾—è¿™ä¸ªæ­¥éª¤å®¹æ˜“å¿˜è®°ã€‚</div>2019-03-10</li><br/>
</ul>