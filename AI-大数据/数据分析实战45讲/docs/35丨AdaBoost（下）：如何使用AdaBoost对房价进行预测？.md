ä»Šå¤©æˆ‘å¸¦ä½ ç”¨AdaBoostç®—æ³•åšä¸€ä¸ªå®æˆ˜é¡¹ç›®ã€‚AdaBoostä¸ä»…å¯ä»¥ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œè¿˜å¯ä»¥ç”¨äºå›å½’åˆ†æã€‚

æˆ‘ä»¬å…ˆåšä¸ªç®€å•å›å¿†ï¼Œä»€ä¹ˆæ˜¯åˆ†ç±»ï¼Œä»€ä¹ˆæ˜¯å›å½’å‘¢ï¼Ÿå®é™…ä¸Šåˆ†ç±»å’Œå›å½’çš„æœ¬è´¨æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯å¯¹æœªçŸ¥äº‹ç‰©åšé¢„æµ‹ã€‚ä¸åŒä¹‹å¤„åœ¨äºè¾“å‡ºç»“æœçš„ç±»å‹ï¼Œåˆ†ç±»è¾“å‡ºçš„æ˜¯ä¸€ä¸ªç¦»æ•£å€¼ï¼Œå› ä¸ºç‰©ä½“çš„åˆ†ç±»æ•°æœ‰é™çš„ï¼Œè€Œå›å½’è¾“å‡ºçš„æ˜¯è¿ç»­å€¼ï¼Œä¹Ÿå°±æ˜¯åœ¨ä¸€ä¸ªåŒºé—´èŒƒå›´å†…ä»»ä½•å–å€¼éƒ½æœ‰å¯èƒ½ã€‚

è¿™æ¬¡æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯ä½¿ç”¨AdaBoosté¢„æµ‹æˆ¿ä»·ï¼Œè¿™æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ã€‚é™¤äº†å¯¹é¡¹ç›®è¿›è¡Œç¼–ç å®æˆ˜å¤–ï¼Œæˆ‘å¸Œæœ›ä½ èƒ½æŒæ¡ï¼š

1. AdaBoostå·¥å…·çš„ä½¿ç”¨ï¼ŒåŒ…æ‹¬ä½¿ç”¨AdaBoostè¿›è¡Œåˆ†ç±»ï¼Œä»¥åŠå›å½’åˆ†æã€‚
2. ä½¿ç”¨å…¶ä»–çš„å›å½’å·¥å…·ï¼Œæ¯”å¦‚å†³ç­–æ ‘å›å½’ï¼Œå¯¹æ¯”AdaBoostå›å½’å’Œå†³ç­–æ ‘å›å½’çš„ç»“æœã€‚

## å¦‚ä½•ä½¿ç”¨AdaBoostå·¥å…·

æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨sklearnä¸­ä½¿ç”¨AdaBoostã€‚å¦‚æœæˆ‘ä»¬è¦ç”¨AdaBoostè¿›è¡Œåˆ†ç±»ï¼Œéœ€è¦åœ¨ä½¿ç”¨å‰å¼•ç”¨ä»£ç ï¼š

```
from sklearn.ensemble import AdaBoostClassifier
```

æˆ‘ä»¬ä¹‹å‰è®²åˆ°è¿‡ï¼Œå¦‚æœä½ çœ‹åˆ°äº†Classifierè¿™ä¸ªç±»ï¼Œä¸€èˆ¬éƒ½ä¼šå¯¹åº”ç€Regressorç±»ã€‚AdaBoostä¹Ÿä¸ä¾‹å¤–ï¼Œå›å½’å·¥å…·åŒ…çš„å¼•ç”¨ä»£ç å¦‚ä¸‹ï¼š

```
from sklearn.ensemble import AdaBoostRegressor
```

æˆ‘ä»¬å…ˆçœ‹ä¸‹å¦‚ä½•åœ¨sklearnä¸­åˆ›å»ºAdaBooståˆ†ç±»å™¨ã€‚

æˆ‘ä»¬éœ€è¦ä½¿ç”¨AdaBoostClassifier(base\_estimator=None, n\_estimators=50, learning\_rate=1.0, algorithm=â€™SAMME.Râ€™, random\_state=None)è¿™ä¸ªå‡½æ•°ï¼Œå…¶ä¸­æœ‰å‡ ä¸ªæ¯”è¾ƒä¸»è¦çš„å‚æ•°ï¼Œæˆ‘åˆ†åˆ«æ¥è®²è§£ä¸‹ï¼š

1. base\_estimatorï¼šä»£è¡¨çš„æ˜¯å¼±åˆ†ç±»å™¨ã€‚åœ¨AdaBoostçš„åˆ†ç±»å™¨å’Œå›å½’å™¨ä¸­éƒ½æœ‰è¿™ä¸ªå‚æ•°ï¼Œåœ¨AdaBoostä¸­é»˜è®¤ä½¿ç”¨çš„æ˜¯å†³ç­–æ ‘ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¸éœ€è¦ä¿®æ”¹è¿™ä¸ªå‚æ•°ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥æŒ‡å®šå…·ä½“çš„åˆ†ç±»å™¨ã€‚
2. n\_estimatorsï¼šç®—æ³•çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œä¹Ÿæ˜¯åˆ†ç±»å™¨çš„ä¸ªæ•°ï¼Œæ¯ä¸€æ¬¡è¿­ä»£éƒ½ä¼šå¼•å…¥ä¸€ä¸ªæ–°çš„å¼±åˆ†ç±»å™¨æ¥å¢åŠ åŸæœ‰çš„åˆ†ç±»å™¨çš„ç»„åˆèƒ½åŠ›ã€‚é»˜è®¤æ˜¯50ã€‚
3. learning\_rateï¼šä»£è¡¨å­¦ä¹ ç‡ï¼Œå–å€¼åœ¨0-1ä¹‹é—´ï¼Œé»˜è®¤æ˜¯1.0ã€‚å¦‚æœå­¦ä¹ ç‡è¾ƒå°ï¼Œå°±éœ€è¦æ¯”è¾ƒå¤šçš„è¿­ä»£æ¬¡æ•°æ‰èƒ½æ”¶æ•›ï¼Œä¹Ÿå°±æ˜¯è¯´å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°æ˜¯æœ‰ç›¸å…³æ€§çš„ã€‚å½“ä½ è°ƒæ•´learning\_rateçš„æ—¶å€™ï¼Œå¾€å¾€ä¹Ÿéœ€è¦è°ƒæ•´n\_estimatorsè¿™ä¸ªå‚æ•°ã€‚
4. algorithmï¼šä»£è¡¨æˆ‘ä»¬è¦é‡‡ç”¨å“ªç§boostingç®—æ³•ï¼Œä¸€å…±æœ‰ä¸¤ç§é€‰æ‹©ï¼šSAMME å’ŒSAMME.Rã€‚é»˜è®¤æ˜¯SAMME.Rã€‚è¿™ä¸¤è€…ä¹‹é—´çš„åŒºåˆ«åœ¨äºå¯¹å¼±åˆ†ç±»æƒé‡çš„è®¡ç®—æ–¹å¼ä¸åŒã€‚
5. random\_stateï¼šä»£è¡¨éšæœºæ•°ç§å­çš„è®¾ç½®ï¼Œé»˜è®¤æ˜¯Noneã€‚éšæœºç§å­æ˜¯ç”¨æ¥æ§åˆ¶éšæœºæ¨¡å¼çš„ï¼Œå½“éšæœºç§å­å–äº†ä¸€ä¸ªå€¼ï¼Œä¹Ÿå°±ç¡®å®šäº†ä¸€ç§éšæœºè§„åˆ™ï¼Œå…¶ä»–äººå–è¿™ä¸ªå€¼å¯ä»¥å¾—åˆ°åŒæ ·çš„ç»“æœã€‚å¦‚æœä¸è®¾ç½®éšæœºç§å­ï¼Œæ¯æ¬¡å¾—åˆ°çš„éšæœºæ•°ä¹Ÿå°±ä¸åŒã€‚

é‚£ä¹ˆå¦‚ä½•åˆ›å»ºAdaBoostå›å½’å‘¢ï¼Ÿ

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨AdaBoostRegressor(base\_estimator=None, n\_estimators=50, learning\_rate=1.0, loss=â€˜linearâ€™, random\_state=None)è¿™ä¸ªå‡½æ•°ã€‚

ä½ èƒ½çœ‹å‡ºæ¥å›å½’å’Œåˆ†ç±»çš„å‚æ•°åŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œä¸åŒç‚¹åœ¨äºå›å½’ç®—æ³•é‡Œæ²¡æœ‰algorithmè¿™ä¸ªå‚æ•°ï¼Œä½†å¤šäº†ä¸€ä¸ªlosså‚æ•°ã€‚

lossä»£è¡¨æŸå¤±å‡½æ•°çš„è®¾ç½®ï¼Œä¸€å…±æœ‰3ç§é€‰æ‹©ï¼Œåˆ†åˆ«ä¸ºlinearã€squareå’Œexponentialï¼Œå®ƒä»¬çš„å«ä¹‰åˆ†åˆ«æ˜¯çº¿æ€§ã€å¹³æ–¹å’ŒæŒ‡æ•°ã€‚é»˜è®¤æ˜¯çº¿æ€§ã€‚ä¸€èˆ¬é‡‡ç”¨çº¿æ€§å°±å¯ä»¥å¾—åˆ°ä¸é”™çš„æ•ˆæœã€‚

åˆ›å»ºå¥½AdaBooståˆ†ç±»å™¨æˆ–å›å½’å™¨ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥è¾“å…¥è®­ç»ƒé›†å¯¹å®ƒè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨fitå‡½æ•°ï¼Œä¼ å…¥è®­ç»ƒé›†ä¸­çš„æ ·æœ¬ç‰¹å¾å€¼train\_Xå’Œç»“æœtrain\_yï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨æ‹Ÿåˆã€‚ä½¿ç”¨predictå‡½æ•°è¿›è¡Œé¢„æµ‹ï¼Œä¼ å…¥æµ‹è¯•é›†ä¸­çš„æ ·æœ¬ç‰¹å¾å€¼test\_Xï¼Œç„¶åå°±å¯ä»¥å¾—åˆ°é¢„æµ‹ç»“æœã€‚

## å¦‚ä½•ç”¨AdaBoostå¯¹æˆ¿ä»·è¿›è¡Œé¢„æµ‹

äº†è§£äº†AdaBoostå·¥å…·åŒ…ä¹‹åï¼Œæˆ‘ä»¬çœ‹ä¸‹sklearnä¸­è‡ªå¸¦çš„æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†ã€‚

è¿™ä¸ªæ•°æ®é›†ä¸€å…±åŒ…æ‹¬äº†506æ¡æˆ¿å±‹ä¿¡æ¯æ•°æ®ï¼Œæ¯ä¸€æ¡æ•°æ®éƒ½åŒ…æ‹¬äº†13ä¸ªæŒ‡æ ‡ï¼Œä»¥åŠä¸€ä¸ªæˆ¿å±‹ä»·ä½ã€‚

13ä¸ªæŒ‡æ ‡çš„å«ä¹‰ï¼Œå¯ä»¥å‚è€ƒä¸‹é¢çš„è¡¨æ ¼ï¼š

![](https://static001.geekbang.org/resource/image/42/b7/426dec532f34d7f458e36ee59a6617b7.png?wh=468%2A447)  
è¿™äº›æŒ‡æ ‡åˆ†æå¾—è¿˜æ˜¯æŒºç»†çš„ï¼Œä½†å®é™…ä¸Šï¼Œæˆ‘ä»¬ä¸ç”¨å…³å¿ƒå…·ä½“çš„å«ä¹‰ï¼Œè¦åšçš„å°±æ˜¯å¦‚ä½•é€šè¿‡è¿™13ä¸ªæŒ‡æ ‡æ¨å¯¼å‡ºæœ€ç»ˆçš„æˆ¿ä»·ç»“æœã€‚

å¦‚æœä½ å­¦ä¹ äº†ä¹‹å‰çš„ç®—æ³•å®æˆ˜ï¼Œè¿™ä¸ªæ•°æ®é›†çš„é¢„æµ‹å¹¶ä¸å¤æ‚ã€‚

é¦–å…ˆåŠ è½½æ•°æ®ï¼Œå°†æ•°æ®åˆ†å‰²æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç„¶ååˆ›å»ºAdaBoostå›å½’æ¨¡å‹ï¼Œä¼ å…¥è®­ç»ƒé›†æ•°æ®è¿›è¡Œæ‹Ÿåˆï¼Œå†ä¼ å…¥æµ‹è¯•é›†æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå°±å¯ä»¥å¾—åˆ°é¢„æµ‹ç»“æœã€‚æœ€åå°†é¢„æµ‹çš„ç»“æœä¸å®é™…ç»“æœè¿›è¡Œå¯¹æ¯”ï¼Œå¾—åˆ°ä¸¤è€…ä¹‹é—´çš„è¯¯å·®ã€‚å…·ä½“ä»£ç å¦‚ä¸‹ï¼š

```
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_boston
from sklearn.ensemble import AdaBoostRegressor
# åŠ è½½æ•°æ®
data=load_boston()
# åˆ†å‰²æ•°æ®
train_x, test_x, train_y, test_y = train_test_split(data.data, data.target, test_size=0.25, random_state=33)
# ä½¿ç”¨AdaBoostå›å½’æ¨¡å‹
regressor=AdaBoostRegressor()
regressor.fit(train_x,train_y)
pred_y = regressor.predict(test_x)
mse = mean_squared_error(test_y, pred_y)
print("æˆ¿ä»·é¢„æµ‹ç»“æœ ", pred_y)
print("å‡æ–¹è¯¯å·® = ",round(mse,2))
```

è¿è¡Œç»“æœï¼š

```
æˆ¿ä»·é¢„æµ‹ç»“æœ  [20.2        10.4137931  14.63820225 17.80322581 24.58931298 21.25076923
 27.52222222 17.8372093  31.79642857 20.86428571 27.87431694 31.09142857
 12.81666667 24.13131313 12.81666667 24.58931298 17.80322581 17.66333333
 27.83       24.58931298 17.66333333 20.90823529 20.10555556 20.90823529
 28.20877193 20.10555556 21.16882129 24.58931298 13.27619048 31.09142857
 17.08095238 26.19217391  9.975      21.03404255 26.74583333 31.09142857
 25.83960396 11.859375   13.38235294 24.58931298 14.97931034 14.46699029
 30.12777778 17.66333333 26.19217391 20.10206186 17.70540541 18.45909091
 26.19217391 20.10555556 17.66333333 33.31025641 14.97931034 17.70540541
 24.64421053 20.90823529 25.83960396 17.08095238 24.58931298 21.43571429
 19.31617647 16.33733333 46.04888889 21.25076923 17.08095238 25.83960396
 24.64421053 11.81470588 17.80322581 27.63636364 23.59731183 17.94444444
 17.66333333 27.7253886  20.21465517 46.04888889 14.97931034  9.975
 17.08095238 24.13131313 21.03404255 13.4        11.859375   26.19214286
 21.25076923 21.03404255 47.11395349 16.33733333 43.21111111 31.65730337
 30.12777778 20.10555556 17.8372093  18.40833333 14.97931034 33.31025641
 24.58931298 22.88813559 18.27179487 17.80322581 14.63820225 21.16882129
 26.91538462 24.64421053 13.05       14.97931034  9.975      26.19217391
 12.81666667 26.19214286 49.46511628 13.27619048 17.70540541 25.83960396
 31.09142857 24.13131313 21.25076923 21.03404255 26.91538462 21.03404255
 21.16882129 17.8372093  12.81666667 21.03404255 21.03404255 17.08095238
 45.16666667]
å‡æ–¹è¯¯å·® =  18.05
```

è¿™ä¸ªæ•°æ®é›†æ˜¯æ¯”è¾ƒè§„èŒƒçš„ï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦åœ¨æ•°æ®æ¸…æ´—ï¼Œæ•°æ®è§„èŒƒåŒ–ä¸ŠèŠ±å¤ªå¤šç²¾åŠ›ï¼Œä»£ç ç¼–å†™èµ·æ¥æ¯”è¾ƒç®€å•ã€‚

åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒçš„å›å½’åˆ†ææ¨¡å‹åˆ†æè¿™ä¸ªæ•°æ®é›†ï¼Œæ¯”å¦‚ä½¿ç”¨å†³ç­–æ ‘å›å½’å’ŒKNNå›å½’ã€‚

ç¼–å†™ä»£ç å¦‚ä¸‹ï¼š

```
# ä½¿ç”¨å†³ç­–æ ‘å›å½’æ¨¡å‹
dec_regressor=DecisionTreeRegressor()
dec_regressor.fit(train_x,train_y)
pred_y = dec_regressor.predict(test_x)
mse = mean_squared_error(test_y, pred_y)
print("å†³ç­–æ ‘å‡æ–¹è¯¯å·® = ",round(mse,2))
# ä½¿ç”¨KNNå›å½’æ¨¡å‹
knn_regressor=KNeighborsRegressor()
knn_regressor.fit(train_x,train_y)
pred_y = knn_regressor.predict(test_x)
mse = mean_squared_error(test_y, pred_y)
print("KNNå‡æ–¹è¯¯å·® = ",round(mse,2))
```

è¿è¡Œç»“æœï¼š

```
å†³ç­–æ ‘å‡æ–¹è¯¯å·® =  23.84
KNNå‡æ–¹è¯¯å·® =  27.87
```

ä½ èƒ½çœ‹åˆ°ç›¸æ¯”ä¹‹ä¸‹ï¼ŒAdaBoostçš„å‡æ–¹è¯¯å·®æ›´å°ï¼Œä¹Ÿå°±æ˜¯ç»“æœæ›´ä¼˜ã€‚è™½ç„¶AdaBoostä½¿ç”¨äº†å¼±åˆ†ç±»å™¨ï¼Œä½†æ˜¯é€šè¿‡50ä¸ªç”šè‡³æ›´å¤šçš„å¼±åˆ†ç±»å™¨ç»„åˆèµ·æ¥è€Œå½¢æˆçš„å¼ºåˆ†ç±»å™¨ï¼Œåœ¨å¾ˆå¤šæƒ…å†µä¸‹ç»“æœéƒ½ä¼˜äºå…¶ä»–ç®—æ³•ã€‚å› æ­¤AdaBoostä¹Ÿæ˜¯å¸¸ç”¨çš„åˆ†ç±»å’Œå›å½’ç®—æ³•ä¹‹ä¸€ã€‚

## AdaBoostä¸å†³ç­–æ ‘æ¨¡å‹çš„æ¯”è¾ƒ

åœ¨sklearnä¸­AdaBoosté»˜è®¤é‡‡ç”¨çš„æ˜¯å†³ç­–æ ‘æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥éšæœºç”Ÿæˆä¸€äº›æ•°æ®ï¼Œç„¶åå¯¹æ¯”ä¸‹AdaBoostä¸­çš„å¼±åˆ†ç±»å™¨ï¼ˆä¹Ÿå°±æ˜¯å†³ç­–æ ‘å¼±åˆ†ç±»å™¨ï¼‰ã€å†³ç­–æ ‘åˆ†ç±»å™¨å’ŒAdaBoostæ¨¡å‹åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šçš„è¡¨ç°ã€‚

å¦‚æœæƒ³è¦éšæœºç”Ÿæˆæ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨sklearnä¸­çš„make\_hastie\_10\_2å‡½æ•°ç”ŸæˆäºŒåˆ†ç±»æ•°æ®ã€‚å‡è®¾æˆ‘ä»¬ç”Ÿæˆ12000ä¸ªæ•°æ®ï¼Œå–å‰2000ä¸ªä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†ã€‚

æœ‰äº†æ•°æ®å’Œè®­ç»ƒæ¨¡å‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥ç¼–å†™ä»£ç ã€‚æˆ‘è®¾ç½®äº†AdaBoostçš„è¿­ä»£æ¬¡æ•°ä¸º200ï¼Œä»£è¡¨AdaBoostç”±200ä¸ªå¼±åˆ†ç±»å™¨ç»„æˆã€‚é’ˆå¯¹è®­ç»ƒé›†ï¼Œæˆ‘ä»¬ç”¨ä¸‰ç§æ¨¡å‹åˆ†åˆ«è¿›è¡Œè®­ç»ƒï¼Œç„¶åç”¨æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å°†ä¸‰ä¸ªåˆ†ç±»å™¨çš„é”™è¯¯ç‡è¿›è¡Œå¯è§†åŒ–å¯¹æ¯”ï¼Œå¯ä»¥çœ‹åˆ°è¿™ä¸‰è€…ä¹‹é—´çš„åŒºåˆ«ï¼š

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.metrics import zero_one_loss
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import  AdaBoostClassifier
# è®¾ç½®AdaBoostè¿­ä»£æ¬¡æ•°
n_estimators=200
# ä½¿ç”¨
X,y=datasets.make_hastie_10_2(n_samples=12000,random_state=1)
# ä»12000ä¸ªæ•°æ®ä¸­å–å‰2000è¡Œä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
train_x, train_y = X[2000:],y[2000:]
test_x, test_y = X[:2000],y[:2000]
# å¼±åˆ†ç±»å™¨
dt_stump = DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)
dt_stump.fit(train_x, train_y)
dt_stump_err = 1.0-dt_stump.score(test_x, test_y)
# å†³ç­–æ ‘åˆ†ç±»å™¨
dt = DecisionTreeClassifier()
dt.fit(train_x,  train_y)
dt_err = 1.0-dt.score(test_x, test_y)
# AdaBooståˆ†ç±»å™¨
ada = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=n_estimators)
ada.fit(train_x,  train_y)
# ä¸‰ä¸ªåˆ†ç±»å™¨çš„é”™è¯¯ç‡å¯è§†åŒ–
fig = plt.figure()
# è®¾ç½®pltæ­£ç¡®æ˜¾ç¤ºä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei']
ax = fig.add_subplot(111)
ax.plot([1,n_estimators],[dt_stump_err]*2, 'k-', label=u'å†³ç­–æ ‘å¼±åˆ†ç±»å™¨ é”™è¯¯ç‡')
ax.plot([1,n_estimators],[dt_err]*2,'k--', label=u'å†³ç­–æ ‘æ¨¡å‹ é”™è¯¯ç‡')
ada_err = np.zeros((n_estimators,))
# éå†æ¯æ¬¡è¿­ä»£çš„ç»“æœ iä¸ºè¿­ä»£æ¬¡æ•°, pred_yä¸ºé¢„æµ‹ç»“æœ
for i,pred_y in enumerate(ada.staged_predict(test_x)):
     # ç»Ÿè®¡é”™è¯¯ç‡
    ada_err[i]=zero_one_loss(pred_y, test_y)
# ç»˜åˆ¶æ¯æ¬¡è¿­ä»£çš„AdaBoosté”™è¯¯ç‡ 
ax.plot(np.arange(n_estimators)+1, ada_err, label='AdaBoost Test é”™è¯¯ç‡', color='orange')
ax.set_xlabel('è¿­ä»£æ¬¡æ•°')
ax.set_ylabel('é”™è¯¯ç‡')
leg=ax.legend(loc='upper right',fancybox=True)
plt.show()
```

è¿è¡Œç»“æœï¼š

![](https://static001.geekbang.org/resource/image/8a/35/8ad4bb6a8c6848f2061ff6f442568735.png?wh=865%2A659)  
ä»å›¾ä¸­ä½ èƒ½çœ‹å‡ºæ¥ï¼Œå¼±åˆ†ç±»å™¨çš„é”™è¯¯ç‡æœ€é«˜ï¼Œåªæ¯”éšæœºåˆ†ç±»ç»“æœç•¥å¥½ï¼Œå‡†ç¡®ç‡ç¨å¾®å¤§äº50%ã€‚å†³ç­–æ ‘æ¨¡å‹çš„é”™è¯¯ç‡æ˜æ˜¾è¦ä½å¾ˆå¤šã€‚è€ŒAdaBoostæ¨¡å‹åœ¨è¿­ä»£æ¬¡æ•°è¶…è¿‡25æ¬¡ä¹‹åï¼Œé”™è¯¯ç‡æœ‰äº†æ˜æ˜¾ä¸‹é™ï¼Œç»è¿‡125æ¬¡è¿­ä»£ä¹‹åé”™è¯¯ç‡çš„å˜åŒ–å½¢åŠ¿è¶‹äºå¹³ç¼“ã€‚

å› æ­¤æˆ‘ä»¬èƒ½çœ‹å‡ºï¼Œè™½ç„¶å•ç‹¬çš„ä¸€ä¸ªå†³ç­–æ ‘å¼±åˆ†ç±»å™¨æ•ˆæœä¸å¥½ï¼Œä½†æ˜¯å¤šä¸ªå†³ç­–æ ‘å¼±åˆ†ç±»å™¨ç»„åˆèµ·æ¥å½¢æˆçš„AdaBooståˆ†ç±»å™¨ï¼Œåˆ†ç±»æ•ˆæœè¦å¥½äºå†³ç­–æ ‘æ¨¡å‹ã€‚

## æ€»ç»“

ä»Šå¤©æˆ‘å¸¦ä½ ç”¨AdaBoostå›å½’åˆ†æå¯¹æ³¢å£«é¡¿æˆ¿ä»·è¿›è¡Œäº†é¢„æµ‹ã€‚å› ä¸ºè¿™æ˜¯ä¸ªå›å½’åˆ†æçš„é—®é¢˜ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨sklearnä¸­çš„AdaBoostRegressorå³å¯ã€‚å¦‚æœæ˜¯åˆ†ç±»ï¼Œæˆ‘ä»¬ä½¿ç”¨AdaBoostClassifierã€‚

å¦å¤–æˆ‘ä»¬å°†AdaBooståˆ†ç±»å™¨ã€å¼±åˆ†ç±»å™¨å’Œå†³ç­–æ ‘åˆ†ç±»å™¨åšäº†å¯¹æ¯”ï¼Œå¯ä»¥çœ‹å‡ºç»è¿‡å¤šä¸ªå¼±åˆ†ç±»å™¨ç»„åˆå½¢æˆçš„AdaBoostå¼ºåˆ†ç±»å™¨ï¼Œå‡†ç¡®ç‡è¦æ˜æ˜¾é«˜äºå†³ç­–æ ‘ç®—æ³•ã€‚æ‰€ä»¥AdaBoostçš„ä¼˜åŠ¿åœ¨äºæ¡†æ¶æœ¬èº«ï¼Œå®ƒé€šè¿‡ä¸€ç§è¿­ä»£æœºåˆ¶è®©åŸæœ¬æ€§èƒ½ä¸å¼ºçš„åˆ†ç±»å™¨ç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå¼ºåˆ†ç±»å™¨ã€‚

å…¶å®åœ¨ç°å®å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½æ‰¾åˆ°ç±»ä¼¼çš„æ¡ˆä¾‹ã€‚IBMæœåŠ¡å™¨è¿½æ±‚çš„æ˜¯å•ä¸ªæœåŠ¡å™¨æ€§èƒ½çš„å¼ºå¤§ï¼Œæ¯”å¦‚æ‰“é€ è¶…çº§æœåŠ¡å™¨ã€‚è€ŒGoogleåœ¨åˆ›å»ºé›†ç¾¤çš„æ—¶å€™ï¼Œåˆ©ç”¨äº†å¾ˆå¤šPCçº§çš„æœåŠ¡å™¨ï¼Œå°†å®ƒä»¬ç»„æˆé›†ç¾¤ï¼Œæ•´ä½“æ€§èƒ½è¿œæ¯”ä¸€ä¸ªè¶…çº§æœåŠ¡å™¨çš„æ€§èƒ½å¼ºå¤§ã€‚

å†æ¯”å¦‚æˆ‘ä»¬è®²çš„â€œä¸‰ä¸ªè‡­çš®åŒ ï¼Œé¡¶ä¸ªè¯¸è‘›äº®â€ï¼Œä¹Ÿå°±æ˜¯AdaBoostçš„ä»·å€¼æ‰€åœ¨ã€‚

![](https://static001.geekbang.org/resource/image/6c/17/6c4fcd75a65dc354bc65590c18e77d17.png?wh=1638%2A822)  
ä»Šå¤©æˆ‘ä»¬ç”¨AdaBooståˆ†ç±»å™¨ä¸å†³ç­–æ ‘åˆ†ç±»åšå¯¹æ¯”çš„æ—¶å€™ï¼Œä½¿ç”¨åˆ°äº†sklearnä¸­çš„make\_hastie\_10\_2å‡½æ•°ç”Ÿæˆæ•°æ®ã€‚å®é™…ä¸Šåœ¨[ç¬¬19ç¯‡](http://time.geekbang.org/column/article/79072)ï¼Œæˆ‘ä»¬å¯¹æ³°å¦å°¼å…‹å·çš„ä¹˜å®¢åšç”Ÿå­˜é¢„æµ‹çš„æ—¶å€™ï¼Œä¹Ÿè®²åˆ°äº†å†³ç­–æ ‘å·¥å…·çš„ä½¿ç”¨ã€‚ä½ èƒ½ä¸èƒ½ç¼–å†™ä»£ç ï¼Œä½¿ç”¨AdaBoostç®—æ³•å¯¹æ³°å¦å°¼å…‹å·ä¹˜å®¢çš„ç”Ÿå­˜åšé¢„æµ‹ï¼Œçœ‹çœ‹å®ƒå’Œå†³ç­–æ ‘æ¨¡å‹ï¼Œè°çš„å‡†ç¡®ç‡æ›´é«˜ï¼Ÿ

ä½ ä¹Ÿå¯ä»¥æŠŠè¿™ç¯‡æ–‡ç« åˆ†äº«ç»™ä½ çš„æœ‹å‹æˆ–è€…åŒäº‹ï¼Œä¸€èµ·åˆ‡ç£‹ä¸€ä¸‹ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>TKbook</span> ğŸ‘ï¼ˆ18ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æºä»£ç ä¸­ï¼š
# ä» 12000 ä¸ªæ•°æ®ä¸­å–å‰ 2000 è¡Œä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
test_x, test_y = X[2000:],y[2000:]
train_x, train_y = X[:2000],y[:2000]

è¿™ä¸ªéƒ¨åˆ†çš„ä»£ç å†™é”™äº†å§
åº”è¯¥æ˜¯ï¼š
test_x, test_y = x[: 2000], y[: 2000]
train_x, train_y = x[2000:], y[2000:]</p>2019-03-05</li><br/><li><span>third</span> ğŸ‘ï¼ˆ7ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç»“æœä»ç„¶ä¸ºAdaBoostç®—æ³•æœ€ä¼˜ã€‚
ä¸ªäººå‘ç°ï¼Œå‰ä¸¤ä¸ªåˆ†ç±»å™¨å‡ºç»“æœå¾ˆå¿«
åˆ†ææœ€ä¼˜ï¼š
1.AdaBoostç®—æ³•ç»è¿‡äº†æ›´å¤šè¿ç®—ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿­ä»£å¼±åˆ†ç±»å™¨å’Œç»„åˆä¸Š
2.è‰¯å¥½ç»„åˆèµ·æ¥çš„ä¸ªä½“ï¼Œèƒ½å¤Ÿåˆ›é€ æ›´å¤§çš„ä»·å€¼ã€‚

å†³ç­–æ ‘å¼±åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º 0.7867
å†³ç­–æ ‘åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º 0.7891
AdaBoost åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º 0.8138

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.feature_extraction import DictVectorizer

# 1.æ•°æ®åŠ è½½
train_data=pd.read_csv(&#39;.&#47;Titanic_Data&#47;train.csv&#39;)
test_data=pd.read_csv(&#39;.&#47;Titanic_Data&#47;test.csv&#39;)

# 2.æ•°æ®æ¸…æ´—
# ä½¿ç”¨å¹³å‡å¹´é¾„æ¥å¡«å……å¹´é¾„ä¸­çš„ NaN å€¼
train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)
test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(),inplace=True)
# å‡ä»·å¡«å……
train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(),inplace=True)
test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(),inplace=True)
# ä½¿ç”¨ç™»é™†æœ€å¤šçš„æ¸¯å£æ¥å¡«å……
train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)
test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)

# ç‰¹å¾é€‰æ‹©
features=[&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Embarked&#39;]
train_features=train_data[features]
train_labels=train_data[&#39;Survived&#39;]
test_features=test_data[features]

# å°†ç¬¦å·åŒ–çš„Embarkedå¯¹è±¡æŠ½è±¡å¤„ç†æˆ0&#47;1è¿›è¡Œè¡¨ç¤º
dvec=DictVectorizer(sparse=False)
train_features=dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))
test_features=dvec.transform(test_features.to_dict(orient=&#39;record&#39;))

# å†³ç­–æ ‘å¼±åˆ†ç±»å™¨
dt_stump = DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)
dt_stump.fit(train_features, train_labels)

print(u&#39;å†³ç­–æ ‘å¼±åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º %.4lf&#39; % np.mean(cross_val_score(dt_stump, train_features, train_labels, cv=10)))

# å†³ç­–æ ‘åˆ†ç±»å™¨
dt = DecisionTreeClassifier()
dt.fit(train_features, train_labels)

print(u&#39;å†³ç­–æ ‘åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º %.4lf&#39; % np.mean(cross_val_score(dt, train_features, train_labels, cv=10)))

# AdaBoost åˆ†ç±»å™¨
ada = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=200)
ada.fit(train_features, train_labels)

print(u&#39;AdaBoost åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º %.4lf&#39; % np.mean(cross_val_score(ada, train_features, train_labels, cv=10)))</p>2019-03-04</li><br/><li><span>Geek_hve78z</span> ğŸ‘ï¼ˆ6ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç”±äºä¹˜å®¢æµ‹è¯•é›†ç¼ºå¤±çœŸå®å€¼ï¼Œé‡‡ç”¨ K æŠ˜äº¤å‰éªŒè¯å‡†ç¡®ç‡
--------------------
è¿è¡Œç»“æœï¼š
å†³ç­–æ ‘å¼±åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º 0.7867
å†³ç­–æ ‘åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º 0.7813
AdaBoost åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º 0.8138
-------------------------
ä»£ç ï¼š
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import  AdaBoostClassifier
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import cross_val_score

# è®¾ç½® AdaBoost è¿­ä»£æ¬¡æ•°
n_estimators=200

# æ•°æ®åŠ è½½
train_data=pd.read_csv(&#39;.&#47;Titanic_Data&#47;train.csv&#39;)
test_data=pd.read_csv(&#39;.&#47;Titanic_Data&#47;test.csv&#39;)

# æ¨¡å— 2ï¼šæ•°æ®æ¸…æ´—
# ä½¿ç”¨å¹³å‡å¹´é¾„æ¥å¡«å……å¹´é¾„ä¸­çš„ NaN å€¼
train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)
test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(),inplace=True)
# ä½¿ç”¨ç¥¨ä»·çš„å‡å€¼å¡«å……ç¥¨ä»·ä¸­çš„ nan å€¼
train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(),inplace=True)
test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(),inplace=True)
# ä½¿ç”¨ç™»å½•æœ€å¤šçš„æ¸¯å£æ¥å¡«å……ç™»å½•æ¸¯å£Embarkedçš„ nan å€¼
train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)
test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)

# ç‰¹å¾é€‰æ‹©
features=[&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Embarked&#39;]
train_features=train_data[features]
train_labels=train_data[&#39;Survived&#39;]
test_features=test_data[features]

# å°†ç¬¦å·åŒ–çš„Embarkedå¯¹è±¡å¤„ç†æˆ0&#47;1è¿›è¡Œè¡¨ç¤º
dvec=DictVectorizer(sparse=False)
train_features=dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))
test_features=dvec.transform(test_features.to_dict(orient=&#39;record&#39;))

# å†³ç­–æ ‘å¼±åˆ†ç±»å™¨
dt_stump = DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)
dt_stump.fit(train_features, train_labels)

print(u&#39;å†³ç­–æ ‘å¼±åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º %.4lf&#39; % np.mean(cross_val_score(dt_stump, train_features, train_labels, cv=10)))

# å†³ç­–æ ‘åˆ†ç±»å™¨
dt = DecisionTreeClassifier()
dt.fit(train_features, train_labels)

print(u&#39;å†³ç­–æ ‘åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º %.4lf&#39; % np.mean(cross_val_score(dt, train_features, train_labels, cv=10)))

# AdaBoost åˆ†ç±»å™¨
ada = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=n_estimators)
ada.fit(train_features, train_labels)

print(u&#39;AdaBoost åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º %.4lf&#39; % np.mean(cross_val_score(ada, train_features, train_labels, cv=10)))</p>2019-03-04</li><br/><li><span>æ¢æ—æ¾</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è·‘ç¬¬äºŒå—ä»£ç æ˜¯éœ€è¦å¼•å…¥ä¸¤ä¸ªæ¨¡å—
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
</p>2019-03-04</li><br/><li><span>Liam</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ax = fig.add_subplot(111)ax.plot([1,n_estimators],[dt_stump_err]*2, &#39;k-&#39;, label=u&#39;å†³ç­–æ ‘å¼±åˆ†ç±»å™¨ é”™è¯¯ç‡&#39;)ax.plot([1,n_estimators],[dt_err]*2,&#39;k--&#39;, label=u&#39;å†³ç­–æ ‘æ¨¡å‹ é”™è¯¯ç‡&#39;)ada_err = np.zeros((n_estimators,)).  ç–‘é—®ï¼šè¿™é‡Œ*2æ˜¯ä»€ä¹ˆæ„æ€ï¼Œèƒ½è§£æä¸‹ä»£ç å—ï¼Ÿ</p>2021-03-26</li><br/><li><span>æ»¢</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>å¾—åˆ°ç»“æœï¼š
CARTå†³ç­–æ ‘KæŠ˜äº¤å‰éªŒè¯å‡†ç¡®ç‡: 0.39480897860892333
AdaBoostKæŠ˜äº¤å‰éªŒè¯å‡†ç¡®ç‡: 0.4376641797318339

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import cross_val_predict
import pandas as pd
import numpy as np

#è¯»å–æ•°æ®
path = &#39;&#47;Users&#47;apple&#47;Desktop&#47;GitHubProject&#47;Read mark&#47;æ•°æ®åˆ†æ&#47;geekTime&#47;data&#47;&#39;
train_data = pd.read_csv(path + &#39;Titannic_Data_train.csv&#39;)
test_data = pd.read_csv(path + &#39;Titannic_Data_test.csv&#39;)

#æ•°æ®æ¸…æ´—
train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)
test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)
train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)
test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)

#ç‰¹å¾é€‰æ‹©
features = [&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Embarked&#39;]
train_features = train_data[features]
train_result = train_data[&#39;Survived&#39;]
test_features = test_data[features]
devc = DictVectorizer(sparse=False)
train_features = devc.fit_transform(train_features.to_dict(orient=&#39;record&#39;))
test_features = devc.fit_transform(test_features.to_dict(orient=&#39;record&#39;))

#æ„é€ å†³ç­–æ ‘ï¼Œè¿›è¡Œé¢„æµ‹
tree_regressor = DecisionTreeRegressor()
tree_regressor.fit(train_features,train_result)
predict_tree = tree_regressor.predict(test_features)
#äº¤å‰éªŒè¯å‡†ç¡®ç‡
print(&#39;CARTå†³ç­–æ ‘KæŠ˜äº¤å‰éªŒè¯å‡†ç¡®ç‡:&#39;, np.mean(cross_val_predict(tree_regressor,train_features,train_result,cv=10)))

#æ„é€ AdaBoost
ada_regressor = AdaBoostRegressor()
ada_regressor.fit(train_features,train_result)
predict_ada = ada_regressor.predict(test_features)
#äº¤å‰éªŒè¯å‡†ç¡®ç‡
print(&#39;AdaBoostKæŠ˜äº¤å‰éªŒè¯å‡†ç¡®ç‡:&#39;,np.mean(cross_val_predict(ada_regressor,train_features,train_result,cv=10)))
</p>2019-04-21</li><br/><li><span>å°æ™¨</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>å¼±åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º 0.7868
å†³ç­–æ ‘åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º 0.7823
AdaBooståˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º:0.8115

#!&#47;usr&#47;bin&#47;env python
# -*- coding:utf-8 -*-
# Author:Peter

import numpy as np
import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

# è¿­ä»£æ¬¡æ•°
n_estimators = 200
train_data = pd.read_csv(r&#39;data&#47;Titanic_Data_train.csv&#39;)
test_data = pd.read_csv(r&#39;data&#47;Titanic_Data_Test.csv&#39;)

# ç”¨å¹³å‡å¹´é¾„å°†ç¼ºå¤±çš„å¹´é¾„è¡¥é½
train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)
test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)

# ç”¨å¹³å‡ç¥¨ä»·å°†ç¼ºå¤±çš„ç¥¨ä»·è¡¥é½
train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)
test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(), inplace=True)

# ç”¨ç™»èˆ¹æ¸¯å£æœ€å¤šçš„Sè¡¥é½ç¼ºå¤±
train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)
test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)

# å°†å¯ç”¨æ¥åˆ†ç±»çš„æ•°æ®æ”¾åˆ°è®­ç»ƒé›†ä¸­
features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]
train_features = train_data[features]
train_labels = train_data[&#39;Survived&#39;]
test_features = test_data[features]

# å­—ç¬¦ä¸²æ•°æ®è§„èŒƒåŒ–ï¼Œè½¬ä¸ºintå‹
dvec = DictVectorizer(sparse=False)
train_features = dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))
test_features = dvec.transform(test_features.to_dict(orient=&#39;record&#39;))

# å¼±åˆ†ç±»å™¨
dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)
dt_stump.fit(train_features, train_labels)
print(u&#39;å¼±åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º %.4lf&#39; % dt_stump.score(train_features, train_labels))
# å†³ç­–æ ‘åˆ†ç±»å™¨
dt = DecisionTreeClassifier()
dt.fit(train_features, train_labels)
print(u&#39;å†³ç­–æ ‘åˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º %.4lf&#39; % np.mean(cross_val_score(dt, train_features, train_labels, cv=10)))

# AdaBooståˆ†ç±»å™¨
ada = AdaBoostClassifier(base_estimator=dt_stump, n_estimators=n_estimators)
ada.fit(train_features, train_labels)
ada_score = np.mean(cross_val_score(ada, train_features, train_labels, cv=10))
print(&quot;AdaBooståˆ†ç±»å™¨å‡†ç¡®ç‡ä¸º:%.4lf&quot; % ada_score)
</p>2021-03-10</li><br/><li><span>èŒè¾°</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>åœ¨AdaBoostã€å†³ç­–æ ‘å›å½’ã€KNNæˆ¿ä»·é¢„æµ‹å¯¹æ¯”ä¸­å‘ç°ï¼Œéšæœºç§å­å¯¹å†³ç­–æ ‘çš„é¢„æµ‹ç»“æœæœ‰å½±å“ã€‚
åˆ†åˆ«æµ‹è¯•äº†ä¸‰ç§ä¸åŒçš„éšæœºç§å­ï¼š
dec_regressor=DecisionTreeRegressor(random_state=1)
dec_regressor=DecisionTreeRegressor(random_state=20)
dec_regressor=DecisionTreeRegressor(random_state=30)
æµ‹è¯•ç»“æœä¸ºï¼š
å†³ç­–æ ‘å‡æ–¹è¯¯å·®1 =  36.65
å†³ç­–æ ‘å‡æ–¹è¯¯å·®20 =  25.54
å†³ç­–æ ‘å‡æ–¹è¯¯å·®30 =  37.19
æ€è€ƒï¼š
æ­¤å¤„è€ƒè™‘è¿™é‡Œæ²¡æœ‰é™åˆ¶ç§å­çš„éšæœºæ€§ï¼Œå¯¹æ¯”çš„ç»“æœå¯èƒ½è¿‡äºéšæœºäº†ï¼Œæ— æ³•çœŸå®åæ˜ ç®—æ³•æ•ˆæœï¼Œä¸¤ç§ç®—æ³•åŸç†ä¸­éšæœºç§å­çš„åº”ç”¨æƒ…å†µä¸åŒã€‚æ€è€ƒæ˜¯ä¸æ˜¯é‡‡ç”¨å¤šæ¬¡éšæœºMSEç»“æœæ±‚å¹³å‡çš„æ–¹æ³•ä½œä¸ºã€æ¯”è¾ƒé¡¹ã€‘æ›´ä¸ºåˆé€‚
KNNç®—æ³•æ— éšæœºç§å­å½±å“ã€‚</p>2020-07-05</li><br/><li><span>Â§mcÂ²ompleXWr</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ä½¿ç”¨è‡ªå¸¦çš„æ•°æ®é›†å°±ä¸ç”¨åšæ•°æ®è§„èŒƒåŒ–ä¹ˆï¼Ÿ</p>2020-06-09</li><br/><li><span>é²¨é±¼é²¸é±¼é³„é±¼</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œè¯·é—®AdaBoostæ¨¡å‹åœ¨é¢„æµ‹å‰éœ€ä¸éœ€è¦å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–æˆ–è€…å½’ä¸€åŒ–ï¼Œåšæœ‰ä»€ä¹ˆå¥½å¤„ï¼Œä¸åšæœ‰ä»€ä¹ˆå¥½å¤„å‘¢</p>2020-05-25</li><br/><li><span>å¼ è´º</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆè®²çš„å¾ˆæ¸…æ™°</p>2020-03-27</li><br/><li><span>æ»¨æ»¨</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>åˆ†ç±»å’Œå›å½’éƒ½æ˜¯åšé¢„æµ‹ï¼Œåˆ†ç±»æ˜¯ç¦»æ•£å€¼ï¼Œå›å½’æ˜¯è¿ç»­å€¼</p>2019-04-21</li><br/><li><span>JingZ</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p># AdaBoost
ä¸€å¼€å§‹ç«Ÿç„¶è“¦ç„¶æƒ¯æ€§ç”¨äº†AdaBoostRegressorï¼Œå¾—åˆ°0.33çš„å‡†ç¡®ç‡ï¼Œæœ€åçœ‹äº†å°ä¼™ä¼´ä»£ç ï¼Œç«‹é©¬ä¿®æ­£

æ„Ÿè§‰ç®—æ³•ä»£ç ä¸å¤æ‚ï¼Œå…³é”®è¦è‡ªå·±ä»ç©ºç™½å¼€å§‹å†™ï¼Œè¿˜éœ€å¤šå®æˆ˜

from sklearn.ensemble import AdaBoostClassifier

# ä½¿ç”¨ Adaboost åˆ†ç±»æ¨¡å‹
ada = AdaBoostClassifier()
ada.fit(train_features, train_labels)

pred_labels = ada.predict(test_features)

acc_ada_classifier = round(ada.score(train_features, train_labels), 6)
print(u&#39;Adaboost score å‡†ç¡®ç‡ä¸º %.4lf&#39; % acc_ada_classifier)
print(u&#39;Adaboost cross_val_score å‡†ç¡®ç‡ä¸º %.4lf&#39; % np.mean(cross_val_score(ada, train_features, train_labels, cv=10)))

è¿è¡Œ
Adaboost score å‡†ç¡®ç‡ä¸º 0.8339
Adaboost cross_val_score å‡†ç¡®ç‡ä¸º 0.8104</p>2019-03-05</li><br/><li><span>FORWARDâ€•MOUNT</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œæˆ¿ä»·é¢„æµ‹è¿™ä¸ªç®—æ³•ï¼Œ50ä¸ªå¼±åˆ†ç±»å™¨æ˜¯æ€ä¹ˆæ¥çš„ï¼Ÿ</p>2019-03-05</li><br/><li><span>ä½³ä½³çš„çˆ¸</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ä½ å¥½è€å¸ˆï¼Œå®Œæ•´çš„æºä»£ç åœ¨å“ªé‡Œå¯ä»¥ä¸‹è½½åˆ°?  æˆ‘è¯´çš„æ˜¯æ¯èŠ‚è¯¾é‡Œè¾¹çš„æºä»£ç ã€‚</p>2019-03-04</li><br/>
</ul>