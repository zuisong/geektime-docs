ä¸Šä¸€è®²ä¸­æˆ‘ç»™ä½ è®²äº†å¦‚ä½•ä½¿ç”¨å…«çˆªé±¼é‡‡é›†æ•°æ®ï¼Œå¯¹äºæ•°æ®é‡‡é›†åˆšåˆšå…¥é—¨çš„äººæ¥è¯´ï¼Œåƒå…«çˆªé±¼è¿™ç§å¯è§†åŒ–çš„é‡‡é›†æ˜¯ä¸€ç§éå¸¸å¥½çš„æ–¹å¼ã€‚å®ƒæœ€å¤§çš„ä¼˜ç‚¹å°±æ˜¯ä¸Šæ‰‹é€Ÿåº¦å¿«ï¼Œå½“ç„¶ä¹Ÿå­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚è¿è¡Œé€Ÿåº¦æ…¢ã€å¯æ§æ€§å·®ç­‰ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œçˆ¬è™«å¯ä»¥å¾ˆå¥½åœ°é¿å…è¿™äº›é—®é¢˜ï¼Œä»Šå¤©æˆ‘æ¥åˆ†äº«ä¸‹å¦‚ä½•é€šè¿‡ç¼–å†™çˆ¬è™«æŠ“å–æ•°æ®ã€‚

## çˆ¬è™«çš„æµç¨‹

ç›¸ä¿¡ä½ å¯¹â€œçˆ¬è™«â€è¿™ä¸ªè¯å·²ç»éå¸¸ç†Ÿæ‚‰äº†ï¼Œçˆ¬è™«å®é™…ä¸Šæ˜¯ç”¨æµè§ˆå™¨è®¿é—®çš„æ–¹å¼æ¨¡æ‹Ÿäº†è®¿é—®ç½‘ç«™çš„è¿‡ç¨‹ï¼Œæ•´ä¸ªè¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ‰“å¼€ç½‘é¡µã€æå–æ•°æ®å’Œä¿å­˜æ•°æ®ã€‚

åœ¨Pythonä¸­ï¼Œè¿™ä¸‰ä¸ªé˜¶æ®µéƒ½æœ‰å¯¹åº”çš„å·¥å…·å¯ä»¥ä½¿ç”¨ã€‚

åœ¨â€œæ‰“å¼€ç½‘é¡µâ€è¿™ä¸€æ­¥éª¤ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ Requests è®¿é—®é¡µé¢ï¼Œå¾—åˆ°æœåŠ¡å™¨è¿”å›ç»™æˆ‘ä»¬çš„æ•°æ®ï¼Œè¿™é‡ŒåŒ…æ‹¬HTMLé¡µé¢ä»¥åŠJSONæ•°æ®ã€‚

åœ¨â€œæå–æ•°æ®â€è¿™ä¸€æ­¥éª¤ä¸­ï¼Œä¸»è¦ç”¨åˆ°äº†ä¸¤ä¸ªå·¥å…·ã€‚é’ˆå¯¹HTMLé¡µé¢ï¼Œå¯ä»¥ä½¿ç”¨ XPath è¿›è¡Œå…ƒç´ å®šä½ï¼Œæå–æ•°æ®ï¼›é’ˆå¯¹JSONæ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨JSONè¿›è¡Œè§£æã€‚

åœ¨æœ€åä¸€æ­¥â€œä¿å­˜æ•°æ®â€ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Pandas ä¿å­˜æ•°æ®ï¼Œæœ€åå¯¼å‡ºCSVæ–‡ä»¶ã€‚

ä¸‹é¢æˆ‘æ¥åˆ†åˆ«ä»‹ç»ä¸‹è¿™äº›å·¥å…·çš„ä½¿ç”¨ã€‚

**Requestsè®¿é—®é¡µé¢**

Requestsæ˜¯Python HTTPçš„å®¢æˆ·ç«¯åº“ï¼Œç¼–å†™çˆ¬è™«çš„æ—¶å€™éƒ½ä¼šç”¨åˆ°ï¼Œç¼–å†™èµ·æ¥ä¹Ÿå¾ˆç®€å•ã€‚å®ƒæœ‰ä¸¤ç§è®¿é—®æ–¹å¼ï¼šGetå’ŒPostã€‚è¿™ä¸¤è€…æœ€ç›´è§‚çš„åŒºåˆ«å°±æ˜¯ï¼šGetæŠŠå‚æ•°åŒ…å«åœ¨urlä¸­ï¼Œè€ŒPosté€šè¿‡request bodyæ¥ä¼ é€’å‚æ•°ã€‚

å‡è®¾æˆ‘ä»¬æƒ³è®¿é—®è±†ç“£ï¼Œé‚£ä¹ˆç”¨Getè®¿é—®çš„è¯ï¼Œä»£ç å¯ä»¥å†™æˆä¸‹é¢è¿™æ ·çš„ï¼š

```
r = requests.get('http://www.douban.com')
```

ä»£ç é‡Œçš„â€œrâ€å°±æ˜¯Getè¯·æ±‚åçš„è®¿é—®ç»“æœï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨r.textæˆ–r.contentæ¥è·å–HTMLçš„æ­£æ–‡ã€‚

å¦‚æœæˆ‘ä»¬æƒ³è¦ä½¿ç”¨Postè¿›è¡Œè¡¨å•ä¼ é€’ï¼Œä»£ç å°±å¯ä»¥è¿™æ ·å†™ï¼š

```
r = requests.post('http://xxx.com', data = {'key':'value'})
```

è¿™é‡Œdataå°±æ˜¯ä¼ é€’çš„è¡¨å•å‚æ•°ï¼Œdataçš„æ•°æ®ç±»å‹æ˜¯ä¸ªå­—å…¸çš„ç»“æ„ï¼Œé‡‡ç”¨keyå’Œvalueçš„æ–¹å¼è¿›è¡Œå­˜å‚¨ã€‚

**XPathå®šä½**

XPathæ˜¯XMLçš„è·¯å¾„è¯­è¨€ï¼Œå®é™…ä¸Šæ˜¯é€šè¿‡å…ƒç´ å’Œå±æ€§è¿›è¡Œå¯¼èˆªï¼Œå¸®æˆ‘ä»¬å®šä½ä½ç½®ã€‚å®ƒæœ‰å‡ ç§å¸¸ç”¨çš„è·¯å¾„è¡¨è¾¾æ–¹å¼ã€‚

![](https://static001.geekbang.org/resource/image/3b/ea/3bcb311361c76bfbeb90d360b21195ea.jpg?wh=518%2A690)

æˆ‘æ¥ç»™ä½ ç®€å•ä¸¾ä¸€äº›ä¾‹å­ï¼š

1. xpath(â€˜nodeâ€™) é€‰å–äº†nodeèŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ï¼›
2. xpath(â€™/divâ€™) ä»æ ¹èŠ‚ç‚¹ä¸Šé€‰å–divèŠ‚ç‚¹ï¼›
3. xpath(â€™//divâ€™) é€‰å–æ‰€æœ‰çš„divèŠ‚ç‚¹ï¼›
4. xpath(â€™./divâ€™) é€‰å–å½“å‰èŠ‚ç‚¹ä¸‹çš„divèŠ‚ç‚¹ï¼›
5. xpath(â€™â€¦â€™) å›åˆ°ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼›
6. xpath(â€™//@idâ€™) é€‰å–æ‰€æœ‰çš„idå±æ€§ï¼›
7. xpath(â€™//book\[@id]â€™) é€‰å–æ‰€æœ‰æ‹¥æœ‰åä¸ºidçš„å±æ€§çš„bookå…ƒç´ ï¼›
8. xpath(â€™//book\[@id=â€œabcâ€]â€™) é€‰å–æ‰€æœ‰bookå…ƒç´ ï¼Œä¸”è¿™äº›bookå…ƒç´ æ‹¥æœ‰id= "abc"çš„å±æ€§ï¼›
9. xpath(â€™//book/title | //book/priceâ€™) é€‰å–bookå…ƒç´ çš„æ‰€æœ‰titleå’Œpriceå…ƒç´ ã€‚

ä¸Šé¢æˆ‘åªæ˜¯åˆ—ä¸¾äº†XPathçš„éƒ¨åˆ†åº”ç”¨ï¼ŒXPathçš„é€‰æ‹©åŠŸèƒ½éå¸¸å¼ºå¤§ï¼Œå®ƒå¯ä»¥æä¾›è¶…è¿‡100ä¸ªå†…å»ºå‡½æ•°ï¼Œæ¥åšåŒ¹é…ã€‚æˆ‘ä»¬æƒ³è¦å®šä½çš„èŠ‚ç‚¹ï¼Œå‡ ä¹éƒ½å¯ä»¥ä½¿ç”¨XPathæ¥é€‰æ‹©ã€‚

ä½¿ç”¨XPathå®šä½ï¼Œä½ ä¼šç”¨åˆ°Pythonçš„ä¸€ä¸ªè§£æåº“lxmlã€‚è¿™ä¸ªåº“çš„è§£ææ•ˆç‡éå¸¸é«˜ï¼Œä½¿ç”¨èµ·æ¥ä¹Ÿå¾ˆç®€ä¾¿ï¼Œåªéœ€è¦è°ƒç”¨HTMLè§£æå‘½ä»¤å³å¯ï¼Œç„¶åå†å¯¹HTMLè¿›è¡ŒXPathå‡½æ•°çš„è°ƒç”¨ã€‚

æ¯”å¦‚æˆ‘ä»¬æƒ³è¦å®šä½åˆ°HTMLä¸­çš„æ‰€æœ‰åˆ—è¡¨é¡¹ç›®ï¼Œå¯ä»¥é‡‡ç”¨ä¸‹é¢è¿™æ®µä»£ç ã€‚

```
from lxml import etree
html = etree.HTML(html)
result = html.xpath('//li')
```

**JSONå¯¹è±¡**

JSONæ˜¯ä¸€ç§è½»é‡çº§çš„äº¤äº’æ–¹å¼ï¼Œåœ¨Pythonä¸­æœ‰JSONåº“ï¼Œå¯ä»¥è®©æˆ‘ä»¬å°†Pythonå¯¹è±¡å’ŒJSONå¯¹è±¡è¿›è¡Œè½¬æ¢ã€‚ä¸ºä»€ä¹ˆè¦è½¬æ¢å‘¢ï¼ŸåŸå› ä¹Ÿå¾ˆç®€å•ã€‚å°†JSONå¯¹è±¡è½¬æ¢æˆä¸ºPythonå¯¹è±¡ï¼Œæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œè§£æå°±æ›´æ–¹ä¾¿äº†ã€‚

![](https://static001.geekbang.org/resource/image/9a/43/9a6d6564a64cf2b1c256265eea78c543.png?wh=477%2A237)

è¿™æ˜¯ä¸€æ®µå°†JSONæ ¼å¼è½¬æ¢æˆPythonå¯¹è±¡çš„ä»£ç ï¼Œä½ å¯ä»¥è‡ªå·±è¿è¡Œä¸‹è¿™ä¸ªç¨‹åºçš„ç»“æœã€‚

```
import json
jsonData = '{"a":1,"b":2,"c":3,"d":4,"e":5}';
input = json.loads(jsonData)
print input
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±è¦è¿›è¡Œå®æˆ˜äº†ï¼Œæˆ‘ä¼šä»ä¸¤ä¸ªè§’åº¦ç»™ä½ è®²è§£å¦‚ä½•ä½¿ç”¨Pythonçˆ¬å–æµ·æŠ¥ï¼Œä¸€ä¸ªæ˜¯é€šè¿‡JSONæ•°æ®çˆ¬å–ï¼Œä¸€ä¸ªæ˜¯é€šè¿‡XPathå®šä½çˆ¬å–ã€‚

## å¦‚ä½•ä½¿ç”¨JSONæ•°æ®è‡ªåŠ¨ä¸‹è½½ç‹ç¥–è´¤çš„æµ·æŠ¥

æˆ‘åœ¨ä¸Šé¢è®²äº†Pythonçˆ¬è™«çš„åŸºæœ¬åŸç†å’Œå®ç°çš„å·¥å…·ï¼Œä¸‹é¢æˆ‘ä»¬æ¥å®æˆ˜ä¸€ä¸‹ã€‚å¦‚æœæƒ³è¦ä»è±†ç“£å›¾ç‰‡ä¸­ä¸‹è½½ç‹ç¥–è´¤çš„æµ·æŠ¥ï¼Œä½ åº”è¯¥å…ˆæŠŠæˆ‘ä»¬æ—¥å¸¸çš„æ“ä½œæ­¥éª¤æ•´ç†ä¸‹æ¥ï¼š

1. æ‰“å¼€ç½‘é¡µï¼›
2. è¾“å…¥å…³é”®è¯â€œç‹ç¥–è´¤â€ï¼›
3. åœ¨æœç´¢ç»“æœé¡µä¸­é€‰æ‹©â€œå›¾ç‰‡â€ï¼›
4. ä¸‹è½½å›¾ç‰‡é¡µä¸­çš„æ‰€æœ‰æµ·æŠ¥ã€‚

è¿™é‡Œä½ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœçˆ¬å–çš„é¡µé¢æ˜¯åŠ¨æ€é¡µé¢ï¼Œå°±éœ€è¦å…³æ³¨XHRæ•°æ®ã€‚å› ä¸ºåŠ¨æ€é¡µé¢çš„åŸç†å°±æ˜¯é€šè¿‡åŸç”Ÿçš„XHRæ•°æ®å¯¹è±¡å‘å‡ºHTTPè¯·æ±‚ï¼Œå¾—åˆ°æœåŠ¡å™¨è¿”å›çš„æ•°æ®åï¼Œå†è¿›è¡Œå¤„ç†ã€‚XHRä¼šç”¨äºåœ¨åå°ä¸æœåŠ¡å™¨äº¤æ¢æ•°æ®ã€‚

ä½ éœ€è¦ä½¿ç”¨æµè§ˆå™¨çš„æ’ä»¶æŸ¥çœ‹XHRæ•°æ®ï¼Œæ¯”å¦‚åœ¨Chromeæµè§ˆå™¨ä¸­ä½¿ç”¨å¼€å‘è€…å·¥å…·ã€‚

åœ¨è±†ç“£æœç´¢ä¸­ï¼Œæˆ‘ä»¬å¯¹â€œç‹ç¥–è´¤â€è¿›è¡Œäº†æ¨¡æ‹Ÿï¼Œå‘ç°XHRæ•°æ®ä¸­æœ‰ä¸€ä¸ªè¯·æ±‚æ˜¯è¿™æ ·çš„ï¼š

[https://www.douban.com/j/search\_photo?q=%E7%8E%8B%E7%A5%96%E8%B4%A4&amp;limit=20&amp;start=0](https://www.douban.com/j/search_photo?q=%E7%8E%8B%E7%A5%96%E8%B4%A4&limit=20&start=0)

urlä¸­çš„ä¹±ç æ­£æ˜¯ä¸­æ–‡çš„urlç¼–ç ï¼Œæ‰“å¼€åï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¾ˆæ¸…çˆ½çš„JSONæ ¼å¼å¯¹è±¡ï¼Œå±•ç¤ºçš„å½¢å¼æ˜¯è¿™æ ·çš„ï¼š

```
{"images":
       [{"src": â€¦, "author": â€¦, "url":â€¦, "id": â€¦, "title": â€¦, "width":â€¦, "height":â€¦},
    â€¦
	 {"src": â€¦, "author": â€¦, "url":â€¦, "id": â€¦, "title": â€¦, "width":â€¦, "height":â€¦}],
 "total":22471,"limit":20,"more":true}
```

ä»è¿™ä¸ªJSONå¯¹è±¡ä¸­ï¼Œæˆ‘ä»¬èƒ½çœ‹åˆ°ï¼Œç‹ç¥–è´¤çš„å›¾ç‰‡ä¸€å…±æœ‰22471å¼ ï¼Œå…¶ä¸­ä¸€æ¬¡åªè¿”å›äº†20å¼ ï¼Œè¿˜æœ‰æ›´å¤šçš„æ•°æ®å¯ä»¥è¯·æ±‚ã€‚æ•°æ®è¢«æ”¾åˆ°äº†imageså¯¹è±¡é‡Œï¼Œå®ƒæ˜¯ä¸ªæ•°ç»„çš„ç»“æ„ï¼Œæ¯ä¸ªæ•°ç»„çš„å…ƒç´ æ˜¯ä¸ªå­—å…¸çš„ç±»å‹ï¼Œåˆ†åˆ«å‘Šè¯‰äº†srcã€authorã€urlã€idã€titleã€widthå’Œheightå­—æ®µï¼Œè¿™äº›å­—æ®µä»£è¡¨çš„å«ä¹‰åˆ†åˆ«æ˜¯åŸå›¾ç‰‡çš„åœ°å€ã€ä½œè€…ã€å‘å¸ƒåœ°å€ã€å›¾ç‰‡IDã€æ ‡é¢˜ã€å›¾ç‰‡å®½åº¦ã€å›¾ç‰‡é«˜åº¦ç­‰ä¿¡æ¯ã€‚

æœ‰äº†è¿™ä¸ªJSONä¿¡æ¯ï¼Œä½ å¾ˆå®¹æ˜“å°±å¯ä»¥æŠŠå›¾ç‰‡ä¸‹è½½ä¸‹æ¥ã€‚å½“ç„¶ä½ è¿˜éœ€è¦å¯»æ‰¾XHRè¯·æ±‚çš„urlè§„å¾‹ã€‚

å¦‚ä½•æŸ¥çœ‹å‘¢ï¼Œæˆ‘ä»¬å†æ¥é‡æ–°çœ‹ä¸‹è¿™ä¸ªç½‘å€æœ¬èº«ã€‚

[https://www.douban.com/j/search\_photo?q=ç‹ç¥–è´¤&amp;limit=20&amp;start=0](https://www.douban.com/j/search_photo?q=%E7%8E%8B%E7%A5%96%E8%B4%A4&limit=20&start=0)

ä½ ä¼šå‘ç°ï¼Œç½‘å€ä¸­æœ‰ä¸‰ä¸ªå‚æ•°ï¼šqã€limitå’Œstartã€‚startå®é™…ä¸Šæ˜¯è¯·æ±‚çš„èµ·å§‹IDï¼Œè¿™é‡Œæˆ‘ä»¬æ³¨æ„åˆ°å®ƒå¯¹å›¾ç‰‡çš„é¡ºåºæ ‡è¯†æ˜¯ä»0å¼€å§‹è®¡ç®—çš„ã€‚æ‰€ä»¥å¦‚æœä½ æƒ³è¦ä»ç¬¬21ä¸ªå›¾ç‰‡è¿›è¡Œä¸‹è½½ï¼Œä½ å¯ä»¥å°†startè®¾ç½®ä¸º20ã€‚

ç‹ç¥–è´¤çš„å›¾ç‰‡ä¸€å…±æœ‰22471å¼ ï¼Œä½ å¯ä»¥å†™ä¸ªforå¾ªç¯æ¥è·‘å®Œæ‰€æœ‰çš„è¯·æ±‚ï¼Œå…·ä½“çš„ä»£ç å¦‚ä¸‹ï¼š

```
# coding:utf-8
import requests
import json
query = 'ç‹ç¥–è´¤'
''' ä¸‹è½½å›¾ç‰‡ '''
def download(src, id):
	dir = './' + str(id) + '.jpg'
	try:
		pic = requests.get(src, timeout=10)
		fp = open(dir, 'wb')
		fp.write(pic.content)
		fp.close()
	except requests.exceptions.ConnectionError:
		print('å›¾ç‰‡æ— æ³•ä¸‹è½½')
            
''' for å¾ªç¯ è¯·æ±‚å…¨éƒ¨çš„ url '''
for i in range(0, 22471, 20):
	url = 'https://www.douban.com/j/search_photo?q='+query+'&limit=20&start='+str(i)
	html = requests.get(url).text    # å¾—åˆ°è¿”å›ç»“æœ
	response = json.loads(html,encoding='utf-8') # å°† JSON æ ¼å¼è½¬æ¢æˆ Python å¯¹è±¡
	for image in response['images']:
		print(image['src']) # æŸ¥çœ‹å½“å‰ä¸‹è½½çš„å›¾ç‰‡ç½‘å€
		download(image['src'], image['id']) # ä¸‹è½½ä¸€å¼ å›¾ç‰‡
```

## å¦‚ä½•ä½¿ç”¨XPathè‡ªåŠ¨ä¸‹è½½ç‹ç¥–è´¤çš„ç”µå½±æµ·æŠ¥å°é¢

å¦‚æœä½ é‡åˆ°JSONçš„æ•°æ®æ ¼å¼ï¼Œé‚£ä¹ˆæ­å–œä½ ï¼Œæ•°æ®ç»“æ„å¾ˆæ¸…çˆ½ï¼Œé€šè¿‡Pythonçš„JSONåº“å°±å¯ä»¥è§£æã€‚

ä½†æœ‰æ—¶å€™ï¼Œç½‘é¡µä¼šç”¨JSè¯·æ±‚æ•°æ®ï¼Œé‚£ä¹ˆåªæœ‰JSéƒ½åŠ è½½å®Œä¹‹åï¼Œæˆ‘ä»¬æ‰èƒ½è·å–å®Œæ•´çš„HTMLæ–‡ä»¶ã€‚XPathå¯ä»¥ä¸å—åŠ è½½çš„é™åˆ¶ï¼Œå¸®æˆ‘ä»¬å®šä½æƒ³è¦çš„å…ƒç´ ã€‚

æ¯”å¦‚ï¼Œæˆ‘ä»¬æƒ³è¦ä»è±†ç“£ç”µå½±ä¸Šä¸‹è½½ç‹ç¥–è´¤çš„ç”µå½±å°é¢ï¼Œéœ€è¦å…ˆæ¢³ç†ä¸‹äººå·¥çš„æ“ä½œæµç¨‹ï¼š

1. [æ‰“å¼€ç½‘é¡µmovie.douban.com](http://xn--movie-hr2j95qrv1e8j7b.douban.com)ï¼›
2. è¾“å…¥å…³é”®è¯â€œç‹ç¥–è´¤â€ï¼›
3. ä¸‹è½½å›¾ç‰‡é¡µä¸­çš„æ‰€æœ‰ç”µå½±å°é¢ã€‚

è¿™é‡Œä½ éœ€è¦ç”¨XPathå®šä½å›¾ç‰‡çš„ç½‘å€ï¼Œä»¥åŠç”µå½±çš„åç§°ã€‚

ä¸€ä¸ªå¿«é€Ÿå®šä½XPathçš„æ–¹æ³•å°±æ˜¯é‡‡ç”¨æµè§ˆå™¨çš„XPath Helperæ’ä»¶ï¼Œä½¿ç”¨Ctrl+Shift+Xå¿«æ·é”®çš„æ—¶å€™ï¼Œç”¨é¼ æ ‡é€‰ä¸­ä½ æƒ³è¦å®šä½çš„å…ƒç´ ï¼Œå°±ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹é¢çš„ç»“æœã€‚

![](https://static001.geekbang.org/resource/image/0e/c7/0e0fef601ee043f4bea8dd2874e788c7.png?wh=865%2A111)

XPath Helperæ’ä»¶ä¸­æœ‰ä¸¤ä¸ªå‚æ•°ï¼Œä¸€ä¸ªæ˜¯Queryï¼Œå¦ä¸€ä¸ªæ˜¯Resultsã€‚Queryå…¶å®å°±æ˜¯è®©ä½ æ¥è¾“å…¥XPathè¯­æ³•ï¼Œç„¶ååœ¨Resultsé‡Œçœ‹åˆ°åŒ¹é…çš„å…ƒç´ çš„ç»“æœã€‚

æˆ‘ä»¬çœ‹åˆ°ï¼Œè¿™é‡Œé€‰ä¸­çš„æ˜¯ä¸€ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬è¦åŒ¹é…ä¸Šæ‰€æœ‰çš„ç”µå½±æµ·æŠ¥ï¼Œå°±éœ€è¦ç¼©å‡XPathè¡¨è¾¾å¼ã€‚ä½ å¯ä»¥åœ¨Queryä¸­è¿›è¡ŒXPathè¡¨è¾¾å¼çš„ç¼©å‡ï¼Œå°è¯•å»æ‰XPathè¡¨è¾¾å¼ä¸­çš„ä¸€äº›å†…å®¹ï¼Œåœ¨Resultsä¸­ä¼šè‡ªåŠ¨å‡ºç°åŒ¹é…çš„ç»“æœã€‚

ç»è¿‡ç¼©å‡ä¹‹åï¼Œä½ å¯ä»¥å¾—åˆ°ç”µå½±æµ·æŠ¥çš„XPathï¼ˆå‡è®¾ä¸ºå˜é‡src\_xpathï¼‰ï¼š

```
//div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src
```

ä»¥åŠç”µå½±åç§°çš„XPathï¼ˆå‡è®¾ä¸ºå˜é‡title\_xpathï¼‰ï¼š

```
//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']
```

ä½†æœ‰æ—¶å€™å½“æˆ‘ä»¬ç›´æ¥ç”¨Requestsè·å–HTMLçš„æ—¶å€™ï¼Œå‘ç°æƒ³è¦çš„XPathå¹¶ä¸å­˜åœ¨ã€‚è¿™æ˜¯å› ä¸ºHTMLè¿˜æ²¡æœ‰åŠ è½½å®Œï¼Œå› æ­¤ä½ éœ€è¦ä¸€ä¸ªå·¥å…·ï¼Œæ¥è¿›è¡Œç½‘é¡µåŠ è½½çš„æ¨¡æ‹Ÿï¼Œç›´åˆ°å®ŒæˆåŠ è½½åå†ç»™ä½ å®Œæ•´çš„HTMLã€‚

åœ¨Pythonä¸­ï¼Œè¿™ä¸ªå·¥å…·å°±æ˜¯Seleniumåº“ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```
from selenium import webdriver
driver = webdriver.Chrome()
driver.get(request_url)
```

Seleniumæ˜¯Webåº”ç”¨çš„æµ‹è¯•å·¥å…·ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œåœ¨æµè§ˆå™¨ä¸­ï¼Œå®ƒçš„åŸç†æ˜¯æ¨¡æ‹Ÿç”¨æˆ·åœ¨è¿›è¡Œæ“ä½œï¼Œæ”¯æŒå½“å‰å¤šç§ä¸»æµçš„æµè§ˆå™¨ã€‚

è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹ŸChromeæµè§ˆå™¨çš„é¡µé¢è®¿é—®ã€‚

ä½ éœ€è¦å…ˆå¼•ç”¨Seleniumä¸­çš„WebDriveråº“ã€‚WebDriverå®é™…ä¸Šå°±æ˜¯Selenium 2ï¼Œæ˜¯ä¸€ç§ç”¨äºWebåº”ç”¨ç¨‹åºçš„è‡ªåŠ¨æµ‹è¯•å·¥å…·ï¼Œæä¾›äº†ä¸€å¥—å‹å¥½çš„APIï¼Œæ–¹ä¾¿æˆ‘ä»¬è¿›è¡Œæ“ä½œã€‚

ç„¶åé€šè¿‡WebDriveråˆ›å»ºä¸€ä¸ªChromeæµè§ˆå™¨çš„driveï¼Œå†é€šè¿‡driveè·å–è®¿é—®é¡µé¢çš„å®Œæ•´HTMLã€‚

å½“ä½ è·å–åˆ°å®Œæ•´çš„HTMLæ—¶ï¼Œå°±å¯ä»¥å¯¹HTMLä¸­çš„XPathè¿›è¡Œæå–ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å›¾ç‰‡åœ°å€srcså’Œç”µå½±åç§°titlesã€‚è¿™é‡Œé€šè¿‡XPathè¯­æ³•åŒ¹é…åˆ°äº†å¤šä¸ªå…ƒç´ ï¼Œå› ä¸ºæ˜¯å¤šä¸ªå…ƒç´ ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç”¨forå¾ªç¯æ¥å¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œæå–ã€‚

```
srcs = html.xpath(src_xpath)
titles = html.xpath(title_path)
for src, title in zip(srcs, titles):
	download(src, title.text)
```

ç„¶åä½¿ç”¨ä¸Šé¢æˆ‘ç¼–å†™å¥½çš„downloadå‡½æ•°è¿›è¡Œå›¾ç‰‡ä¸‹è½½ã€‚

## æ€»ç»“

å¥½äº†ï¼Œè¿™æ ·å°±å¤§åŠŸå‘Šæˆäº†ï¼Œç¨‹åºå¯ä»¥æºæºä¸æ–­åœ°é‡‡é›†ä½ æƒ³è¦çš„å†…å®¹ã€‚è¿™èŠ‚è¯¾ï¼Œæˆ‘æƒ³è®©ä½ æŒæ¡çš„æ˜¯ï¼š

1. Pythonçˆ¬è™«çš„æµç¨‹ï¼›
2. äº†è§£XPathå®šä½ï¼ŒJSONå¯¹è±¡è§£æï¼›
3. å¦‚ä½•ä½¿ç”¨lxmlåº“ï¼Œè¿›è¡ŒXPathçš„æå–ï¼›
4. å¦‚ä½•åœ¨Pythonä¸­ä½¿ç”¨Seleniumåº“æ¥å¸®åŠ©ä½ æ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œè·å–å®Œæ•´çš„HTMLã€‚

å…¶ä¸­ï¼ŒPython + Selenium + ç¬¬ä¸‰æ–¹æµè§ˆå™¨å¯ä»¥è®©æˆ‘ä»¬å¤„ç†å¤šç§å¤æ‚åœºæ™¯ï¼ŒåŒ…æ‹¬ç½‘é¡µåŠ¨æ€åŠ è½½ã€JSå“åº”ã€Postè¡¨å•ç­‰ã€‚å› ä¸ºSeleniumæ¨¡æ‹Ÿçš„å°±æ˜¯ä¸€ä¸ªçœŸå®çš„ç”¨æˆ·çš„æ“ä½œè¡Œä¸ºï¼Œå°±ä¸ç”¨æ‹…å¿ƒcookieè¿½è¸ªå’Œéšè—å­—æ®µçš„å¹²æ‰°äº†ã€‚

å½“ç„¶ï¼ŒPythonè¿˜ç»™æˆ‘ä»¬æä¾›äº†æ•°æ®å¤„ç†å·¥å…·ï¼Œæ¯”å¦‚lxmlåº“å’ŒJSONåº“ï¼Œè¿™æ ·å°±å¯ä»¥æå–æƒ³è¦çš„å†…å®¹äº†ã€‚

![](https://static001.geekbang.org/resource/image/eb/ab/eb3e48f714ca857a79948d831de521ab.jpg?wh=3200%2A1800)

æœ€åï¼Œä½ ä¸å¦¨æ¥å®è·µä¸€ä¸‹ï¼Œä½ æœ€å–œæ¬¢å“ªä¸ªæ˜æ˜Ÿï¼Ÿå¦‚æœæƒ³è¦è‡ªåŠ¨ä¸‹è½½è¿™ä¸ªæ˜æ˜Ÿçš„å›¾ç‰‡ï¼Œè¯¥å¦‚ä½•æ“ä½œå‘¢ï¼Ÿæ¬¢è¿å’Œæˆ‘åœ¨è¯„è®ºåŒºè¿›è¡Œæ¢è®¨ã€‚

ä½ ä¹Ÿå¯ä»¥æŠŠè¿™ç¯‡æ–‡ç« åˆ†äº«ç»™ä½ çš„æœ‹å‹æˆ–è€…åŒäº‹ï¼Œä¸€èµ·åŠ¨æ‰‹ç»ƒä¹ ä¸€ä¸‹ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>æ»¢</span> ğŸ‘ï¼ˆ63ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è¯´æ˜ä¸¤ç‚¹é—®é¢˜ï¼š
ï¼ˆä¸€ï¼‰.ç•™è¨€é‡Œæœ‰äººè¯„è®ºè¯´ç”¨XPathä¸‹è½½çš„å›¾ç‰‡æ‰“ä¸å¼€ï¼Œå…¶åŸå› æ˜¯å®šä¹‰çš„ä¸‹è½½å‡½æ•°ä¿å­˜è·¯å¾„åç¼€åä¸º&#39;.jpg&#39;ï¼Œä½†æ˜¯ç”¨XPathä¸‹è½½è·å¾—çš„å›¾ç‰‡urlä¸º&#39;https:&#47;&#47;img3.doubanio.com&#47;view&#47;celebrity&#47;s_ratio_celebrity&#47;public&#47;p616.webp&#39;ï¼Œæœ¬èº«å›¾ç‰‡ä¸ºwebpæ ¼å¼ï¼Œæ‰€ä»¥è‹¥ä¿å­˜ä¸ºjpgæ ¼å¼ï¼Œè‚¯å®šæ˜¯æ‰“ä¸å¼€çš„ã€‚
(äºŒ).  è€å¸ˆåœ¨æ–‡ç« å†…è®²çš„ç”¨XPathä¸‹è½½ä»£ç åªèƒ½ä¸‹è½½ç¬¬ä¸€é¡µçš„å†…å®¹ï¼Œå¹¶ä¸æ˜¯å…¨éƒ¨çš„æ•°æ®ï¼Œä¸çŸ¥é“å¤§å®¶æœ‰æ²¡æœ‰æŸ¥çœ‹ç”¨xpathå‡½æ•°è·å¾—çš„æ•°ç»„ï¼Œå¤§å®¶ç•™è¨€é‡Œçš„ä»£ç ä¼¼ä¹å’Œè€å¸ˆçš„ä¸€æ ·ï¼Œåªèƒ½å¾—åˆ°é¦–é¡µçš„å†…å®¹ï¼Œæ‰€ä»¥ä¹Ÿæ˜¯éœ€è¦æ¨¡æ‹Ÿç¿»é¡µæ“ä½œæ‰èƒ½è·å¾—å®Œæ•´çš„æ•°æ®ã€‚

ä»¥ä¸‹æ˜¯è¯¾åç»ƒä¹ é¢˜ï¼šçˆ¬å–å®«å´éªçš„ç”µå½±æµ·æŠ¥ï¼Œ Python3.6 IDLE
&gt;&gt;&gt; import json
&gt;&gt;&gt; import requests as req
&gt;&gt;&gt; from lxml import etree
&gt;&gt;&gt; from selenium import webdriver
&gt;&gt;&gt; import os
&gt;&gt;&gt; request_url = &#39;https:&#47;&#47;movie.douban.com&#47;subject_search?search_text=å®«å´éª&amp;cat=1002&#39;
&gt;&gt;&gt; src_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
&gt;&gt;&gt; title_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;
&gt;&gt;&gt; driver = webdriver.Chrome(&#39;&#47;Users&#47;apple&#47;Downloads&#47;chromedriver&#39;)
&gt;&gt;&gt; driver.get(request_url)
&gt;&gt;&gt; html = etree.HTML(driver.page_source)
&gt;&gt;&gt; srcs = html.xpath(src_xpath)
&gt;&gt;&gt; print (srcs)  #å¤§å®¶å¯è¦çœ‹ä¸‹æ‰“å°å‡ºæ¥çš„æ•°æ®æ˜¯å¦åªæ˜¯ä¸€é¡µçš„å†…å®¹ï¼Œä»¥åŠå›¾ç‰‡urlçš„åç¼€æ ¼å¼
&gt;&gt;&gt; picpath = &#39;&#47;Users&#47;apple&#47;Downloads&#47;å®«å´éªç”µå½±æµ·æŠ¥&#39;
&gt;&gt;&gt; if not os.path.isdir(picpath):
	os.mkdir(picpath)
&gt;&gt;&gt; def download(src, id):
	dic = picpath + &#39;&#47;&#39; + str(id) + &#39;.webp&#39;
	try:
		pic = req.get(src, timeout = 30)
		fp = open(dic, &#39;wb&#39;)
		fp.write(pic.content)
		fp.close()
	except req.exceptions.ConnectionError:
		print (&#39;å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;)
&gt;&gt;&gt; for i in range(0, 150, 15):
	url = request_url + &#39;&amp;start=&#39; + str(i)
	driver.get(url)
	html = etree.HTML(driver.page_source)
	srcs = html.xpath(src_xpath)
	titles = html.xpath(title_xpath)
	for src,title in zip(srcs, titles):
		download(src, title.text)
</p>2019-04-10</li><br/><li><span>rOMEoç½—å¯†æ¬§</span> ğŸ‘ï¼ˆ46ï¼‰ ğŸ’¬ï¼ˆ4ï¼‰<p>è€å¸ˆè¯·é—®ä¸€ä¸‹ï¼šå¦‚æœæ˜¯éœ€è¦ç”¨æˆ·ç™»é™†åæ‰èƒ½çˆ¬å–çš„æ•°æ®è¯¥æ€ä¹ˆç”¨pythonæ¥å®ç°å‘¢ï¼Ÿ</p>2019-01-04</li><br/><li><span>Bayes</span> ğŸ‘ï¼ˆ18ï¼‰ ğŸ’¬ï¼ˆ8ï¼‰<p>è€å¸ˆä½ è¿™è·³è¿‡äº†å¤ªå¤šæ­¥éª¤äº†ï¼Œè¡¨ç¤ºå¯¹äºpythonè·Ÿç€ä½ å‰å‡ èŠ‚è¯¾å…¥é—¨çš„äººä»€ä¹ˆéƒ½ä¸ä¼šï¼ŒæŒ‰ç€ä½ çš„ä»£ç è¿è¡Œï¼Œè¦ä¸å°±æ˜¯æ²¡æœ‰å®šä¹‰ï¼Œè¦ä¸å°±æ˜¯æ²¡æœ‰è¿™ä¸ªå‡½æ•°ã€‚åˆšå¼€å§‹çš„äººä¹Ÿä¸çŸ¥é“å“ªä¸ªå‡½æ•°åœ¨å“ªä¸ªåº“ï¼Œå»ºè®®è€å¸ˆæŒ‰ç…§æµç¨‹æ¥ä¸€æ­¥ä¸€æ­¥ç»™ä»£ç ï¼Œè¦ä¸å°±åœ¨æœ€åç»™ä¸€ä¸ªå®Œæ•´çš„ä»£ç ç¤ºä¾‹ï¼ŒçœŸçš„æ˜¯å­¦çš„å¾ˆå›°éš¾åŠ ä¸Šæƒ³æ”¾å¼ƒ</p>2019-07-30</li><br/><li><span>LY</span> ğŸ‘ï¼ˆ16ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>#ç¯å¢ƒï¼šMac Python3
#pip install selenium
#ä¸‹è½½chromedriverï¼Œæ”¾åˆ°é¡¹ç›®è·¯å¾„ä¸‹ï¼ˆhttps:&#47;&#47;npm.taobao.org&#47;mirrors&#47;chromedriver&#47;2.33&#47;ï¼‰
# coding:utf-8
import requests
import json
import os
from lxml import etree
from selenium import webdriver

query = &#39;å¼ æŸèŠ&#39;
downloadPath = &#39;&#47;Users&#47;yong&#47;Desktop&#47;Python&#47;xpath&#47;images&#47;&#39;

&#39;&#39;&#39; ä¸‹è½½å›¾ç‰‡ &#39;&#39;&#39;
def download(src, id):
    dir = downloadPath + str(id) + &#39;.jpg&#39;
    try:
        pic = requests.get(src, timeout=10)
    except requests.exceptions.ConnectionError:
    # print &#39;error, %d å½“å‰å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;, %id
        print(&#39;å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;)
    if not os.path.exists(downloadPath):
        os.mkdir(downloadPath)
    if os.path.exists(dir):
        print(&#39;å·²å­˜åœ¨:&#39;+ id)
        return
    fp = open(dir, &#39;wb&#39;)
    fp.write(pic.content)
    fp.close()
 
def searchImages():
    &#39;&#39;&#39; for å¾ªç¯ è¯·æ±‚å…¨éƒ¨çš„ url &#39;&#39;&#39;
    for i in range(0, 22471, 20):
        url = &#39;https:&#47;&#47;www.douban.com&#47;j&#47;search_photo?q=&#39;+query+&#39;&amp;limit=20&amp;start=&#39;+str(i)
        html = requests.get(url).text    # å¾—åˆ°è¿”å›ç»“æœ
        print(&#39;html:&#39;+html)
        response = json.loads(html,encoding=&#39;utf-8&#39;) # å°† JSON æ ¼å¼è½¬æ¢æˆ Python å¯¹è±¡
        for image in response[&#39;images&#39;]:
            print(image[&#39;src&#39;]) # æŸ¥çœ‹å½“å‰ä¸‹è½½çš„å›¾ç‰‡ç½‘å€
            download(image[&#39;src&#39;], image[&#39;id&#39;]) # ä¸‹è½½ä¸€å¼ å›¾ç‰‡

def getMovieImages():
    url = &#39;https:&#47;&#47;movie.douban.com&#47;subject_search?search_text=&#39;+ query +&#39;&amp;cat=1002&#39;
    driver = webdriver.Chrome(&#39;&#47;Users&#47;yong&#47;Desktop&#47;Python&#47;xpath&#47;libs&#47;chromedriver&#39;)
    driver.get(url)
    html = etree.HTML(driver.page_source)
    # ä½¿ç”¨xpath helper, ctrl+shit+x é€‰ä¸­å…ƒç´ ï¼Œå¦‚æœè¦åŒ¹é…å…¨éƒ¨ï¼Œåˆ™éœ€è¦ä¿®æ”¹query è¡¨è¾¾å¼
    src_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
    title_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;

    srcs = html.xpath(src_xpath)
    titles = html.xpath(title_xpath)
    for src, title in zip(srcs, titles):
        print(&#39;\t&#39;.join([str(src),str(title.text)]))
        download(src, title.text)

    driver.close()

getMovieImages()</p>2019-01-04</li><br/><li><span>ä¼ªå›å­</span> ğŸ‘ï¼ˆ16ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>é‚£äº›ç”¨ ChromeDriver çš„å‡ºç°æŠ¥é”™çš„å¯èƒ½æ˜¯æ²¡æœ‰å®‰è£… ChromeDriverï¼Œæˆ–è€…æ˜¯æ²¡ç»™å‡º ChromeDriver çš„è·¯å¾„ï¼Œå…·ä½“å¯ä»¥çœ‹çœ‹ä¸‹é¢è¿™ç¯‡æ–‡ç« ã€‚
https:&#47;&#47;mp.weixin.qq.com&#47;s&#47;UL0bcLr3KOb-qpI9oegaIQ</p>2019-01-04</li><br/><li><span>é£˜</span> ğŸ‘ï¼ˆ9ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>æ„Ÿè°¢ä½œè€…ä»¥åŠè¯„è®ºåŒºçš„å„ä½å¤§ç¥ï¼Œç»ˆäºå®Œæˆäº†çˆ¬è™«ä»£ç ï¼Œæ€»ç»“ä¸€ä¸‹å°ç™½ç¼–å†™æ—¶é‡åˆ°çš„å‡ ä¸ªé—®é¢˜ï¼š
1ï¼‰è·å–xpathæ—¶ï¼Œchromeæµè§ˆå™¨éœ€è¦å®‰è£…æ’ä»¶xpatn-helperï¼›
2ï¼‰ä½¿ç”¨python3.7ï¼Œæå‰å¼•å…¥æ¨¡å—requestsï¼Œlxmlï¼Œseleniumï¼Œå®‰è£…è¿™äº›æ¨¡å—éœ€è¦æ›´æ–°pipè‡³20ç‰ˆæœ¬ï¼›
3ï¼‰æ¨¡æ‹Ÿç”¨æˆ·è®¿é—®æµè§ˆå™¨ï¼Œéœ€è¦ä¸‹è½½chromedriver.exe,æ”¾å…¥python.exeæ‰€åœ¨ç›®å½•ï¼›
4ï¼‰å›¾ç‰‡è·¯å¾„ä¸­å‡ºç°å¯¼è‡´ç¼–è¯‘å¤±è´¥çš„å­—ç¬¦ï¼Œä½¿ç”¨replaceæ›¿æ¢æŠ¥é”™å­—ç¬¦ã€‚
å…·ä½“ä»£ç å¦‚ä¸‹ï¼š
import os
import requests
from lxml import etree
from selenium import webdriver

search_text = &quot;ç‹ç¥–è´¤&quot;
start = 0
limit = 15
total = 90

def download(img, title):
	dir = &quot;D:\\æ•°æ®åˆ†æ\\python test\\query\\&quot; + search_text + &quot;\\&quot; 
	id = title.replace(u&#39;\u200e&#39;, u&#39;&#39;).replace(u&#39;?&#39;, u&#39;&#39;) .replace(u&#39;&#47;&#39;, u&#39;or&#39;)
	if not os.path.exists(dir):
		os.makedirs(dir)
	try:    
		pic = requests.get(img, timeout=10) 
		img_path = dir  + str(id) + &#39;.jpg&#39;   
		fp = open(img_path, &#39;wb&#39;)    
		fp.write(pic.content)    
		fp.close()  
	except requests.exceptions.ConnectionError:    
		print(&#39;å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;)

def crawler_xpath():
	src_img = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
	src_title = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;
	
	for i in range(start,total,limit):
		request_url = &quot;https:&#47;&#47;search.douban.com&#47;movie&#47;subject_search?search_text=&quot;+search_text+&quot;&amp;cat=1002&amp;start=&quot;+str(i)
		driver = webdriver.Chrome()
		driver.get(request_url)
		html = etree.HTML(driver.page_source)
		imgs = html.xpath(src_img)
		titles = html.xpath(src_title)
		print(imgs,titles)
		for img, title in zip(imgs, titles): 
			download(img, title.text)
if __name__ == &#39;__main__&#39;:
	crawler_xpath()
</p>2020-04-08</li><br/><li><span>germany</span> ğŸ‘ï¼ˆ6ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è€å¸ˆï¼šä¸ºä»€ä¹ˆæˆ‘åœ¨è±†ç“£ç½‘æŸ¥è¯¢å›¾ç‰‡çš„ç½‘å€ä¸ä½ ä¸ä¸€æ ·ï¼Ÿhttps:&#47;&#47;www.douban.com&#47;search?cat=1025&amp;q=ç‹ç¥–è´¤&amp;source=suggest  ã€‚æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ</p>2019-01-04</li><br/><li><span>è¨±æ•²æ•²</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¦ä¸‹è½½æ‰€æœ‰James å“ˆç™»çš„å›¾ç‰‡</p>2019-01-04</li><br/><li><span>Geek_2008d9</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>ä¸ºä»€ä¹ˆæˆ‘æ€»æ˜¯response=json.loadsé‚£ä¸€è¡Œæ˜¾ç¤ºjson.decoder.JSONDecoderError:expecting value:line 1 column 1(char 0) å‘¢ï¼Œæ€ä¹ˆè§£å†³å•Šï¼Œå„ä½å¤§ä½¬</p>2019-12-14</li><br/><li><span>Geek_c45626</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ7ï¼‰<p>è€å¸ˆï¼Œè¿è¡Œä»£ç æ€»æ˜¯å‡ºé”™ï¼šJSONDecodeError: Expecting value: line 1 column 1 (char 0)ï¼Œè¿™ä¸ªæ€ä¹ˆè§£å†³ï¼Ÿ</p>2019-12-06</li><br/><li><span>qinggeouye</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>https:&#47;&#47;github.com&#47;qinggeouye&#47;GeekTime&#47;blob&#47;master&#47;DataAnalysis&#47;10_crawl_xpath.py

import os
import requests
from lxml import etree
from selenium import webdriver

search_text = &#39;ç‹ç¥–è´¤&#39;
start = 0  # è¯·æ±‚ url çš„ start ä» 0 å¼€å§‹ï¼Œæ¯ä¸€é¡µé—´éš” 15ï¼Œæœ‰ 6 é¡µ
total = 90
limit = 15

# ç”µå½±æµ·æŠ¥å›¾ç‰‡åœ°å€
src_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
# ç”µå½±æµ·æŠ¥å›¾ç‰‡title
title_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;

# ä¿å­˜ç›®å½•
pic_path = &#39;10&#47;xpath&#39;  # ç›¸å¯¹ç›®å½•
# WebDriver åˆ›å»ºä¸€ä¸ª Chrome æµè§ˆå™¨çš„ drive
driver = webdriver.Chrome(&#39;.&#47;chromedriver&#39;)  # MAC ç‰ˆæœ¬


# åˆ›å»ºå›¾ç‰‡ä¿å­˜è·¯å¾„
def mk_save_path(pic_path_):
    if not os.path.exists(pic_path_):
        os.makedirs(pic_path_)
    return os.getcwd() + &#39;&#47;&#39; + pic_path_ + &#39;&#47;&#39;


# ä¸‹è½½å›¾ç‰‡
def download(src, pic_id, save_path_):
    directory = save_path_ + str(pic_id) + &#39;.jpg&#39;
    try:
        pic = requests.get(src, timeout=10)
        fp = open(directory, &#39;wb&#39;)
        fp.write(pic.content)
        fp.close()
    except requests.exceptions.ConnectionError:
        print(&#39;å›¾ç‰‡å¦‚æ— æ³•ä¸‹è½½&#39;)


def get_response_xpath():
    save_path = mk_save_path(pic_path)
    for i in range(start, total, limit):
        requests_url = &#39;https:&#47;&#47;search.douban.com&#47;movie&#47;subject_search?search_text=&#39; + search_text + &#39;&amp;cat=1002&#39; + \
                       &#39;&amp;start=&#39; + str(i)
        driver.get(url=requests_url)
        html = etree.HTML(driver.page_source)
        src_list = html.xpath(src_xpath)
        title_list = html.xpath(title_xpath)
        for src, title in zip(src_list, title_list):
            download(src, title.text, save_path)


if __name__ == &#39;__main__&#39;:
    get_response_xpath()</p>2019-11-06</li><br/><li><span>Yezhiwei</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç”¨Scrapyçˆ¬å–æ•°æ®æ›´æ–¹ä¾¿å“ˆï¼Œè¯·é—®è€å¸ˆæ€ä¹ˆåšä¸€ä¸ªé€šç”¨çš„çˆ¬è™«å‘¢ï¼Ÿæ¯”å¦‚è¦çˆ¬å–æ–‡ç« æ ‡é¢˜å’Œå†…å®¹ï¼Œä¸åŒçš„ç½‘ç«™Xpathç»“æ„ä¸ä¸€æ ·ï¼Œå¦‚æœæºå°‘çš„è¯å¯ä»¥åˆ†åˆ«é…ç½®ï¼Œä½†å¦‚æœè¦çˆ¬å–å‡ ç™¾ä¸Šåƒçš„ç½‘ç«™æ•°æ®ï¼Œåˆ†åˆ«é…ç½®XpathæŒºéº»çƒ¦çš„ã€‚è¯·é—®è¿™ä¸ªé—®é¢˜æœ‰ä»€ä¹ˆè§£å†³æ–¹æ¡ˆå—ï¼Ÿè°¢è°¢</p>2019-01-04</li><br/><li><span>qinggeouye</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>https:&#47;&#47;github.com&#47;qinggeouye&#47;GeekTime&#47;blob&#47;master&#47;DataAnalysis&#47;10_crawl.py

# coding: utf-8
import os

import requests
import json


# ä¸‹è½½å›¾ç‰‡
def download(src, pic_id, save_path_):
    directory = save_path_ + str(pic_id) + &#39;.jpg&#39;

    try:
        pic = requests.get(src, timeout=10)
        fp = open(directory, &#39;wb&#39;)
        fp.write(pic.content)
        fp.close()
    except requests.exceptions.ConnectionError:
        print(&#39;å›¾ç‰‡å¦‚æ— æ³•ä¸‹è½½&#39;)


# è·å–è¿”å›é¡µé¢å†…å®¹
def get_resp(query_, limit_, start_):
    url_ = &#39;https:&#47;&#47;www.douban.com&#47;j&#47;search_photo?q=&#39; + query_ + &#39;&amp;limit=&#39; + str(limit_) + &#39;&amp;start=&#39; + str(start_)
    html_ = requests.get(url_).text  # å¾—åˆ°è¿”å›ç»“æœ
    response_ = json.loads(html_, encoding=&#39;utf-8&#39;)  # å°† JSON æ ¼å¼è½¬æ¢ä¸º Python å¯¹è±¡
    return response_


query = &#39;ç‹ç¥–è´¤&#39;
limit = 20
start = 0

&#39;&#39;&#39; è·å–å›¾ç‰‡æ€»æ•°é‡ &#39;&#39;&#39;
total = get_resp(query, limit, start)[&#39;total&#39;]
print(total)

pic_path = &#39;10&#39;  # ç›¸å¯¹ç›®å½•
if not os.path.exists(pic_path):
    os.mkdir(pic_path)
save_path = os.getcwd() + &#39;&#47;&#39; + pic_path + &#39;&#47;&#39;

# å¾ªç¯ è¯·æ±‚å…¨éƒ¨çš„ url
for i in range(start, total, limit):
    response = get_resp(query, limit, i)
    for image in response[&#39;images&#39;]:
        print(image[&#39;src&#39;])  # æŸ¥çœ‹å½“å‰ä¸‹è½½çš„å›¾ç‰‡åœ°å€
        download(image[&#39;src&#39;], image[&#39;id&#39;], save_path)  # ä¸‹è½½ä¸€å¼ å›¾ç‰‡
</p>2019-11-05</li><br/><li><span>çˆ±å–é…¸å¥¶çš„ç¨‹åºå‘˜</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æœ‰ä¸ªé—®é¢˜seleniumï¼Œæ˜¯ç”¨æ¥è‡ªåŠ¨åŒ–æµ‹è¯•çš„ï¼Œä»–å›æ‰“å¼€æµè§ˆå™¨â€¦â€¦æˆ‘åšçˆ¬è™«æ˜¯ä¸æƒ³è®©ä»£ç æ‰“å¼€æµè§ˆå™¨ï¼Œåªæƒ³è¦ä»–çˆ¬å–çš„åŠ¨ä½œï½è¦æ€ä¹ˆåŠå‘¢ï¼Ÿ</p>2019-02-26</li><br/><li><span>åå…­ã€‚</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>ç”±äºpythonå­¦ä¹ æ˜¯çˆ¬è™«ä¸Šæ‰‹çš„ï¼Œçˆ¬è™«è¿˜æ˜¯ä¼šä¸€ç‚¹çš„ï¼Œå“ˆå“ˆï¼Œå»ä¸‹ä¸€ç« äº†áƒ¦( Â´ï½¥á´—ï½¥` )</p>2020-03-27</li><br/>
</ul>