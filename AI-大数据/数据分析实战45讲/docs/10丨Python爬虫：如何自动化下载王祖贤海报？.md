ä¸Šä¸€è®²ä¸­æˆ‘ç»™ä½ è®²äº†å¦‚ä½•ä½¿ç”¨å…«çˆªé±¼é‡‡é›†æ•°æ®ï¼Œå¯¹äºæ•°æ®é‡‡é›†åˆšåˆšå…¥é—¨çš„äººæ¥è¯´ï¼Œåƒå…«çˆªé±¼è¿™ç§å¯è§†åŒ–çš„é‡‡é›†æ˜¯ä¸€ç§éå¸¸å¥½çš„æ–¹å¼ã€‚å®ƒæœ€å¤§çš„ä¼˜ç‚¹å°±æ˜¯ä¸Šæ‰‹é€Ÿåº¦å¿«ï¼Œå½“ç„¶ä¹Ÿå­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚è¿è¡Œé€Ÿåº¦æ…¢ã€å¯æ§æ€§å·®ç­‰ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œçˆ¬è™«å¯ä»¥å¾ˆå¥½åœ°é¿å…è¿™äº›é—®é¢˜ï¼Œä»Šå¤©æˆ‘æ¥åˆ†äº«ä¸‹å¦‚ä½•é€šè¿‡ç¼–å†™çˆ¬è™«æŠ“å–æ•°æ®ã€‚

## çˆ¬è™«çš„æµç¨‹

ç›¸ä¿¡ä½ å¯¹â€œçˆ¬è™«â€è¿™ä¸ªè¯å·²ç»éå¸¸ç†Ÿæ‚‰äº†ï¼Œçˆ¬è™«å®é™…ä¸Šæ˜¯ç”¨æµè§ˆå™¨è®¿é—®çš„æ–¹å¼æ¨¡æ‹Ÿäº†è®¿é—®ç½‘ç«™çš„è¿‡ç¨‹ï¼Œæ•´ä¸ªè¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ‰“å¼€ç½‘é¡µã€æå–æ•°æ®å’Œä¿å­˜æ•°æ®ã€‚

åœ¨Pythonä¸­ï¼Œè¿™ä¸‰ä¸ªé˜¶æ®µéƒ½æœ‰å¯¹åº”çš„å·¥å…·å¯ä»¥ä½¿ç”¨ã€‚

åœ¨â€œæ‰“å¼€ç½‘é¡µâ€è¿™ä¸€æ­¥éª¤ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ Requests è®¿é—®é¡µé¢ï¼Œå¾—åˆ°æœåŠ¡å™¨è¿”å›ç»™æˆ‘ä»¬çš„æ•°æ®ï¼Œè¿™é‡ŒåŒ…æ‹¬HTMLé¡µé¢ä»¥åŠJSONæ•°æ®ã€‚

åœ¨â€œæå–æ•°æ®â€è¿™ä¸€æ­¥éª¤ä¸­ï¼Œä¸»è¦ç”¨åˆ°äº†ä¸¤ä¸ªå·¥å…·ã€‚é’ˆå¯¹HTMLé¡µé¢ï¼Œå¯ä»¥ä½¿ç”¨ XPath è¿›è¡Œå…ƒç´ å®šä½ï¼Œæå–æ•°æ®ï¼›é’ˆå¯¹JSONæ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨JSONè¿›è¡Œè§£æã€‚

åœ¨æœ€åä¸€æ­¥â€œä¿å­˜æ•°æ®â€ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Pandas ä¿å­˜æ•°æ®ï¼Œæœ€åå¯¼å‡ºCSVæ–‡ä»¶ã€‚

ä¸‹é¢æˆ‘æ¥åˆ†åˆ«ä»‹ç»ä¸‹è¿™äº›å·¥å…·çš„ä½¿ç”¨ã€‚

**Requestsè®¿é—®é¡µé¢**

Requestsæ˜¯Python HTTPçš„å®¢æˆ·ç«¯åº“ï¼Œç¼–å†™çˆ¬è™«çš„æ—¶å€™éƒ½ä¼šç”¨åˆ°ï¼Œç¼–å†™èµ·æ¥ä¹Ÿå¾ˆç®€å•ã€‚å®ƒæœ‰ä¸¤ç§è®¿é—®æ–¹å¼ï¼šGetå’ŒPostã€‚è¿™ä¸¤è€…æœ€ç›´è§‚çš„åŒºåˆ«å°±æ˜¯ï¼šGetæŠŠå‚æ•°åŒ…å«åœ¨urlä¸­ï¼Œè€ŒPosté€šè¿‡request bodyæ¥ä¼ é€’å‚æ•°ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ30ï¼‰</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg" width="30px"><span>æ»¢</span> ğŸ‘ï¼ˆ63ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>è¯´æ˜ä¸¤ç‚¹é—®é¢˜ï¼š
ï¼ˆä¸€ï¼‰.ç•™è¨€é‡Œæœ‰äººè¯„è®ºè¯´ç”¨XPathä¸‹è½½çš„å›¾ç‰‡æ‰“ä¸å¼€ï¼Œå…¶åŸå› æ˜¯å®šä¹‰çš„ä¸‹è½½å‡½æ•°ä¿å­˜è·¯å¾„åç¼€åä¸º&#39;.jpg&#39;ï¼Œä½†æ˜¯ç”¨XPathä¸‹è½½è·å¾—çš„å›¾ç‰‡urlä¸º&#39;https:&#47;&#47;img3.doubanio.com&#47;view&#47;celebrity&#47;s_ratio_celebrity&#47;public&#47;p616.webp&#39;ï¼Œæœ¬èº«å›¾ç‰‡ä¸ºwebpæ ¼å¼ï¼Œæ‰€ä»¥è‹¥ä¿å­˜ä¸ºjpgæ ¼å¼ï¼Œè‚¯å®šæ˜¯æ‰“ä¸å¼€çš„ã€‚
(äºŒ).  è€å¸ˆåœ¨æ–‡ç« å†…è®²çš„ç”¨XPathä¸‹è½½ä»£ç åªèƒ½ä¸‹è½½ç¬¬ä¸€é¡µçš„å†…å®¹ï¼Œå¹¶ä¸æ˜¯å…¨éƒ¨çš„æ•°æ®ï¼Œä¸çŸ¥é“å¤§å®¶æœ‰æ²¡æœ‰æŸ¥çœ‹ç”¨xpathå‡½æ•°è·å¾—çš„æ•°ç»„ï¼Œå¤§å®¶ç•™è¨€é‡Œçš„ä»£ç ä¼¼ä¹å’Œè€å¸ˆçš„ä¸€æ ·ï¼Œåªèƒ½å¾—åˆ°é¦–é¡µçš„å†…å®¹ï¼Œæ‰€ä»¥ä¹Ÿæ˜¯éœ€è¦æ¨¡æ‹Ÿç¿»é¡µæ“ä½œæ‰èƒ½è·å¾—å®Œæ•´çš„æ•°æ®ã€‚

ä»¥ä¸‹æ˜¯è¯¾åç»ƒä¹ é¢˜ï¼šçˆ¬å–å®«å´éªçš„ç”µå½±æµ·æŠ¥ï¼Œ Python3.6 IDLE
&gt;&gt;&gt; import json
&gt;&gt;&gt; import requests as req
&gt;&gt;&gt; from lxml import etree
&gt;&gt;&gt; from selenium import webdriver
&gt;&gt;&gt; import os
&gt;&gt;&gt; request_url = &#39;https:&#47;&#47;movie.douban.com&#47;subject_search?search_text=å®«å´éª&amp;cat=1002&#39;
&gt;&gt;&gt; src_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
&gt;&gt;&gt; title_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;
&gt;&gt;&gt; driver = webdriver.Chrome(&#39;&#47;Users&#47;apple&#47;Downloads&#47;chromedriver&#39;)
&gt;&gt;&gt; driver.get(request_url)
&gt;&gt;&gt; html = etree.HTML(driver.page_source)
&gt;&gt;&gt; srcs = html.xpath(src_xpath)
&gt;&gt;&gt; print (srcs)  #å¤§å®¶å¯è¦çœ‹ä¸‹æ‰“å°å‡ºæ¥çš„æ•°æ®æ˜¯å¦åªæ˜¯ä¸€é¡µçš„å†…å®¹ï¼Œä»¥åŠå›¾ç‰‡urlçš„åç¼€æ ¼å¼
&gt;&gt;&gt; picpath = &#39;&#47;Users&#47;apple&#47;Downloads&#47;å®«å´éªç”µå½±æµ·æŠ¥&#39;
&gt;&gt;&gt; if not os.path.isdir(picpath):
	os.mkdir(picpath)
&gt;&gt;&gt; def download(src, id):
	dic = picpath + &#39;&#47;&#39; + str(id) + &#39;.webp&#39;
	try:
		pic = req.get(src, timeout = 30)
		fp = open(dic, &#39;wb&#39;)
		fp.write(pic.content)
		fp.close()
	except req.exceptions.ConnectionError:
		print (&#39;å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;)
&gt;&gt;&gt; for i in range(0, 150, 15):
	url = request_url + &#39;&amp;start=&#39; + str(i)
	driver.get(url)
	html = etree.HTML(driver.page_source)
	srcs = html.xpath(src_xpath)
	titles = html.xpath(title_xpath)
	for src,title in zip(srcs, titles):
		download(src, title.text)
</div>2019-04-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/64/bd/cbcdc4a6.jpg" width="30px"><span>rOMEoç½—å¯†æ¬§</span> ğŸ‘ï¼ˆ46ï¼‰ ğŸ’¬ï¼ˆ4ï¼‰<div>è€å¸ˆè¯·é—®ä¸€ä¸‹ï¼šå¦‚æœæ˜¯éœ€è¦ç”¨æˆ·ç™»é™†åæ‰èƒ½çˆ¬å–çš„æ•°æ®è¯¥æ€ä¹ˆç”¨pythonæ¥å®ç°å‘¢ï¼Ÿ</div>2019-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/1e/4c/79590513.jpg" width="30px"><span>Bayes</span> ğŸ‘ï¼ˆ18ï¼‰ ğŸ’¬ï¼ˆ8ï¼‰<div>è€å¸ˆä½ è¿™è·³è¿‡äº†å¤ªå¤šæ­¥éª¤äº†ï¼Œè¡¨ç¤ºå¯¹äºpythonè·Ÿç€ä½ å‰å‡ èŠ‚è¯¾å…¥é—¨çš„äººä»€ä¹ˆéƒ½ä¸ä¼šï¼ŒæŒ‰ç€ä½ çš„ä»£ç è¿è¡Œï¼Œè¦ä¸å°±æ˜¯æ²¡æœ‰å®šä¹‰ï¼Œè¦ä¸å°±æ˜¯æ²¡æœ‰è¿™ä¸ªå‡½æ•°ã€‚åˆšå¼€å§‹çš„äººä¹Ÿä¸çŸ¥é“å“ªä¸ªå‡½æ•°åœ¨å“ªä¸ªåº“ï¼Œå»ºè®®è€å¸ˆæŒ‰ç…§æµç¨‹æ¥ä¸€æ­¥ä¸€æ­¥ç»™ä»£ç ï¼Œè¦ä¸å°±åœ¨æœ€åç»™ä¸€ä¸ªå®Œæ•´çš„ä»£ç ç¤ºä¾‹ï¼ŒçœŸçš„æ˜¯å­¦çš„å¾ˆå›°éš¾åŠ ä¸Šæƒ³æ”¾å¼ƒ</div>2019-07-30</li><br/><li><img src="https://wx.qlogo.cn/mmopen/vi_32/l56HXkqMD4hqDg82yiaTr28GSwB1rCasj7n8M65VbibsJwmDibJnrWpw8o0mYGewH6NeD1wGvU6OGdhDG4nzIa9mA/132" width="30px"><span>LY</span> ğŸ‘ï¼ˆ16ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>#ç¯å¢ƒï¼šMac Python3
#pip install selenium
#ä¸‹è½½chromedriverï¼Œæ”¾åˆ°é¡¹ç›®è·¯å¾„ä¸‹ï¼ˆhttps:&#47;&#47;npm.taobao.org&#47;mirrors&#47;chromedriver&#47;2.33&#47;ï¼‰
# coding:utf-8
import requests
import json
import os
from lxml import etree
from selenium import webdriver

query = &#39;å¼ æŸèŠ&#39;
downloadPath = &#39;&#47;Users&#47;yong&#47;Desktop&#47;Python&#47;xpath&#47;images&#47;&#39;

&#39;&#39;&#39; ä¸‹è½½å›¾ç‰‡ &#39;&#39;&#39;
def download(src, id):
    dir = downloadPath + str(id) + &#39;.jpg&#39;
    try:
        pic = requests.get(src, timeout=10)
    except requests.exceptions.ConnectionError:
    # print &#39;error, %d å½“å‰å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;, %id
        print(&#39;å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;)
    if not os.path.exists(downloadPath):
        os.mkdir(downloadPath)
    if os.path.exists(dir):
        print(&#39;å·²å­˜åœ¨:&#39;+ id)
        return
    fp = open(dir, &#39;wb&#39;)
    fp.write(pic.content)
    fp.close()
 
def searchImages():
    &#39;&#39;&#39; for å¾ªç¯ è¯·æ±‚å…¨éƒ¨çš„ url &#39;&#39;&#39;
    for i in range(0, 22471, 20):
        url = &#39;https:&#47;&#47;www.douban.com&#47;j&#47;search_photo?q=&#39;+query+&#39;&amp;limit=20&amp;start=&#39;+str(i)
        html = requests.get(url).text    # å¾—åˆ°è¿”å›ç»“æœ
        print(&#39;html:&#39;+html)
        response = json.loads(html,encoding=&#39;utf-8&#39;) # å°† JSON æ ¼å¼è½¬æ¢æˆ Python å¯¹è±¡
        for image in response[&#39;images&#39;]:
            print(image[&#39;src&#39;]) # æŸ¥çœ‹å½“å‰ä¸‹è½½çš„å›¾ç‰‡ç½‘å€
            download(image[&#39;src&#39;], image[&#39;id&#39;]) # ä¸‹è½½ä¸€å¼ å›¾ç‰‡

def getMovieImages():
    url = &#39;https:&#47;&#47;movie.douban.com&#47;subject_search?search_text=&#39;+ query +&#39;&amp;cat=1002&#39;
    driver = webdriver.Chrome(&#39;&#47;Users&#47;yong&#47;Desktop&#47;Python&#47;xpath&#47;libs&#47;chromedriver&#39;)
    driver.get(url)
    html = etree.HTML(driver.page_source)
    # ä½¿ç”¨xpath helper, ctrl+shit+x é€‰ä¸­å…ƒç´ ï¼Œå¦‚æœè¦åŒ¹é…å…¨éƒ¨ï¼Œåˆ™éœ€è¦ä¿®æ”¹query è¡¨è¾¾å¼
    src_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
    title_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;

    srcs = html.xpath(src_xpath)
    titles = html.xpath(title_xpath)
    for src, title in zip(srcs, titles):
        print(&#39;\t&#39;.join([str(src),str(title.text)]))
        download(src, title.text)

    driver.close()

getMovieImages()</div>2019-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ca/1a/9ecbf1bb.jpg" width="30px"><span>ä¼ªå›å­</span> ğŸ‘ï¼ˆ16ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>é‚£äº›ç”¨ ChromeDriver çš„å‡ºç°æŠ¥é”™çš„å¯èƒ½æ˜¯æ²¡æœ‰å®‰è£… ChromeDriverï¼Œæˆ–è€…æ˜¯æ²¡ç»™å‡º ChromeDriver çš„è·¯å¾„ï¼Œå…·ä½“å¯ä»¥çœ‹çœ‹ä¸‹é¢è¿™ç¯‡æ–‡ç« ã€‚
https:&#47;&#47;mp.weixin.qq.com&#47;s&#47;UL0bcLr3KOb-qpI9oegaIQ</div>2019-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/87/d3/fe95bda9.jpg" width="30px"><span>é£˜</span> ğŸ‘ï¼ˆ9ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>æ„Ÿè°¢ä½œè€…ä»¥åŠè¯„è®ºåŒºçš„å„ä½å¤§ç¥ï¼Œç»ˆäºå®Œæˆäº†çˆ¬è™«ä»£ç ï¼Œæ€»ç»“ä¸€ä¸‹å°ç™½ç¼–å†™æ—¶é‡åˆ°çš„å‡ ä¸ªé—®é¢˜ï¼š
1ï¼‰è·å–xpathæ—¶ï¼Œchromeæµè§ˆå™¨éœ€è¦å®‰è£…æ’ä»¶xpatn-helperï¼›
2ï¼‰ä½¿ç”¨python3.7ï¼Œæå‰å¼•å…¥æ¨¡å—requestsï¼Œlxmlï¼Œseleniumï¼Œå®‰è£…è¿™äº›æ¨¡å—éœ€è¦æ›´æ–°pipè‡³20ç‰ˆæœ¬ï¼›
3ï¼‰æ¨¡æ‹Ÿç”¨æˆ·è®¿é—®æµè§ˆå™¨ï¼Œéœ€è¦ä¸‹è½½chromedriver.exe,æ”¾å…¥python.exeæ‰€åœ¨ç›®å½•ï¼›
4ï¼‰å›¾ç‰‡è·¯å¾„ä¸­å‡ºç°å¯¼è‡´ç¼–è¯‘å¤±è´¥çš„å­—ç¬¦ï¼Œä½¿ç”¨replaceæ›¿æ¢æŠ¥é”™å­—ç¬¦ã€‚
å…·ä½“ä»£ç å¦‚ä¸‹ï¼š
import os
import requests
from lxml import etree
from selenium import webdriver

search_text = &quot;ç‹ç¥–è´¤&quot;
start = 0
limit = 15
total = 90

def download(img, title):
	dir = &quot;D:\\æ•°æ®åˆ†æ\\python test\\query\\&quot; + search_text + &quot;\\&quot; 
	id = title.replace(u&#39;\u200e&#39;, u&#39;&#39;).replace(u&#39;?&#39;, u&#39;&#39;) .replace(u&#39;&#47;&#39;, u&#39;or&#39;)
	if not os.path.exists(dir):
		os.makedirs(dir)
	try:    
		pic = requests.get(img, timeout=10) 
		img_path = dir  + str(id) + &#39;.jpg&#39;   
		fp = open(img_path, &#39;wb&#39;)    
		fp.write(pic.content)    
		fp.close()  
	except requests.exceptions.ConnectionError:    
		print(&#39;å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;)

def crawler_xpath():
	src_img = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
	src_title = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;
	
	for i in range(start,total,limit):
		request_url = &quot;https:&#47;&#47;search.douban.com&#47;movie&#47;subject_search?search_text=&quot;+search_text+&quot;&amp;cat=1002&amp;start=&quot;+str(i)
		driver = webdriver.Chrome()
		driver.get(request_url)
		html = etree.HTML(driver.page_source)
		imgs = html.xpath(src_img)
		titles = html.xpath(src_title)
		print(imgs,titles)
		for img, title in zip(imgs, titles): 
			download(img, title.text)
if __name__ == &#39;__main__&#39;:
	crawler_xpath()
</div>2020-04-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/67/fb/cd95d624.jpg" width="30px"><span>germany</span> ğŸ‘ï¼ˆ6ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>è€å¸ˆï¼šä¸ºä»€ä¹ˆæˆ‘åœ¨è±†ç“£ç½‘æŸ¥è¯¢å›¾ç‰‡çš„ç½‘å€ä¸ä½ ä¸ä¸€æ ·ï¼Ÿhttps:&#47;&#47;www.douban.com&#47;search?cat=1025&amp;q=ç‹ç¥–è´¤&amp;source=suggest  ã€‚æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ</div>2019-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f8/99/8e760987.jpg" width="30px"><span>è¨±æ•²æ•²</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è¦ä¸‹è½½æ‰€æœ‰James å“ˆç™»çš„å›¾ç‰‡</div>2019-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/37/52/c1c56b6f.jpg" width="30px"><span>Geek_2008d9</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>ä¸ºä»€ä¹ˆæˆ‘æ€»æ˜¯response=json.loadsé‚£ä¸€è¡Œæ˜¾ç¤ºjson.decoder.JSONDecoderError:expecting value:line 1 column 1(char 0) å‘¢ï¼Œæ€ä¹ˆè§£å†³å•Šï¼Œå„ä½å¤§ä½¬</div>2019-12-14</li><br/><li><img src="" width="30px"><span>Geek_c45626</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ7ï¼‰<div>è€å¸ˆï¼Œè¿è¡Œä»£ç æ€»æ˜¯å‡ºé”™ï¼šJSONDecodeError: Expecting value: line 1 column 1 (char 0)ï¼Œè¿™ä¸ªæ€ä¹ˆè§£å†³ï¼Ÿ</div>2019-12-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>https:&#47;&#47;github.com&#47;qinggeouye&#47;GeekTime&#47;blob&#47;master&#47;DataAnalysis&#47;10_crawl_xpath.py

import os
import requests
from lxml import etree
from selenium import webdriver

search_text = &#39;ç‹ç¥–è´¤&#39;
start = 0  # è¯·æ±‚ url çš„ start ä» 0 å¼€å§‹ï¼Œæ¯ä¸€é¡µé—´éš” 15ï¼Œæœ‰ 6 é¡µ
total = 90
limit = 15

# ç”µå½±æµ·æŠ¥å›¾ç‰‡åœ°å€
src_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
# ç”µå½±æµ·æŠ¥å›¾ç‰‡title
title_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;

# ä¿å­˜ç›®å½•
pic_path = &#39;10&#47;xpath&#39;  # ç›¸å¯¹ç›®å½•
# WebDriver åˆ›å»ºä¸€ä¸ª Chrome æµè§ˆå™¨çš„ drive
driver = webdriver.Chrome(&#39;.&#47;chromedriver&#39;)  # MAC ç‰ˆæœ¬


# åˆ›å»ºå›¾ç‰‡ä¿å­˜è·¯å¾„
def mk_save_path(pic_path_):
    if not os.path.exists(pic_path_):
        os.makedirs(pic_path_)
    return os.getcwd() + &#39;&#47;&#39; + pic_path_ + &#39;&#47;&#39;


# ä¸‹è½½å›¾ç‰‡
def download(src, pic_id, save_path_):
    directory = save_path_ + str(pic_id) + &#39;.jpg&#39;
    try:
        pic = requests.get(src, timeout=10)
        fp = open(directory, &#39;wb&#39;)
        fp.write(pic.content)
        fp.close()
    except requests.exceptions.ConnectionError:
        print(&#39;å›¾ç‰‡å¦‚æ— æ³•ä¸‹è½½&#39;)


def get_response_xpath():
    save_path = mk_save_path(pic_path)
    for i in range(start, total, limit):
        requests_url = &#39;https:&#47;&#47;search.douban.com&#47;movie&#47;subject_search?search_text=&#39; + search_text + &#39;&amp;cat=1002&#39; + \
                       &#39;&amp;start=&#39; + str(i)
        driver.get(url=requests_url)
        html = etree.HTML(driver.page_source)
        src_list = html.xpath(src_xpath)
        title_list = html.xpath(title_xpath)
        for src, title in zip(src_list, title_list):
            download(src, title.text, save_path)


if __name__ == &#39;__main__&#39;:
    get_response_xpath()</div>2019-11-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/56/65/22a37a8e.jpg" width="30px"><span>Yezhiwei</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ç”¨Scrapyçˆ¬å–æ•°æ®æ›´æ–¹ä¾¿å“ˆï¼Œè¯·é—®è€å¸ˆæ€ä¹ˆåšä¸€ä¸ªé€šç”¨çš„çˆ¬è™«å‘¢ï¼Ÿæ¯”å¦‚è¦çˆ¬å–æ–‡ç« æ ‡é¢˜å’Œå†…å®¹ï¼Œä¸åŒçš„ç½‘ç«™Xpathç»“æ„ä¸ä¸€æ ·ï¼Œå¦‚æœæºå°‘çš„è¯å¯ä»¥åˆ†åˆ«é…ç½®ï¼Œä½†å¦‚æœè¦çˆ¬å–å‡ ç™¾ä¸Šåƒçš„ç½‘ç«™æ•°æ®ï¼Œåˆ†åˆ«é…ç½®XpathæŒºéº»çƒ¦çš„ã€‚è¯·é—®è¿™ä¸ªé—®é¢˜æœ‰ä»€ä¹ˆè§£å†³æ–¹æ¡ˆå—ï¼Ÿè°¢è°¢</div>2019-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>https:&#47;&#47;github.com&#47;qinggeouye&#47;GeekTime&#47;blob&#47;master&#47;DataAnalysis&#47;10_crawl.py

# coding: utf-8
import os

import requests
import json


# ä¸‹è½½å›¾ç‰‡
def download(src, pic_id, save_path_):
    directory = save_path_ + str(pic_id) + &#39;.jpg&#39;

    try:
        pic = requests.get(src, timeout=10)
        fp = open(directory, &#39;wb&#39;)
        fp.write(pic.content)
        fp.close()
    except requests.exceptions.ConnectionError:
        print(&#39;å›¾ç‰‡å¦‚æ— æ³•ä¸‹è½½&#39;)


# è·å–è¿”å›é¡µé¢å†…å®¹
def get_resp(query_, limit_, start_):
    url_ = &#39;https:&#47;&#47;www.douban.com&#47;j&#47;search_photo?q=&#39; + query_ + &#39;&amp;limit=&#39; + str(limit_) + &#39;&amp;start=&#39; + str(start_)
    html_ = requests.get(url_).text  # å¾—åˆ°è¿”å›ç»“æœ
    response_ = json.loads(html_, encoding=&#39;utf-8&#39;)  # å°† JSON æ ¼å¼è½¬æ¢ä¸º Python å¯¹è±¡
    return response_


query = &#39;ç‹ç¥–è´¤&#39;
limit = 20
start = 0

&#39;&#39;&#39; è·å–å›¾ç‰‡æ€»æ•°é‡ &#39;&#39;&#39;
total = get_resp(query, limit, start)[&#39;total&#39;]
print(total)

pic_path = &#39;10&#39;  # ç›¸å¯¹ç›®å½•
if not os.path.exists(pic_path):
    os.mkdir(pic_path)
save_path = os.getcwd() + &#39;&#47;&#39; + pic_path + &#39;&#47;&#39;

# å¾ªç¯ è¯·æ±‚å…¨éƒ¨çš„ url
for i in range(start, total, limit):
    response = get_resp(query, limit, i)
    for image in response[&#39;images&#39;]:
        print(image[&#39;src&#39;])  # æŸ¥çœ‹å½“å‰ä¸‹è½½çš„å›¾ç‰‡åœ°å€
        download(image[&#39;src&#39;], image[&#39;id&#39;], save_path)  # ä¸‹è½½ä¸€å¼ å›¾ç‰‡
</div>2019-11-05</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJjiaBHJyfAKK02CCcibkqI0jpaHJEcyrTRI4xbrqHCWiaia88WQs4r8zJVmHfibqricUYeUT2ezAZAC7wQ/132" width="30px"><span>çˆ±å–é…¸å¥¶çš„ç¨‹åºå‘˜</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æœ‰ä¸ªé—®é¢˜seleniumï¼Œæ˜¯ç”¨æ¥è‡ªåŠ¨åŒ–æµ‹è¯•çš„ï¼Œä»–å›æ‰“å¼€æµè§ˆå™¨â€¦â€¦æˆ‘åšçˆ¬è™«æ˜¯ä¸æƒ³è®©ä»£ç æ‰“å¼€æµè§ˆå™¨ï¼Œåªæƒ³è¦ä»–çˆ¬å–çš„åŠ¨ä½œï½è¦æ€ä¹ˆåŠå‘¢ï¼Ÿ</div>2019-02-26</li><br/><li><img src="" width="30px"><span>åå…­ã€‚</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ç”±äºpythonå­¦ä¹ æ˜¯çˆ¬è™«ä¸Šæ‰‹çš„ï¼Œçˆ¬è™«è¿˜æ˜¯ä¼šä¸€ç‚¹çš„ï¼Œå“ˆå“ˆï¼Œå»ä¸‹ä¸€ç« äº†áƒ¦( Â´ï½¥á´—ï½¥` )</div>2020-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg" width="30px"><span>æ˜ç¿¼</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>æˆ‘åœ¨å®è·µè¿™ç¯‡è¿‡ç¨‹ä¸­é‡åˆ°äº†å¾ˆå¤šé—®é¢˜ï¼Œæœ€ç»ˆè§£å†³äº†ï¼Œå†™åœ¨æˆ‘çš„å…¬ä¼—å·é‡Œé¢ï¼Œè¿è¡Œä»£ç æœ‰é—®é¢˜çš„åŒå­¦å¯ä»¥å‚è€ƒä¸‹ï¼š
https:&#47;&#47;mp.weixin.qq.com&#47;s?__biz=MjM5OTE4MzcyNA==&amp;tempkey=MTA0MF95UW5FRmVZZURiWVR2ZWZiZVVaUEctS3FhUF90OVljc3RZTEV6eHpKSjF3NlpxMjhMcXdoU2trV2Y1RzdCQXZQamptXzZTODZNbGw0U3ZmMlhzT1BFOWZWeXhaOTM3bHFITjl6dVBLbUxfSlI4ZG15bFpvYnpvUTJienlKOWx2M0V1QURvZWVZUU1rTVRudk96WXZTb01qekdEWmJhaW5zMDd3OFB3fn4%3D&amp;chksm=3f29d0a1085e59b7d37190f6e8580bc03526dca518fc70e844a76ae1a571bc90d88ac52acc88#rd</div>2019-12-21</li><br/><li><img src="https://wx.qlogo.cn/mmopen/vi_32/DYAIOgq83eppQqDE6TNibvr3DNdxG323AruicIgWo5DpVr6U7yZVNkbF2rKluyDfhdpgAEcYEOZTAnbrMdTzFkUw/0" width="30px"><span>å›¾Â·ç¾å…‹å°”</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æˆ‘æ›´å–œæ¬¢ç”¨bs4ç¾å‘³æ±¤</div>2019-07-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/18/86/3599130b.jpg" width="30px"><span>dany</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>downloadåˆ°å“ªé‡Œå»äº†ï¼Ÿä¸å¥½æ„æ€æˆ‘æ˜¯èœé¸Ÿ</div>2019-07-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/dc/ce/03fdeb60.jpg" width="30px"><span>ç™½è‰²çº¯åº¦</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>ç½‘å€ï¼šè±†ç“£ç”µå½±ï¼›ä»»åŠ¡ï¼šæ‰¹é‡ä¸‹è½½èµµä¸½é¢–ç”µå½±æµ·æŠ¥ï¼ˆæ”¯æŒç¿»é¡µï¼Œè‡ªå®šä¹‰ç»ˆæ­¢ä¸‹è½½é‡ï¼‰ï¼›python3.Xï¼›
æµè§ˆå™¨ï¼šGoogle Chrome ï¼›å”¯ä¸€è¦æ³¨æ„çš„æ˜¯webdriverçš„è·¯å¾„ã€‚å…¨éƒ½æ˜¯è¿™é—¨è¯¾ç¨‹é‡Œé¢çš„çŸ¥è¯†ç‚¹
# -*- coding: utf-8 -*-
import requests
from lxml import etree
from selenium import webdriver
import os

name = &#39;èµµä¸½é¢–&#39;

def download(src, id):
    if not os.path.isdir(&quot;Xpathçš„ç¿»é¡µå›¾ç‰‡åŒ…&quot;):
        os.mkdir(&quot;Xpathçš„ç¿»é¡µå›¾ç‰‡åŒ…&quot;)
    dir = os.path.join(&quot;Xpathçš„ç¿»é¡µå›¾ç‰‡åŒ…&#47;&quot;, str(id) + &#39;.webp&#39;)
    try:
        pic = requests.get(src, timeout = 10)
        with open(dir, &#39;wb&#39;) as d:
            d.write(pic.content)
    except requests.exceptions.ConnectionError:
        print(&quot;å›¾ç‰‡æ— æ³•ä¸‹è½½&quot;)

def down_load(request_url):
    driver.get(request_url)
    html = etree.HTML(driver.page_source)
    src_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;a[@class=&#39;cover-link&#39;]&#47;img[@class=&#39;cover&#39;]&#47;@src&quot;
    title_xpath = &quot;&#47;&#47;div[@class=&#39;item-root&#39;]&#47;div[@class=&#39;detail&#39;]&#47;div[@class=&#39;title&#39;]&#47;a[@class=&#39;title-text&#39;]&quot;
    srcs = html.xpath(src_xpath)


    titles = html.xpath(title_xpath)
    num = len(srcs)
    if num &gt; 15:
        srcs = srcs[1:]
        titles = titles[1:]

    for src, title in zip(srcs, titles):
        if title is None:
            continue
        print(src)
        download(src, title.text)
    print(&#39;OK&#39;)
    print(num)
    if num &gt;= 1:
        return True
    else:
        return False

if __name__ == &#39;__main__&#39;:
    requests_url = &quot;https:&#47;&#47;movie.douban.com&#47;subject_search?search_text=&quot; + name
    driver = webdriver.Chrome(executable_path=r&#39;C:\Users\XXX\AppData\Local\Google\Chrome\Application\chromedriver.exe&#39;)
    driver.get(requests_url)
    html = etree.HTML(driver.page_source)
    print(html)

    base_url = &#39;https:&#47;&#47;movie.douban.com&#47;subject_search?search_text=&#39; + name + &#39;&amp;cat=1002&amp;start=&#39;
    start = 0
    while start &lt; 70:
        request_url = base_url + str(start)
        flag = down_load(request_url)
        if flag:
            start += 15
        else:
            break
    print(&quot;ç»“æŸ&quot;)</div>2019-06-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/02/ce/57c871e0.jpg" width="30px"><span>äº‘æ·±ä¸çŸ¥å¤„</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ç»“åˆè€å¸ˆå’Œç²¾é€‰ç•™è¨€æºç ï¼ŒæŠ“å–â€œç‹ç¥–è´¤â€å›¾ç‰‡å’Œç”µå½±æµ·æŠ¥ï¼Œä»£ç åœ¨è‡ªå·±ç¯å¢ƒä¸­è°ƒè¯•æˆåŠŸï¼Œè¿˜æŒºæœ‰è¶£ã€‚</div>2019-06-09</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/4kF5cFK9MN6WX9Dibodh8pWIib06icoSgSPb6pAhGVjO3gpD72R77eicGkCUWfl3feNtn2icEibhUvgWt890rYLYGoqg/132" width="30px"><span>chitanda</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>åˆ†äº«ä¸€ä¸ªå¯ä»¥åœ¨ä¸“é¢˜é¡µé¢ä¸‹è½½èŒ…é‡çˆ±è¡£ç¼©ç•¥å›¾çš„è„šæœ¬ï¼Œsrc_xpath = &quot;&#47;&#47;img[@class=&#39;&#39;]&#47;@src&quot;ä¸­çš„class=&#39;&#39; è®©æˆ‘æäº†åŠå¤©ï¼Œè‡³ä»Šä¸çŸ¥é“ä¸ºä»€ä¹ˆä¸å­˜åœ¨class nameæ—¶å¿…é¡»åŠ ä¸€å¥class=&#39;&#39;ï¼Œä¸‹é¢æ˜¯ä»£ç 
import os
import uuid

from lxml import etree

import requests


def download(src, name=None):
    if not name:
        name = uuid.uuid1()
    if not os.path.isdir(&#39;Kayano&#39;):
        os.mkdir(&#39;Kayano&#39;)
    adir = os.path.join(&#39;Kayano&#47;&#39;, str(name) + &#39;.jpg&#39;)
    try:
        pic = requests.get(src, timeout=10)
        with open(adir, &#39;wb&#39;) as f:
            f.write(pic.content)
    except requests.exceptions.ConnectionError:
        print(&#39;å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;)


if __name__ == &#39;__main__&#39;:
    
    from selenium import webdriver
    
    request_url = &#39;https:&#47;&#47;movie.douban.com&#47;celebrity&#47;1314532&#47;photos&#47;&#39;
    driver = webdriver.Chrome(executable_path=&#39;chromedriver.exe&#39;)
    driver.get(request_url)
    html = etree.HTML(driver.page_source)
    
    src_xpath = &quot;&#47;&#47;img[@class=&#39;&#39;]&#47;@src&quot;
    srcs = html.xpath(src_xpath)
    for src in srcs:
        download(src)
</div>2019-04-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/b6/43/c7b2a6f9.jpg" width="30px"><span>ç«¹æœ¬å…ˆç”Ÿ</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div># coding:utf-8
import requests as rq
import json
import re
from lxml import etree

# ä¸‹è½½å›¾ç‰‡
def download(src, title):
	# è¿‡æ»¤éæ³•å­—ç¬¦
	rstr = r&quot;[\&#47;\\\:\*\?\&quot;\&lt;\&gt;\|]&quot;
	title = re.sub(rstr, &quot;&quot;, title)
	
	dir = &#39;.&#47;film_pic&#47;&#39; + str(title) + &#39;.jpg&#39;
	
	try:
		pic = rq.get(src, timeout=10)
		with open(dir, &#39;wb&#39;) as fp:
			fp.write(pic.content)
	except rq.exceptions.ConnectionError:
		return False
	else:
		return True

# è·å–ç”µå½±æ•°é‡
def get_film_amount(performer_name):
	url = &#39;https:&#47;&#47;www.douban.com&#47;j&#47;search?q=&#39; +performer_name+ &#39;&amp;start=0&amp;cat=1002&#39;
	result = rq.get(url)
	result_obj = json.loads(result.text)
	return int(result_obj[&#39;total&#39;])

# è·å–ç”µå½±ä¿¡æ¯
def get_film_info(query_name):
	# XPathè§„åˆ™
	title_xpath = &quot;&#47;&#47;div[@class=&#39;content&#39;]&#47;div[@class=&#39;title&#39;]&#47;h3&#47;a&quot;
	pic_xpath = &quot;&#47;&#47;div[@class=&#39;pic&#39;]&#47;a[@class=&#39;nbg&#39;]&#47;img&#47;@src&quot;

	titles = []
	pics = []

	film_amount = get_film_amount(query_name)

	for i in range(0, film_amount, 20):
		url = &#39;https:&#47;&#47;www.douban.com&#47;j&#47;search?q=&#39; +query_name+ &#39;&amp;start=&#39;+str(i)+&#39;&amp;cat=1002&#39;
		result = rq.get(url)
		result_obj = json.loads(result.text)
		
		for item in result_obj[&#39;items&#39;]:
			html = etree.HTML(item)
			tmp_titles = html.xpath(title_xpath)
			tmp_pics = html.xpath(pic_xpath)
			
			if &#39;default_small&#39; not in tmp_pics[0]:
				titles.append(tmp_titles[0].text.strip())
				pics.append(tmp_pics[0])
	
	return {&#39;name&#39;: query_name, &#39;amount&#39;: film_amount, &#39;list&#39;: zip(titles,pics)}

query_name_list = [&#39;å‘¨æ¶¦å‘&#39;,&#39;ç”„å­ä¸¹&#39;,&#39;å‘¨æ˜Ÿé©°&#39;,&#39;åˆ˜å¾·å&#39;,&#39;ç‹ç¥–è´¤&#39;,&#39;å…ƒå½ª&#39;,&#39;é³ä¸œ&#39;,&#39;ç‹å‡¯&#39;,&#39;å¼ é“æ—&#39;,&#39;æ®µå¥•å®&#39;,&#39;éƒ‘ä¼Šå¥&#39;,&#39;å¼ è€€æ‰¬&#39;,&#39;é™ˆä¹”æ©&#39;]

for query_name in query_name_list:
	print(query_name + &#39; å…±æœ‰&#39; + str(get_film_amount(query_name)) + &#39;éƒ¨ç”µå½±&#39;)
	film_info = get_film_info(query_name)
	order = 1
	for title,src in film_info[&#39;list&#39;]:
		
		if download(src, query_name +&quot; - &quot;+ title):
			print(str(order).zfill(3) + &#39;. ä¸‹è½½æˆåŠŸï¼š&#39; + title)
		else:
			print(str(order).zfill(3) + &#39;. ä¸‹è½½å¤±è´¥ï¼š&#39; + title)
		order += 1</div>2019-01-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/10/df/e4511752.jpg" width="30px"><span>å‘¨èåœ</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ä¸‹è½½ç‹ç¥–è´¤çš„æµ·æŠ¥ï¼Œå¹¶ä¸”æŠŠæ¯å¼ æµ·æŠ¥çš„è¯„è®ºä¿å­˜åœ¨MongoDBä¸­ï¼Œä»£ç å¦‚ä¸‹ï¼š
https:&#47;&#47;github.com&#47;zhouwei713&#47;douban&#47;tree&#47;master&#47;wangzuxian_poster
</div>2019-01-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/de/a0/88be6607.jpg" width="30px"><span>Allen</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div># Selenium æ¨¡æ‹Ÿ ä¸‹è½½å®«å´éªåŠ¨ç”»ç”µå½±æµ·æŠ¥
import os
import requests
from selenium import webdriver

&#39;&#39;&#39;è®¿é—®è±†ç“£ç”µå½±ä¸»é¡µ æœç´¢å®«å´éª&#39;&#39;&#39;
query = &#39;å®«å´éª&#39;
url = &#39;https:&#47;&#47;movie.douban.com&#47;&#39;
driver = webdriver.Chrome(executable_path=&#39;.&#47;chromedriver&#39;)
driver.get(url)
query_input = driver.find_element_by_xpath(&#39;&#47;&#47;*[@id=&quot;inp-query&quot;]&#39;)
query_input.send_keys(query)
query_input.submit()

&#39;&#39;&#39;ä¸‹è½½å›¾ç‰‡&#39;&#39;&#39;
def download(src, id):
    dir = f&quot;.&#47;{query}&#47;{id}.{src.split(&#39;.&#39;)[-1]}&quot;
    if not os.path.isdir(query):
        os.mkdir(query)
    if os.path.exists(dir):
        return
    try:
        pic = requests.get(src, timeout=10)
        with open(dir, &#39;wb&#39;) as f:
            f.write(pic.content)
    except requests.exceptions.ConnectionError:
        print(&#39;å›¾ç‰‡æ— æ³•ä¸‹è½½&#39;)
        
&#39;&#39;&#39;æŸ¥çœ‹ç¿»é¡µæŒ‰é’®ï¼Œå®ç°ç¿»é¡µæ•ˆæœ&#39;&#39;&#39;
next_page = driver.find_element_by_xpath(&#39;&#47;&#47;*[@class=&quot;next&quot;]&#39;)

num = 1
while next_page:
    print(f&#39;å½“å‰é¡µä¸º ç¬¬ {num} é¡µ&#39;)
   # æŸ¥çœ‹å½“å‰é¡µï¼Œå¹¶ä¿å­˜å›¾ç‰‡
    srcs = driver.find_elements_by_xpath(&#39;&#47;&#47;div[@class=&quot;item-root&quot;]&#47;a[@class=&quot;cover-link&quot;]&#47;img[@class=&quot;cover&quot;]&#39;)
    titles = driver.find_elements_by_xpath(&#39;&#47;&#47;div[@class=&quot;item-root&quot;]&#47;div[@class=&quot;detail&quot;]&#47;div[@class=&quot;title&quot;]&#47;a[@class=&quot;title-text&quot;]&#39;)
    for src, title in zip(srcs, titles):
        download(src.get_attribute(&#39;src&#39;), title.text)
    next_page = driver.find_element_by_xpath(&#39;&#47;&#47;*[@class=&quot;next&quot;]&#39;)
    next_page.click()
    num += 1
    if num &gt; 5:
        break

driver.quit()        </div>2020-09-21</li><br/><li><img src="" width="30px"><span>ä¸ä¼šç‹—å¸¦</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>è€å¸ˆï¼Œæˆ‘çš„ä»£ç æŠ¥é”™æ˜¯no module named requests.  è¿˜æœ‰å¤§éƒ¨åˆ†å†…å®¹ä¸çŸ¥é“è€å¸ˆåœ¨è¯´ä»€ä¹ˆï¼Œæ˜¯å› ä¸ºè‡ªå·±çš„é—®é¢˜ï¼Œè¿˜æ˜¯è€å¸ˆä¸­é—´è·³è¿‡äº†å‡ æ­¥å‘€ï¼ŸLOLã€‚ğŸ˜‚ è¯·é—®è€å¸ˆå¯ä¸å¯ä»¥æŠŠå®Œæ•´çš„ä»£ç å…¬å¸ƒä¸€ä¸‹ï¼Œæˆ–è€…æœ‰æ²¡æœ‰å°çš„è§†é¢‘å¯ä»¥è·Ÿç€æ“ä½œå‘¢ï¼Ÿ</div>2020-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/86/32/6a8049eb.jpg" width="30px"><span>æå®½</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è¯•äº†ä¸‹ï¼Œéœ€è¦æ·»åŠ headersæ‰èƒ½æˆåŠŸ
ä¸è¿‡ï¼Œä¸‹è½½ä¸‹æ¥çš„å›¾ç‰‡æ–‡ä»¶ä¸ºä»€ä¹ˆæ‰“ä¸å¼€å‘€ï¼Ÿ</div>2020-01-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg" width="30px"><span>æ˜ç¿¼</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æ¨¡æ‹Ÿæµè§ˆå™¨ä¸‹è½½å›¾ç‰‡çš„å¯ä»¥å‚è€ƒhttps:&#47;&#47;mp.weixin.qq.com&#47;s&#47;1qAcKbtV2Mr_Q-ieb_8gqg</div>2019-12-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/64/9b/d1ab239e.jpg" width="30px"><span>J.Smile</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼Œä½ è®°å½•çš„ç¬”è®°å¥½æ¼‚äº®å•Š</div>2019-11-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/26/16/9de2666e.jpg" width="30px"><span>é™³é™½</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>ä¸é”™</div>2019-10-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/65/1a/5f3d7f5f.jpg" width="30px"><span>é”¦æ——å°èƒ½çŒ«</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æˆ‘è§‰å¾—è€å¸ˆè¿™ä¸ªæ­¥éª¤å°±åº”è¯¥å†™å‡ºæ¥ å¾ˆå¤šäººé—®chromedriveæŠ¥é”™å’Œxpathè¿™å—ï¼Œè·Ÿç€åšä¼°è®¡æ˜¯çœ‹ä¸å¤ªæ‡‚</div>2019-08-20</li><br/>
</ul>