æˆ‘ä»¬ä¸Šä¸€èŠ‚è®²äº†æœ´ç´ è´å¶æ–¯çš„å·¥ä½œåŸç†ï¼Œä»Šå¤©æˆ‘ä»¬æ¥è®²ä¸‹è¿™äº›åŸç†æ˜¯å¦‚ä½•æŒ‡å¯¼å®é™…ä¸šåŠ¡çš„ã€‚

æœ´ç´ è´å¶æ–¯åˆ†ç±»æœ€é€‚åˆçš„åœºæ™¯å°±æ˜¯æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œåƒåœ¾é‚®ä»¶è¯†åˆ«ã€‚å…¶ä¸­æƒ…æ„Ÿåˆ†æå’Œåƒåœ¾é‚®ä»¶è¯†åˆ«éƒ½æ˜¯é€šè¿‡æ–‡æœ¬æ¥è¿›è¡Œåˆ¤æ–­ã€‚ä»è¿™é‡Œä½ èƒ½çœ‹å‡ºæ¥ï¼Œè¿™ä¸‰ä¸ªåœºæ™¯æœ¬è´¨ä¸Šéƒ½æ˜¯æ–‡æœ¬åˆ†ç±»ï¼Œè¿™ä¹Ÿæ˜¯æœ´ç´ è´å¶æ–¯æœ€æ“…é•¿çš„åœ°æ–¹ã€‚æ‰€ä»¥æœ´ç´ è´å¶æ–¯ä¹Ÿå¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†NLPçš„å·¥å…·ã€‚

ä»Šå¤©æˆ‘å¸¦ä½ ä¸€èµ·ä½¿ç”¨æœ´ç´ è´å¶æ–¯åšä¸‹æ–‡æ¡£åˆ†ç±»çš„é¡¹ç›®ï¼Œæœ€é‡è¦çš„å·¥å…·å°±æ˜¯sklearnè¿™ä¸ªæœºå™¨å­¦ä¹ ç¥å™¨ã€‚

## sklearnæœºå™¨å­¦ä¹ åŒ…

sklearnçš„å…¨ç§°å«Scikit-learnï¼Œå®ƒç»™æˆ‘ä»¬æä¾›äº†3ä¸ªæœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•ï¼Œåˆ†åˆ«æ˜¯é«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼ˆGaussianNBï¼‰ã€å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ï¼ˆMultinomialNBï¼‰å’Œä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼ˆBernoulliNBï¼‰ã€‚

è¿™ä¸‰ç§ç®—æ³•é€‚åˆåº”ç”¨åœ¨ä¸åŒçš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥æ ¹æ®ç‰¹å¾å˜é‡çš„ä¸åŒé€‰æ‹©ä¸åŒçš„ç®—æ³•ï¼š

**é«˜æ–¯æœ´ç´ è´å¶æ–¯**ï¼šç‰¹å¾å˜é‡æ˜¯è¿ç»­å˜é‡ï¼Œç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œæ¯”å¦‚è¯´äººçš„èº«é«˜ï¼Œç‰©ä½“çš„é•¿åº¦ã€‚

**å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯**ï¼šç‰¹å¾å˜é‡æ˜¯ç¦»æ•£å˜é‡ï¼Œç¬¦åˆå¤šé¡¹åˆ†å¸ƒï¼Œåœ¨æ–‡æ¡£åˆ†ç±»ä¸­ç‰¹å¾å˜é‡ä½“ç°åœ¨ä¸€ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°ï¼Œæˆ–è€…æ˜¯å•è¯çš„TF-IDFå€¼ç­‰ã€‚

**ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯**ï¼šç‰¹å¾å˜é‡æ˜¯å¸ƒå°”å˜é‡ï¼Œç¬¦åˆ0/1åˆ†å¸ƒï¼Œåœ¨æ–‡æ¡£åˆ†ç±»ä¸­ç‰¹å¾æ˜¯å•è¯æ˜¯å¦å‡ºç°ã€‚

ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯æ˜¯ä»¥æ–‡ä»¶ä¸ºç²’åº¦ï¼Œå¦‚æœè¯¥å•è¯åœ¨æŸæ–‡ä»¶ä¸­å‡ºç°äº†å³ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚è€Œå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æ˜¯ä»¥å•è¯ä¸ºç²’åº¦ï¼Œä¼šè®¡ç®—åœ¨æŸä¸ªæ–‡ä»¶ä¸­çš„å…·ä½“æ¬¡æ•°ã€‚è€Œé«˜æ–¯æœ´ç´ è´å¶æ–¯é€‚åˆå¤„ç†ç‰¹å¾å˜é‡æ˜¯è¿ç»­å˜é‡ï¼Œä¸”ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰çš„æƒ…å†µã€‚æ¯”å¦‚èº«é«˜ã€ä½“é‡è¿™ç§è‡ªç„¶ç•Œçš„ç°è±¡å°±æ¯”è¾ƒé€‚åˆç”¨é«˜æ–¯æœ´ç´ è´å¶æ–¯æ¥å¤„ç†ã€‚è€Œæ–‡æœ¬åˆ†ç±»æ˜¯ä½¿ç”¨å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æˆ–è€…ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ã€‚

## ä»€ä¹ˆæ˜¯TF-IDFå€¼å‘¢ï¼Ÿ

æˆ‘åœ¨å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ä¸­æåˆ°äº†â€œè¯çš„TF-IDFå€¼â€ï¼Œå¦‚ä½•ç†è§£è¿™ä¸ªæ¦‚å¿µå‘¢ï¼Ÿ

TF-IDFæ˜¯ä¸€ä¸ªç»Ÿè®¡æ–¹æ³•ï¼Œç”¨æ¥è¯„ä¼°æŸä¸ªè¯è¯­å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–æ–‡æ¡£åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚

TF-IDFå®é™…ä¸Šæ˜¯ä¸¤ä¸ªè¯ç»„Term Frequencyå’ŒInverse Document Frequencyçš„æ€»ç§°ï¼Œä¸¤è€…ç¼©å†™ä¸ºTFå’ŒIDFï¼Œåˆ†åˆ«ä»£è¡¨äº†è¯é¢‘å’Œé€†å‘æ–‡æ¡£é¢‘ç‡ã€‚

**è¯é¢‘TF**è®¡ç®—äº†ä¸€ä¸ªå•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œå®ƒè®¤ä¸ºä¸€ä¸ªå•è¯çš„é‡è¦æ€§å’Œå®ƒåœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°å‘ˆæ­£æ¯”ã€‚

**é€†å‘æ–‡æ¡£é¢‘ç‡IDF**ï¼Œæ˜¯æŒ‡ä¸€ä¸ªå•è¯åœ¨æ–‡æ¡£ä¸­çš„åŒºåˆ†åº¦ã€‚å®ƒè®¤ä¸ºä¸€ä¸ªå•è¯å‡ºç°åœ¨çš„æ–‡æ¡£æ•°è¶Šå°‘ï¼Œå°±è¶Šèƒ½é€šè¿‡è¿™ä¸ªå•è¯æŠŠè¯¥æ–‡æ¡£å’Œå…¶ä»–æ–‡æ¡£åŒºåˆ†å¼€ã€‚IDFè¶Šå¤§å°±ä»£è¡¨è¯¥å•è¯çš„åŒºåˆ†åº¦è¶Šå¤§ã€‚

**æ‰€ä»¥TF-IDFå®é™…ä¸Šæ˜¯è¯é¢‘TFå’Œé€†å‘æ–‡æ¡£é¢‘ç‡IDFçš„ä¹˜ç§¯**ã€‚è¿™æ ·æˆ‘ä»¬å€¾å‘äºæ‰¾åˆ°TFå’ŒIDFå–å€¼éƒ½é«˜çš„å•è¯ä½œä¸ºåŒºåˆ†ï¼Œå³è¿™ä¸ªå•è¯åœ¨ä¸€ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°å¤šï¼ŒåŒæ—¶åˆå¾ˆå°‘å‡ºç°åœ¨å…¶ä»–æ–‡æ¡£ä¸­ã€‚è¿™æ ·çš„å•è¯é€‚åˆç”¨äºåˆ†ç±»ã€‚

## TF-IDFå¦‚ä½•è®¡ç®—

é¦–å…ˆæˆ‘ä»¬çœ‹ä¸‹è¯é¢‘TFå’Œé€†å‘æ–‡æ¡£æ¦‚ç‡IDFçš„å…¬å¼ã€‚

![](https://static001.geekbang.org/resource/image/bc/4d/bc31ff1f31f9cd26144404221f705d4d.png?wh=276%2A87)

![](https://static001.geekbang.org/resource/image/b7/65/b7ad53560f61407e6964e7436da14365.png?wh=469%2A80)

ä¸ºä»€ä¹ˆIDFçš„åˆ†æ¯ä¸­ï¼Œå•è¯å‡ºç°çš„æ–‡æ¡£æ•°è¦åŠ 1å‘¢ï¼Ÿå› ä¸ºæœ‰äº›å•è¯å¯èƒ½ä¸ä¼šå­˜åœ¨æ–‡æ¡£ä¸­ï¼Œä¸ºäº†é¿å…åˆ†æ¯ä¸º0ï¼Œç»Ÿä¸€ç»™å•è¯å‡ºç°çš„æ–‡æ¡£æ•°éƒ½åŠ 1ã€‚

**TF-IDF=TF\*IDFã€‚**

ä½ å¯ä»¥çœ‹åˆ°ï¼ŒTF-IDFå€¼å°±æ˜¯TFä¸IDFçš„ä¹˜ç§¯,è¿™æ ·å¯ä»¥æ›´å‡†ç¡®åœ°å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç±»ã€‚æ¯”å¦‚â€œæˆ‘â€è¿™æ ·çš„é«˜é¢‘å•è¯ï¼Œè™½ç„¶TFè¯é¢‘é«˜ï¼Œä½†æ˜¯IDFå€¼å¾ˆä½ï¼Œæ•´ä½“çš„TF-IDFä¹Ÿä¸é«˜ã€‚

æˆ‘åœ¨è¿™é‡Œä¸¾ä¸ªä¾‹å­ã€‚å‡è®¾ä¸€ä¸ªæ–‡ä»¶å¤¹é‡Œä¸€å…±æœ‰10ç¯‡æ–‡æ¡£ï¼Œå…¶ä¸­ä¸€ç¯‡æ–‡æ¡£æœ‰1000ä¸ªå•è¯ï¼Œâ€œthisâ€è¿™ä¸ªå•è¯å‡ºç°20æ¬¡ï¼Œâ€œbayesâ€å‡ºç°äº†5æ¬¡ã€‚â€œthisâ€åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­å‡å‡ºç°è¿‡ï¼Œè€Œâ€œbayesâ€åªåœ¨2ç¯‡æ–‡æ¡£ä¸­å‡ºç°è¿‡ã€‚æˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹è¿™ä¸¤ä¸ªè¯è¯­çš„TF-IDFå€¼ã€‚

é’ˆå¯¹â€œthisâ€ï¼Œè®¡ç®—TF-IDFå€¼ï¼š

![](https://static001.geekbang.org/resource/image/63/12/63abe3ce8aa0ea4a78ba537b5504df12.png?wh=226%2A77)

![](https://static001.geekbang.org/resource/image/b5/7e/b5ac88c4e2a71cc2d4ceef4c01e0ba7e.png?wh=403%2A76)

æ‰€ä»¥TF-IDF=0.02\*(-0.0414)=-8.28e-4ã€‚

é’ˆå¯¹â€œbayesâ€ï¼Œè®¡ç®—TF-IDFå€¼ï¼š

![](https://static001.geekbang.org/resource/image/3b/8d/3bbe56a7b76513604bfe6b39b890dd8d.png?wh=241%2A53)

![](https://static001.geekbang.org/resource/image/1e/2e/1e8b7465b9949fe071e95aede172a52e.png?wh=375%2A50)

TF-IDF=0.005\*0.5229=2.61e-3ã€‚

å¾ˆæ˜æ˜¾â€œbayesâ€çš„TF-IDFå€¼è¦å¤§äºâ€œthisâ€çš„TF-IDFå€¼ã€‚è¿™å°±è¯´æ˜ç”¨â€œbayesâ€è¿™ä¸ªå•è¯åšåŒºåˆ†æ¯”å•è¯â€œthisâ€è¦å¥½ã€‚

**å¦‚ä½•æ±‚TF-IDF**

åœ¨sklearnä¸­æˆ‘ä»¬ç›´æ¥ä½¿ç”¨TfidfVectorizerç±»ï¼Œå®ƒå¯ä»¥å¸®æˆ‘ä»¬è®¡ç®—å•è¯TF-IDFå‘é‡çš„å€¼ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œå–sklearnè®¡ç®—çš„å¯¹æ•°logæ—¶ï¼Œåº•æ•°æ˜¯eï¼Œä¸æ˜¯10ã€‚

ä¸‹é¢æˆ‘æ¥è®²ä¸‹å¦‚ä½•åˆ›å»ºTfidfVectorizerç±»ã€‚

## TfidfVectorizerç±»çš„åˆ›å»ºï¼š

åˆ›å»ºTfidfVectorizerçš„æ–¹æ³•æ˜¯ï¼š

```
TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)
```

æˆ‘ä»¬åœ¨åˆ›å»ºçš„æ—¶å€™ï¼Œæœ‰ä¸¤ä¸ªæ„é€ å‚æ•°ï¼Œå¯ä»¥è‡ªå®šä¹‰åœç”¨è¯stop\_wordså’Œè§„å¾‹è§„åˆ™token\_patternã€‚éœ€è¦æ³¨æ„çš„æ˜¯ä¼ é€’çš„æ•°æ®ç»“æ„ï¼Œåœç”¨è¯stop\_wordsæ˜¯ä¸€ä¸ªåˆ—è¡¨Listç±»å‹ï¼Œè€Œè¿‡æ»¤è§„åˆ™token\_patternæ˜¯æ­£åˆ™è¡¨è¾¾å¼ã€‚

ä»€ä¹ˆæ˜¯åœç”¨è¯ï¼Ÿåœç”¨è¯å°±æ˜¯åœ¨åˆ†ç±»ä¸­æ²¡æœ‰ç”¨çš„è¯ï¼Œè¿™äº›è¯ä¸€èˆ¬è¯é¢‘TFé«˜ï¼Œä½†æ˜¯IDFå¾ˆä½ï¼Œèµ·ä¸åˆ°åˆ†ç±»çš„ä½œç”¨ã€‚ä¸ºäº†èŠ‚çœç©ºé—´å’Œè®¡ç®—æ—¶é—´ï¼Œæˆ‘ä»¬æŠŠè¿™äº›è¯ä½œä¸ºåœç”¨è¯stop wordsï¼Œå‘Šè¯‰æœºå™¨è¿™äº›è¯ä¸éœ€è¦å¸®æˆ‘è®¡ç®—ã€‚

![](https://static001.geekbang.org/resource/image/04/e9/040723cc99b36e8ad7e45aa31e0690e9.png?wh=592%2A97)  
å½“æˆ‘ä»¬åˆ›å»ºå¥½TF-IDFå‘é‡ç±»å‹æ—¶ï¼Œå¯ä»¥ç”¨fit\_transformå¸®æˆ‘ä»¬è®¡ç®—ï¼Œè¿”å›ç»™æˆ‘ä»¬æ–‡æœ¬çŸ©é˜µï¼Œè¯¥çŸ©é˜µè¡¨ç¤ºäº†æ¯ä¸ªå•è¯åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­çš„TF-IDFå€¼ã€‚

![](https://static001.geekbang.org/resource/image/0d/43/0d2263fbc97beb520680382f08656b43.png?wh=468%2A71)  
åœ¨æˆ‘ä»¬è¿›è¡Œfit\_transformæ‹Ÿåˆæ¨¡å‹åï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ›´å¤šçš„TF-IDFå‘é‡å±æ€§ï¼Œæ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°è¯æ±‡çš„å¯¹åº”å…³ç³»ï¼ˆå­—å…¸ç±»å‹ï¼‰å’Œå‘é‡çš„IDFå€¼ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥è·å–è®¾ç½®çš„åœç”¨è¯stop\_wordsã€‚

![](https://static001.geekbang.org/resource/image/a4/6b/a42780a5bca0531e75a294b4e2fe356b.png?wh=468%2A128)  
ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾æˆ‘ä»¬æœ‰4ä¸ªæ–‡æ¡£ï¼š

æ–‡æ¡£1ï¼šthis is the bayes documentï¼›

æ–‡æ¡£2ï¼šthis is the second second documentï¼›

æ–‡æ¡£3ï¼šand the third oneï¼›

æ–‡æ¡£4ï¼šis this the documentã€‚

ç°åœ¨æƒ³è¦è®¡ç®—æ–‡æ¡£é‡Œéƒ½æœ‰å“ªäº›å•è¯ï¼Œè¿™äº›å•è¯åœ¨ä¸åŒæ–‡æ¡£ä¸­çš„TF-IDFå€¼æ˜¯å¤šå°‘å‘¢ï¼Ÿ

é¦–å…ˆæˆ‘ä»¬åˆ›å»ºTfidfVectorizerç±»ï¼š

```
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vec = TfidfVectorizer()
```

ç„¶åæˆ‘ä»¬åˆ›å»º4ä¸ªæ–‡æ¡£çš„åˆ—è¡¨documentsï¼Œå¹¶è®©åˆ›å»ºå¥½çš„tfidf\_vecå¯¹documentsè¿›è¡Œæ‹Ÿåˆï¼Œå¾—åˆ°TF-IDFçŸ©é˜µï¼š

```
documents = [
    'this is the bayes document',
    'this is the second second document',
    'and the third one',
    'is this the document'
]
tfidf_matrix = tfidf_vec.fit_transform(documents)
```

è¾“å‡ºæ–‡æ¡£ä¸­æ‰€æœ‰ä¸é‡å¤çš„è¯ï¼š

```
print('ä¸é‡å¤çš„è¯:', tfidf_vec.get_feature_names())
```

è¿è¡Œç»“æœ

```
ä¸é‡å¤çš„è¯: ['and', 'bayes', 'document', 'is', 'one', 'second', 'the', 'third', 'this']
```

è¾“å‡ºæ¯ä¸ªå•è¯å¯¹åº”çš„idå€¼ï¼š

```
print('æ¯ä¸ªå•è¯çš„ID:', tfidf_vec.vocabulary_)
```

è¿è¡Œç»“æœ

```
æ¯ä¸ªå•è¯çš„ID: {'this': 8, 'is': 3, 'the': 6, 'bayes': 1, 'document': 2, 'second': 5, 'and': 0, 'third': 7, 'one': 4}
```

è¾“å‡ºæ¯ä¸ªå•è¯åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­çš„TF-IDFå€¼ï¼Œå‘é‡é‡Œçš„é¡ºåºæ˜¯æŒ‰ç…§è¯è¯­çš„idé¡ºåºæ¥çš„ï¼š

```
print('æ¯ä¸ªå•è¯çš„tfidfå€¼:', tfidf_matrix.toarray())
```

è¿è¡Œç»“æœï¼š

```
æ¯ä¸ªå•è¯çš„tfidfå€¼: [[0.         0.63314609 0.40412895 0.40412895 0.         0.
  0.33040189 0.         0.40412895]
 [0.         0.         0.27230147 0.27230147 0.         0.85322574
  0.22262429 0.         0.27230147]
 [0.55280532 0.         0.         0.         0.55280532 0.
  0.28847675 0.55280532 0.        ]
 [0.         0.         0.52210862 0.52210862 0.         0.
  0.42685801 0.         0.52210862]]
```

## å¦‚ä½•å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç±»

å¦‚æœæˆ‘ä»¬è¦å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç±»ï¼Œæœ‰ä¸¤ä¸ªé‡è¦çš„é˜¶æ®µï¼š

![](https://static001.geekbang.org/resource/image/25/c3/257e01f173e8bc78b37b71b2358ff7c3.jpg?wh=2460%2A1167)

1. **åŸºäºåˆ†è¯çš„æ•°æ®å‡†å¤‡**ï¼ŒåŒ…æ‹¬åˆ†è¯ã€å•è¯æƒé‡è®¡ç®—ã€å»æ‰åœç”¨è¯ï¼›
2. **åº”ç”¨æœ´ç´ è´å¶æ–¯åˆ†ç±»è¿›è¡Œåˆ†ç±»**ï¼Œé¦–å…ˆé€šè¿‡è®­ç»ƒé›†å¾—åˆ°æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼Œç„¶åå°†åˆ†ç±»å™¨åº”ç”¨äºæµ‹è¯•é›†ï¼Œå¹¶ä¸å®é™…ç»“æœåšå¯¹æ¯”ï¼Œæœ€ç»ˆå¾—åˆ°æµ‹è¯•é›†çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚

ä¸‹é¢ï¼Œæˆ‘åˆ†åˆ«å¯¹è¿™äº›æ¨¡å—è¿›è¡Œä»‹ç»ã€‚

**æ¨¡å—1ï¼šå¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯**

åœ¨å‡†å¤‡é˜¶æ®µé‡Œï¼Œæœ€é‡è¦çš„å°±æ˜¯åˆ†è¯ã€‚é‚£ä¹ˆå¦‚æœç»™æ–‡æ¡£è¿›è¡Œåˆ†è¯å‘¢ï¼Ÿè‹±æ–‡æ–‡æ¡£å’Œä¸­æ–‡æ–‡æ¡£æ‰€ä½¿ç”¨çš„åˆ†è¯å·¥å…·ä¸åŒã€‚

åœ¨è‹±æ–‡æ–‡æ¡£ä¸­ï¼Œæœ€å¸¸ç”¨çš„æ˜¯NTLKåŒ…ã€‚NTLKåŒ…ä¸­åŒ…å«äº†è‹±æ–‡çš„åœç”¨è¯stop wordsã€åˆ†è¯å’Œæ ‡æ³¨æ–¹æ³•ã€‚

```
import nltk
word_list = nltk.word_tokenize(text) #åˆ†è¯
nltk.pos_tag(word_list) #æ ‡æ³¨å•è¯çš„è¯æ€§
```

åœ¨ä¸­æ–‡æ–‡æ¡£ä¸­ï¼Œæœ€å¸¸ç”¨çš„æ˜¯jiebaåŒ…ã€‚jiebaåŒ…ä¸­åŒ…å«äº†ä¸­æ–‡çš„åœç”¨è¯stop wordså’Œåˆ†è¯æ–¹æ³•ã€‚

```
import jieba
word_list = jieba.cut (text) #ä¸­æ–‡åˆ†è¯
```

**æ¨¡å—2ï¼šåŠ è½½åœç”¨è¯è¡¨**

æˆ‘ä»¬éœ€è¦è‡ªå·±è¯»å–åœç”¨è¯è¡¨æ–‡ä»¶ï¼Œä»ç½‘ä¸Šå¯ä»¥æ‰¾åˆ°ä¸­æ–‡å¸¸ç”¨çš„åœç”¨è¯ä¿å­˜åœ¨stop\_words.txtï¼Œç„¶ååˆ©ç”¨Pythonçš„æ–‡ä»¶è¯»å–å‡½æ•°è¯»å–æ–‡ä»¶ï¼Œä¿å­˜åœ¨stop\_wordsæ•°ç»„ä¸­ã€‚

```
stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]
```

**æ¨¡å—3ï¼šè®¡ç®—å•è¯çš„æƒé‡**

è¿™é‡Œæˆ‘ä»¬ç”¨åˆ°sklearné‡Œçš„TfidfVectorizerç±»ï¼Œä¸Šé¢æˆ‘ä»¬ä»‹ç»è¿‡å®ƒä½¿ç”¨çš„æ–¹æ³•ã€‚

ç›´æ¥åˆ›å»ºTfidfVectorizerç±»ï¼Œç„¶åä½¿ç”¨fit\_transformæ–¹æ³•è¿›è¡Œæ‹Ÿåˆï¼Œå¾—åˆ°TF-IDFç‰¹å¾ç©ºé—´featuresï¼Œä½ å¯ä»¥ç†è§£ä¸ºé€‰å‡ºæ¥çš„åˆ†è¯å°±æ˜¯ç‰¹å¾ã€‚æˆ‘ä»¬è®¡ç®—è¿™äº›ç‰¹å¾åœ¨æ–‡æ¡£ä¸Šçš„ç‰¹å¾å‘é‡ï¼Œå¾—åˆ°ç‰¹å¾ç©ºé—´featuresã€‚

```
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)
features = tf.fit_transform(train_contents)
```

è¿™é‡Œmax\_dfå‚æ•°ç”¨æ¥æè¿°å•è¯åœ¨æ–‡æ¡£ä¸­çš„æœ€é«˜å‡ºç°ç‡ã€‚å‡è®¾max\_df=0.5ï¼Œä»£è¡¨ä¸€ä¸ªå•è¯åœ¨50%çš„æ–‡æ¡£ä¸­éƒ½å‡ºç°è¿‡äº†ï¼Œé‚£ä¹ˆå®ƒåªæºå¸¦äº†éå¸¸å°‘çš„ä¿¡æ¯ï¼Œå› æ­¤å°±ä¸ä½œä¸ºåˆ†è¯ç»Ÿè®¡ã€‚

ä¸€èˆ¬å¾ˆå°‘è®¾ç½®min\_dfï¼Œå› ä¸ºmin\_dfé€šå¸¸éƒ½ä¼šå¾ˆå°ã€‚

**æ¨¡å—4ï¼šç”Ÿæˆæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨**

æˆ‘ä»¬å°†ç‰¹å¾è®­ç»ƒé›†çš„ç‰¹å¾ç©ºé—´train\_featuresï¼Œä»¥åŠè®­ç»ƒé›†å¯¹åº”çš„åˆ†ç±»train\_labelsä¼ é€’ç»™è´å¶æ–¯åˆ†ç±»å™¨clfï¼Œå®ƒä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªç¬¦åˆç‰¹å¾ç©ºé—´å’Œå¯¹åº”åˆ†ç±»çš„åˆ†ç±»å™¨ã€‚

è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯å¤šé¡¹å¼è´å¶æ–¯åˆ†ç±»å™¨ï¼Œå…¶ä¸­alphaä¸ºå¹³æ»‘å‚æ•°ã€‚ä¸ºä»€ä¹ˆè¦ä½¿ç”¨å¹³æ»‘å‘¢ï¼Ÿå› ä¸ºå¦‚æœä¸€ä¸ªå•è¯åœ¨è®­ç»ƒæ ·æœ¬ä¸­æ²¡æœ‰å‡ºç°ï¼Œè¿™ä¸ªå•è¯çš„æ¦‚ç‡å°±ä¼šè¢«è®¡ç®—ä¸º0ã€‚ä½†è®­ç»ƒé›†æ ·æœ¬åªæ˜¯æ•´ä½“çš„æŠ½æ ·æƒ…å†µï¼Œæˆ‘ä»¬ä¸èƒ½å› ä¸ºä¸€ä¸ªäº‹ä»¶æ²¡æœ‰è§‚å¯Ÿåˆ°ï¼Œå°±è®¤ä¸ºæ•´ä¸ªäº‹ä»¶çš„æ¦‚ç‡ä¸º0ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦åšå¹³æ»‘å¤„ç†ã€‚

å½“alpha=1æ—¶ï¼Œä½¿ç”¨çš„æ˜¯Laplaceå¹³æ»‘ã€‚Laplaceå¹³æ»‘å°±æ˜¯é‡‡ç”¨åŠ 1çš„æ–¹å¼ï¼Œæ¥ç»Ÿè®¡æ²¡æœ‰å‡ºç°è¿‡çš„å•è¯çš„æ¦‚ç‡ã€‚è¿™æ ·å½“è®­ç»ƒæ ·æœ¬å¾ˆå¤§çš„æ—¶å€™ï¼ŒåŠ 1å¾—åˆ°çš„æ¦‚ç‡å˜åŒ–å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œä¹ŸåŒæ—¶é¿å…äº†é›¶æ¦‚ç‡çš„é—®é¢˜ã€‚

å½“0&lt;alpha&lt;1æ—¶ï¼Œä½¿ç”¨çš„æ˜¯Lidstoneå¹³æ»‘ã€‚å¯¹äºLidstoneå¹³æ»‘æ¥è¯´ï¼Œalpha è¶Šå°ï¼Œè¿­ä»£æ¬¡æ•°è¶Šå¤šï¼Œç²¾åº¦è¶Šé«˜ã€‚æˆ‘ä»¬å¯ä»¥è®¾ç½®alphaä¸º0.001ã€‚

```
# å¤šé¡¹å¼è´å¶æ–¯åˆ†ç±»å™¨
from sklearn.naive_bayes import MultinomialNB  
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)
```

**æ¨¡å—5ï¼šä½¿ç”¨ç”Ÿæˆçš„åˆ†ç±»å™¨åšé¢„æµ‹**

é¦–å…ˆæˆ‘ä»¬éœ€è¦å¾—åˆ°æµ‹è¯•é›†çš„ç‰¹å¾çŸ©é˜µã€‚

æ–¹æ³•æ˜¯ç”¨è®­ç»ƒé›†çš„åˆ†è¯åˆ›å»ºä¸€ä¸ªTfidfVectorizerç±»ï¼Œä½¿ç”¨åŒæ ·çš„stop\_wordså’Œmax\_dfï¼Œç„¶åç”¨è¿™ä¸ªTfidfVectorizerç±»å¯¹æµ‹è¯•é›†çš„å†…å®¹è¿›è¡Œfit\_transformæ‹Ÿåˆï¼Œå¾—åˆ°æµ‹è¯•é›†çš„ç‰¹å¾çŸ©é˜µtest\_featuresã€‚

```
test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)
test_features=test_tf.fit_transform(test_contents)
```

ç„¶åæˆ‘ä»¬ç”¨è®­ç»ƒå¥½çš„åˆ†ç±»å™¨å¯¹æ–°æ•°æ®åšé¢„æµ‹ã€‚

æ–¹æ³•æ˜¯ä½¿ç”¨predictå‡½æ•°ï¼Œä¼ å…¥æµ‹è¯•é›†çš„ç‰¹å¾çŸ©é˜µtest\_featuresï¼Œå¾—åˆ°åˆ†ç±»ç»“æœpredicted\_labelsã€‚predictå‡½æ•°åšçš„å·¥ä½œå°±æ˜¯æ±‚è§£æ‰€æœ‰åéªŒæ¦‚ç‡å¹¶æ‰¾å‡ºæœ€å¤§çš„é‚£ä¸ªã€‚

```
predicted_labels=clf.predict(test_features)
```

**æ¨¡å—6ï¼šè®¡ç®—å‡†ç¡®ç‡**

è®¡ç®—å‡†ç¡®ç‡å®é™…ä¸Šæ˜¯å¯¹åˆ†ç±»æ¨¡å‹çš„è¯„ä¼°ã€‚æˆ‘ä»¬å¯ä»¥è°ƒç”¨sklearnä¸­çš„metricsåŒ…ï¼Œåœ¨metricsä¸­æä¾›äº†accuracy\_scoreå‡½æ•°ï¼Œæ–¹ä¾¿æˆ‘ä»¬å¯¹å®é™…ç»“æœå’Œé¢„æµ‹çš„ç»“æœåšå¯¹æ¯”ï¼Œç»™å‡ºæ¨¡å‹çš„å‡†ç¡®ç‡ã€‚

ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```
from sklearn import metrics
print metrics.accuracy_score(test_labels, predicted_labels)
```

## æ•°æ®æŒ–æ˜ç¥å™¨sklearn

ä»æ•°æ®æŒ–æ˜çš„æµç¨‹æ¥çœ‹ï¼Œä¸€èˆ¬åŒ…æ‹¬äº†è·å–æ•°æ®ã€æ•°æ®æ¸…æ´—ã€æ¨¡å‹è®­ç»ƒã€æ¨¡å‹è¯„ä¼°å’Œæ¨¡å‹éƒ¨ç½²è¿™å‡ ä¸ªè¿‡ç¨‹ã€‚

sklearnä¸­åŒ…å«äº†å¤§é‡çš„æ•°æ®æŒ–æ˜ç®—æ³•ï¼Œæ¯”å¦‚ä¸‰ç§æœ´ç´ è´å¶æ–¯ç®—æ³•ï¼Œæˆ‘ä»¬åªéœ€è¦äº†è§£ä¸åŒç®—æ³•çš„é€‚ç”¨æ¡ä»¶ï¼Œä»¥åŠåˆ›å»ºæ—¶æ‰€éœ€çš„å‚æ•°ï¼Œå°±å¯ä»¥ç”¨æ¨¡å‹å¸®æˆ‘ä»¬è¿›è¡Œè®­ç»ƒã€‚åœ¨æ¨¡å‹è¯„ä¼°ä¸­ï¼Œsklearnæä¾›äº†metricsåŒ…ï¼Œå¸®æˆ‘ä»¬å¯¹é¢„æµ‹ç»“æœä¸å®é™…ç»“æœè¿›è¡Œè¯„ä¼°ã€‚

åœ¨æ–‡æ¡£åˆ†ç±»çš„é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ–‡æ¡£çš„ç‰¹ç‚¹ï¼Œç»™å‡ºäº†åŸºäºåˆ†è¯çš„å‡†å¤‡æµç¨‹ã€‚ä¸€èˆ¬æ¥è¯´NTLKåŒ…é€‚ç”¨äºè‹±æ–‡æ–‡æ¡£ï¼Œè€Œjiebaé€‚ç”¨äºä¸­æ–‡æ–‡æ¡£ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®æ–‡æ¡£é€‰æ‹©ä¸åŒçš„åŒ…ï¼Œå¯¹æ–‡æ¡£æå–åˆ†è¯ã€‚è¿™äº›åˆ†è¯å°±æ˜¯è´å¶æ–¯åˆ†ç±»ä¸­æœ€é‡è¦çš„ç‰¹å¾å±æ€§ã€‚åŸºäºè¿™äº›åˆ†è¯ï¼Œæˆ‘ä»¬å¾—åˆ°åˆ†è¯çš„æƒé‡ï¼Œå³ç‰¹å¾çŸ©é˜µã€‚

é€šè¿‡ç‰¹å¾çŸ©é˜µä¸åˆ†ç±»ç»“æœï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ›å»ºå‡ºæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼Œç„¶åç”¨åˆ†ç±»å™¨è¿›è¡Œé¢„æµ‹ï¼Œæœ€åé¢„æµ‹ç»“æœä¸å®é™…ç»“æœåšå¯¹æ¯”å³å¯ä»¥å¾—åˆ°åˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚

![](https://static001.geekbang.org/resource/image/2e/6e/2e2962ddb7e85a71e0cecb9c6d13306e.png?wh=874%2A286)

## ç»ƒä¹ é¢˜

æˆ‘å·²ç»è®²äº†ä¸­æ–‡æ–‡æ¡£åˆ†ç±»ä¸­çš„6ä¸ªå…³é”®çš„æ¨¡å—ï¼Œæœ€åï¼Œæˆ‘ç»™ä½ ç•™ä¸€é“å¯¹ä¸­æ–‡æ–‡æ¡£åˆ†ç±»çš„ç»ƒä¹ é¢˜å§ã€‚

æˆ‘å°†ä¸­æ–‡æ–‡æ¡£æ•°æ®é›†ä¸Šä¼ åˆ°äº†GitHubä¸Šï¼Œ[ç‚¹å‡»è¿™é‡Œä¸‹è½½](https://github.com/cystanford/text_classification)ã€‚

æ•°æ®è¯´æ˜ï¼š

1. æ–‡æ¡£å…±æœ‰4ç§ç±»å‹ï¼šå¥³æ€§ã€ä½“è‚²ã€æ–‡å­¦ã€æ ¡å›­ï¼›

<!--THE END-->

![](https://static001.geekbang.org/resource/image/67/28/67abc1783f7c4e7cd69194fafc514328.png?wh=585%2A120)

1. è®­ç»ƒé›†æ”¾åˆ°trainæ–‡ä»¶å¤¹é‡Œï¼Œæµ‹è¯•é›†æ”¾åˆ°testæ–‡ä»¶å¤¹é‡Œï¼Œåœç”¨è¯æ”¾åˆ°stopæ–‡ä»¶å¤¹é‡Œã€‚

![](https://static001.geekbang.org/resource/image/0c/0f/0c374e3501cc28a24687bc030733050f.png?wh=580%2A97)  
è¯·ä½¿ç”¨æœ´ç´ è´å¶æ–¯åˆ†ç±»å¯¹è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯¹æµ‹è¯•é›†è¿›è¡ŒéªŒè¯ï¼Œå¹¶ç»™å‡ºæµ‹è¯•é›†çš„å‡†ç¡®ç‡ã€‚

æœ€åä½ ä¸å¦¨æ€è€ƒä¸€ä¸‹ï¼Œå‡è®¾æˆ‘ä»¬è¦åˆ¤æ–­ä¸€ä¸ªäººçš„æ€§åˆ«ï¼Œæ˜¯é€šè¿‡èº«é«˜ã€ä½“é‡ã€é‹ç ã€å¤–è²Œç­‰å±æ€§è¿›è¡Œåˆ¤æ–­çš„ï¼Œå¦‚æœæˆ‘ä»¬ç”¨æœ´ç´ è´å¶æ–¯åšåˆ†ç±»ï¼Œé€‚åˆä½¿ç”¨å“ªç§æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼Ÿåœç”¨è¯çš„ä½œç”¨åˆæ˜¯ä»€ä¹ˆï¼Ÿ

æ¬¢è¿ä½ åœ¨è¯„è®ºåŒºè¿›è¡Œç•™è¨€ï¼Œä¸æˆ‘åˆ†äº«ä½ çš„ç­”æ¡ˆã€‚ä¹Ÿæ¬¢è¿ç‚¹å‡»â€œè¯·æœ‹å‹è¯»â€ï¼ŒæŠŠè¿™ç¯‡æ–‡ç« åˆ†äº«ç»™ä½ çš„æœ‹å‹æˆ–è€…åŒäº‹ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ15ï¼‰</strong></div><ul>
<li><span>åŒ—æ–¹</span> ğŸ‘ï¼ˆ35ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>#!&#47;usr&#47;bin&#47;env python
# -*- coding:utf8 -*-
# __author__ = &#39;åŒ—æ–¹å§†Q&#39;
# __datetime__ = 2019&#47;2&#47;14 14:04

import os
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

LABEL_MAP = {&#39;ä½“è‚²&#39;: 0, &#39;å¥³æ€§&#39;: 1, &#39;æ–‡å­¦&#39;: 2, &#39;æ ¡å›­&#39;: 3}
# åŠ è½½åœç”¨è¯
with open(&#39;.&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;rb&#39;) as f:
    STOP_WORDS = [line.strip() for line in f.readlines()]


def load_data(base_path):
    &quot;&quot;&quot;
    :param base_path: åŸºç¡€è·¯å¾„
    :return: åˆ†è¯åˆ—è¡¨ï¼Œæ ‡ç­¾åˆ—è¡¨
    &quot;&quot;&quot;
    documents = []
    labels = []

    for root, dirs, files in os.walk(base_path):    # å¾ªç¯æ‰€æœ‰æ–‡ä»¶å¹¶è¿›è¡Œåˆ†è¯æ‰“æ ‡
        for file in files:
            label = root.split(&#39;\\&#39;)[-1]        # å› ä¸ºwindowsä¸Šè·¯å¾„ç¬¦å·è‡ªåŠ¨è½¬æˆ\äº†ï¼Œæ‰€ä»¥è¦è½¬ä¹‰ä¸‹
            labels.append(label)
            filename = os.path.join(root, file)
            with open(filename, &#39;rb&#39;) as f:     # å› ä¸ºå­—ç¬¦é›†é—®é¢˜å› æ­¤ç›´æ¥ç”¨äºŒè¿›åˆ¶æ–¹å¼è¯»å–
                content = f.read()
                word_list = list(jieba.cut(content))
                words = [wl for wl in word_list]
                documents.append(&#39; &#39;.join(words))
    return documents, labels


def train_fun(td, tl, testd, testl):
    &quot;&quot;&quot;
    æ„é€ æ¨¡å‹å¹¶è®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡ï¼Œå­—æ•°é™åˆ¶å˜é‡åç®€å†™
    :param td: è®­ç»ƒé›†æ•°æ®
    :param tl: è®­ç»ƒé›†æ ‡ç­¾
    :param testd: æµ‹è¯•é›†æ•°æ®
    :param testl: æµ‹è¯•é›†æ ‡ç­¾
    :return: æµ‹è¯•é›†å‡†ç¡®ç‡
    &quot;&quot;&quot;
    # è®¡ç®—çŸ©é˜µ
    tt = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)
    tf = tt.fit_transform(td)
    # è®­ç»ƒæ¨¡å‹
    clf = MultinomialNB(alpha=0.001).fit(tf, tl)
    # æ¨¡å‹é¢„æµ‹
    test_tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tt.vocabulary_)
    test_features = test_tf.fit_transform(testd)
    predicted_labels = clf.predict(test_features)
    # è·å–ç»“æœ
    x = metrics.accuracy_score(testl, predicted_labels)
    return x


# text classificationä¸ä»£ç åŒç›®å½•ä¸‹
train_documents, train_labels = load_data(&#39;.&#47;text classification&#47;train&#39;)
test_documents, test_labels = load_data(&#39;.&#47;text classification&#47;test&#39;)
x = train_fun(train_documents, train_labels, test_documents, test_labels)
print(x)</p>2019-02-14</li><br/><li><span>szm</span> ğŸ‘ï¼ˆ37ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<p>éœ€è¦å®Œæ•´ä»£ç ï¼Œä¸ç„¶çœ‹ä¸æ˜ç™½ï¼</p>2019-01-30</li><br/><li><span>Python</span> ğŸ‘ï¼ˆ18ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œèƒ½ä¸èƒ½åœ¨ç­”ç–‘çš„æ—¶å€™ç»™è¿™é“é¢˜çš„å®Œæ•´ä»£ç çœ‹çœ‹</p>2019-01-30</li><br/><li><span>å§œæˆˆ</span> ğŸ‘ï¼ˆ16ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>çœ‹è¿‡å¾ˆå¤šæœ´ç´ è´å¶æ–¯åŸç†å’Œåˆ†ç±»çš„è®²è§£æ–‡ç« ï¼Œå¾ˆå°‘èƒ½åƒå‰è¾ˆè¿™æ ·æ—¢æœ‰ç†è®ºï¼Œåˆæœ‰å®æˆ˜çš„è®²è§£ï¼Œè®©å¤§å®¶æ—¢äº†è§£äº†ç†è®ºçŸ¥è¯†ï¼Œåˆæœ‰ç›¸åº”å®é™…çš„æ“ä½œç»éªŒå¯å­¦ï¼ŒçœŸçš„å¥½æ£’ï¼Œè¿™ä¸ªä¸“æ ï¼Œå¿…é¡»å¤šå¤šç‚¹èµï¼Œä¸ºè€å¸ˆåŠ æ²¹ï¼ï¼ï¼</p>2019-01-30</li><br/><li><span>Geek_z0wqck</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>https:&#47;&#47;github.com&#47;yourSprite&#47;AnalysisExcercise&#47;tree&#47;master&#47;%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB</p>2019-02-12</li><br/><li><span>å‡ ä½•</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è€å¸ˆï¼Œå¼±å¼±çš„è¯´ä¸€å¥ï¼Œä»£ç æ„Ÿè§‰èƒ½çœ‹æ˜ç™½ï¼Œä½†æ˜¯ä¸æ˜ç™½çš„æ˜¯æ¨¡å‹æ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼Œ æ¯”å¦‚ä¸Šä¸€èŠ‚å’Œæœ¬èŠ‚ï¼Œéƒ½æ˜¯åªçŸ¥é“äº†å‡†ç¡®ç‡ï¼Œä½†æ˜¯å¯¹äºæœ‰æ–°çš„è¦å¤„ç†çš„æ•°æ®ï¼Œå¦‚ä½•åšï¼Œæ€ä¹ˆåšå¥½æ€»æ˜¯æ„Ÿè§‰å·®ä¸€ç‚¹ç‚¹ä¸œè¥¿ã€‚</p>2019-09-08</li><br/><li><span>Jack</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>#!&#47;usr&#47;bin&#47;env python
# coding: utf-8

import os
import jieba
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
# 1. åŠ è½½æ•°æ®
# åŠ è½½åœç”¨è¯è¡¨
l_stopWords = set()
with open(&#39;.&#47;text_classification&#47;text_classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;) as l_f:
    for l_line in l_f:
        l_stopWords.add(l_line.strip())

l_labelMap = {&#39;ä½“è‚²&#39;: 0, &#39;å¥³æ€§&#39;: 1, &#39;æ–‡å­¦&#39;: 2, &#39;æ ¡å›­&#39;: 3}
# åŠ è½½è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
def LoadData(filepath):
    l_documents = []
    l_labels = []
    for root, dirs, files in os.walk(filepath):
        for l_file in files:
            l_label = root.split(&#39;&#47;&#39;)[-1]
            l_filename = os.path.join(root, l_file)
            
            with open(l_filename, &#39;r&#39;) as l_f:
                l_content = l_f.read()
                l_wordlist = list(jieba.cut(l_content))
                l_words = [item for item in l_wordlist if item not in l_stopWords]
                l_documents.append(&#39; &#39;.join(l_words))
                l_labels.append(l_labelMap[l_label])
                
    return l_documents, l_labels

l_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;train&#39;)
l_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;test&#39;)

# # 2. è®¡ç®—æƒé‡çŸ©é˜µ
l_tfidfVec = TfidfVectorizer(max_df=0.5)
l_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)

# for item in l_tfidfVec.get_feature_names():
#     print item
# print l_tfidfVec.get_feature_names()
# print l_tfidfVec.vocabulary_
print l_tfidfMatrix.toarray().shape

# # 3. æœ´ç´ è´å¶æ–¯æ¨¡å‹
# ## 3.1 æ¨¡å‹è®­ç»ƒ
l_clf = MultinomialNB(alpha=0.001)
l_clf.fit(l_tfidfMatrix, l_trainLabels)

# ## 3.2 æ¨¡å‹é¢„æµ‹
l_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)
l_testFeature = l_testTfidf.fit_transform(l_testDocuments)
l_hats = l_clf.predict(l_testFeature)

# ## 3.3 æ¨¡å‹è¯„ä¼°
from sklearn.metrics import accuracy_score
print accuracy_score(l_hats, l_testLabels)</p>2019-02-14</li><br/><li><span>Jasmine</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>è€å¸ˆï¼Œæˆ‘æƒ³è¯·æ•™ä¸€ä¸‹ï¼Œè®¡ç®—å•è¯æƒé‡æ—¶ï¼Œä¸ºä»€ä¹ˆtrain_featuresç”¨çš„fit_transformæ–¹æ³•ï¼Œè€Œtest_featureç”¨çš„æ˜¯transform</p>2020-11-16</li><br/><li><span>æ»¢</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<p>æœ€åé¢çš„ä»£ç å¤ªä¹±ï¼Œå¾ˆå¤šéƒ½ä¸çŸ¥é“ä»å“ªé‡Œæ¥çš„ï¼Œæ— æ³•é¡ºç€çœ‹ä¸‹å»~~~</p>2019-04-17</li><br/><li><span>Geek_hve78z</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p># -*- coding:utf8 -*-
# ç³»ç»Ÿï¼šmac 

# 1. åŠ è½½æ•°æ®
# åŠ è½½åœç”¨è¯è¡¨

l_stopWords = [line.strip() for line in open(&#39;.&#47;text_classification-master&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).readlines()]  
   
l_labelMap = {&#39;ä½“è‚²&#39;: 0, &#39;å¥³æ€§&#39;: 1, &#39;æ–‡å­¦&#39;: 2, &#39;æ ¡å›­&#39;: 3}
# åŠ è½½è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
def LoadData(filepath):
    l_documents = []
    l_labels = []
    
    for root, dirs, files in os.walk(filepath):
        for l_file in files:
            if l_file==&#39;.DS_Store&#39;:
                continue
            l_label = root.split(&#39;&#47;&#39;)[-1]
            l_filename = os.path.join(root, l_file)
            
            with open(l_filename, &#39;r&#39;,encoding=&#39;gbk&#39;) as l_f:
                try:
                    l_content = l_f.read()
                except Exception as err:
                    print(err)
                    print(l_filename)
                    continue
                generator = jieba.cut(l_content)
                words = &#39; &#39;.join(generator)
                l_wordlist=words.split(&#39; &#39;)
                l_words = [item for item in l_wordlist if item not in l_stopWords]
                l_documents.append(&#39; &#39;.join(l_words))
                l_labels.append(l_labelMap[l_label])
                
    return l_documents, l_labels

l_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;train&#39;)
l_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;test&#39;)

# # 2. è®¡ç®—æƒé‡çŸ©é˜µ
l_tfidfVec = TfidfVectorizer(max_df=0.5)
l_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)

print (l_tfidfMatrix.toarray().shape)

# # 3. æœ´ç´ è´å¶æ–¯æ¨¡å‹
# ## 3.1 æ¨¡å‹è®­ç»ƒ
l_clf = MultinomialNB(alpha=0.001)
l_clf.fit(l_tfidfMatrix, l_trainLabels)

# ## 3.2 æ¨¡å‹é¢„æµ‹
l_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)
l_testFeature = l_testTfidf.fit_transform(l_testDocuments)
l_hats = l_clf.predict(l_testFeature)

# ## 3.3 æ¨¡å‹è¯„ä¼°
from sklearn.metrics import accuracy_score
print (accuracy_score(l_hats, l_testLabels))</p>2019-04-05</li><br/><li><span>wzhan366</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>å»ºè®® å¤§å®¶å…ˆåšè‹±æ–‡ç‰ˆæœ¬ï¼Œå› ä¸ºä¸­æ–‡çš„unicode encodeå’Œdecodeä¸æ˜¯å¾ˆå¥½å¼„ï¼Œä¸åˆ©äºä¸­é—´æ­¥éª¤çš„å¯è§†åŒ–ã€‚å¦‚æœå¯¹ä»£ç æœ‰ç–‘æƒ‘ï¼Œå¯ä»¥è¯•è¯•è¿™ä¸ªpipelineï¼Œ sklearn çš„ã€‚ ä¸è¿‡ï¼Œè¿™ä¸ªæ²¡æœ‰ç”¨NTLKã€‚</p>2019-02-06</li><br/><li><span>lemonlxn</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æ­¤å¤„æ²¡æœ‰å¯¹ sklearn çš„tfidfè¿›è¡Œè®²è§£ï¼Œä¸ºå®Œå–„ç»“æœï¼Œæ€»ç»“å¦‚ä¸‹ï¼š

ä¼ ç»Ÿ TF_IDF = TF * IDF

	TF  = è¯¥æ–‡æ¡£æŸå•è¯å‡ºç°æ¬¡æ•° &#47; è¯¥æ–‡æ¡£çš„æ€»å•è¯æ•°

	IDF = loge(æ–‡æ¡£æ€»æ•° &#47; è¯¥å•è¯å‡ºç°çš„æ–‡æ¡£æ•° + 1)


sklearn TF_IDF = TF * IDF

	TF  = è¯¥æ–‡æ¡£æŸå•è¯å‡ºç°æ¬¡æ•°
        IDF = loge((1 + n) &#47; (1 + dn)) + 1
       
        å…¶ä¸­ n  ä¸ºè®­ç»ƒé›†æ–‡æ¡£æ•°
               dn ä¸ºæµ‹è¯•é›†å‡ºç°è¯¥å•è¯çš„æ–‡æ¡£æ•°
        
        å¦‚æœnorm=None åˆ™ç»“æœä¸º 
          X.toarray()
        
        å¦‚æœ norm=&#39;l2&#39; åˆ™ç»“æœä¸ºï¼š
          X.toarray() &#47; np.sum(X.toarray() ** 2) ** 0.5
        
        å¦‚æœ norm=&#39;l1&#39;åˆ™ç»“æœä¸º
          X.toarray() &#47; np.sum(X.toarray())</p>2020-09-25</li><br/><li><span>é»„äº‰è¾‰</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è¿™é‡Œç™¾æ€ä¸å¾—è§£ï¼Œ
è®­ç»ƒé›†ä¸­ï¼Œï¼ˆ4ä¸ªæ–‡ä»¶çš„è¯æ··åœ¨ä¸€èµ·ï¼‰æ¯ä¸ªè¯çš„ TF-IDFå€¼å’Œåˆ†ç±»éƒ½æ··åœ¨ä¸€èµ·äº†ï¼Œæ˜¯å¦‚ä½•å®ç°åˆ†ç±»å™¨çš„å‘¢ï¼Ÿ
</p>2020-09-10</li><br/><li><span>Viola</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>æœ‰å¤§é‡ä»£ç å®ç°çš„ ï¼Œèƒ½å¦å°†githubåœ°å€è´´ä¸€ä¸‹</p>2019-01-31</li><br/><li><span>è®¸æ„¿å­—èŠ‚ä¸Šå²¸å†²å†²å†²</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<p>è€å¸ˆï¼Œæ˜¯ä¸æ˜¯å¯ä»¥ç†è§£ä¸ºfitå‡½æ•°è®¡ç®—äº†idfçš„å€¼å’Œç‰¹å¾å•è¯ï¼Œtransformå‡½æ•°è®¡ç®—äº†tfçš„å€¼å¹¶ä¸idfç›¸ä¹˜ç®—å‡ºäº†çœŸæ­£çš„tf-idfã€‚æ‰€ä»¥train_featuresè°ƒç”¨äº†fit_transformè®¡ç®—å‡ºäº†idfï¼Œtest_featuresè°ƒç”¨äº†transformåˆ™é€šè¿‡test_featuresè®¡ç®—å‡ºçš„TFå€¼ä¹˜ä»¥äº†ä¹‹å‰é€šè¿‡train_featuresç®—å‡ºçš„idfå€¼å¾—å‡ºæ¥æœ€ç»ˆçš„TF-IDFçŸ©é˜µï¼Ÿ</p>2021-04-02</li><br/>
</ul>