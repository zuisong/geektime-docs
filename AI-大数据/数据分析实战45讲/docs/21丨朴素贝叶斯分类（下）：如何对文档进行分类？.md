æˆ‘ä»¬ä¸Šä¸€èŠ‚è®²äº†æœ´ç´ è´å¶æ–¯çš„å·¥ä½œåŸç†ï¼Œä»Šå¤©æˆ‘ä»¬æ¥è®²ä¸‹è¿™äº›åŸç†æ˜¯å¦‚ä½•æŒ‡å¯¼å®é™…ä¸šåŠ¡çš„ã€‚

æœ´ç´ è´å¶æ–¯åˆ†ç±»æœ€é€‚åˆçš„åœºæ™¯å°±æ˜¯æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œåƒåœ¾é‚®ä»¶è¯†åˆ«ã€‚å…¶ä¸­æƒ…æ„Ÿåˆ†æå’Œåƒåœ¾é‚®ä»¶è¯†åˆ«éƒ½æ˜¯é€šè¿‡æ–‡æœ¬æ¥è¿›è¡Œåˆ¤æ–­ã€‚ä»è¿™é‡Œä½ èƒ½çœ‹å‡ºæ¥ï¼Œè¿™ä¸‰ä¸ªåœºæ™¯æœ¬è´¨ä¸Šéƒ½æ˜¯æ–‡æœ¬åˆ†ç±»ï¼Œè¿™ä¹Ÿæ˜¯æœ´ç´ è´å¶æ–¯æœ€æ“…é•¿çš„åœ°æ–¹ã€‚æ‰€ä»¥æœ´ç´ è´å¶æ–¯ä¹Ÿå¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†NLPçš„å·¥å…·ã€‚

ä»Šå¤©æˆ‘å¸¦ä½ ä¸€èµ·ä½¿ç”¨æœ´ç´ è´å¶æ–¯åšä¸‹æ–‡æ¡£åˆ†ç±»çš„é¡¹ç›®ï¼Œæœ€é‡è¦çš„å·¥å…·å°±æ˜¯sklearnè¿™ä¸ªæœºå™¨å­¦ä¹ ç¥å™¨ã€‚

## sklearnæœºå™¨å­¦ä¹ åŒ…

sklearnçš„å…¨ç§°å«Scikit-learnï¼Œå®ƒç»™æˆ‘ä»¬æä¾›äº†3ä¸ªæœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•ï¼Œåˆ†åˆ«æ˜¯é«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼ˆGaussianNBï¼‰ã€å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ï¼ˆMultinomialNBï¼‰å’Œä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼ˆBernoulliNBï¼‰ã€‚

è¿™ä¸‰ç§ç®—æ³•é€‚åˆåº”ç”¨åœ¨ä¸åŒçš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥æ ¹æ®ç‰¹å¾å˜é‡çš„ä¸åŒé€‰æ‹©ä¸åŒçš„ç®—æ³•ï¼š

**é«˜æ–¯æœ´ç´ è´å¶æ–¯**ï¼šç‰¹å¾å˜é‡æ˜¯è¿ç»­å˜é‡ï¼Œç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œæ¯”å¦‚è¯´äººçš„èº«é«˜ï¼Œç‰©ä½“çš„é•¿åº¦ã€‚

**å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯**ï¼šç‰¹å¾å˜é‡æ˜¯ç¦»æ•£å˜é‡ï¼Œç¬¦åˆå¤šé¡¹åˆ†å¸ƒï¼Œåœ¨æ–‡æ¡£åˆ†ç±»ä¸­ç‰¹å¾å˜é‡ä½“ç°åœ¨ä¸€ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°ï¼Œæˆ–è€…æ˜¯å•è¯çš„TF-IDFå€¼ç­‰ã€‚

**ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯**ï¼šç‰¹å¾å˜é‡æ˜¯å¸ƒå°”å˜é‡ï¼Œç¬¦åˆ0/1åˆ†å¸ƒï¼Œåœ¨æ–‡æ¡£åˆ†ç±»ä¸­ç‰¹å¾æ˜¯å•è¯æ˜¯å¦å‡ºç°ã€‚
<div><strong>ç²¾é€‰ç•™è¨€ï¼ˆ30ï¼‰</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/02/82/abed70a0.jpg" width="30px"><span>åŒ—æ–¹</span> ğŸ‘ï¼ˆ35ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>#!&#47;usr&#47;bin&#47;env python
# -*- coding:utf8 -*-
# __author__ = &#39;åŒ—æ–¹å§†Q&#39;
# __datetime__ = 2019&#47;2&#47;14 14:04

import os
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

LABEL_MAP = {&#39;ä½“è‚²&#39;: 0, &#39;å¥³æ€§&#39;: 1, &#39;æ–‡å­¦&#39;: 2, &#39;æ ¡å›­&#39;: 3}
# åŠ è½½åœç”¨è¯
with open(&#39;.&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;rb&#39;) as f:
    STOP_WORDS = [line.strip() for line in f.readlines()]


def load_data(base_path):
    &quot;&quot;&quot;
    :param base_path: åŸºç¡€è·¯å¾„
    :return: åˆ†è¯åˆ—è¡¨ï¼Œæ ‡ç­¾åˆ—è¡¨
    &quot;&quot;&quot;
    documents = []
    labels = []

    for root, dirs, files in os.walk(base_path):    # å¾ªç¯æ‰€æœ‰æ–‡ä»¶å¹¶è¿›è¡Œåˆ†è¯æ‰“æ ‡
        for file in files:
            label = root.split(&#39;\\&#39;)[-1]        # å› ä¸ºwindowsä¸Šè·¯å¾„ç¬¦å·è‡ªåŠ¨è½¬æˆ\äº†ï¼Œæ‰€ä»¥è¦è½¬ä¹‰ä¸‹
            labels.append(label)
            filename = os.path.join(root, file)
            with open(filename, &#39;rb&#39;) as f:     # å› ä¸ºå­—ç¬¦é›†é—®é¢˜å› æ­¤ç›´æ¥ç”¨äºŒè¿›åˆ¶æ–¹å¼è¯»å–
                content = f.read()
                word_list = list(jieba.cut(content))
                words = [wl for wl in word_list]
                documents.append(&#39; &#39;.join(words))
    return documents, labels


def train_fun(td, tl, testd, testl):
    &quot;&quot;&quot;
    æ„é€ æ¨¡å‹å¹¶è®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡ï¼Œå­—æ•°é™åˆ¶å˜é‡åç®€å†™
    :param td: è®­ç»ƒé›†æ•°æ®
    :param tl: è®­ç»ƒé›†æ ‡ç­¾
    :param testd: æµ‹è¯•é›†æ•°æ®
    :param testl: æµ‹è¯•é›†æ ‡ç­¾
    :return: æµ‹è¯•é›†å‡†ç¡®ç‡
    &quot;&quot;&quot;
    # è®¡ç®—çŸ©é˜µ
    tt = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)
    tf = tt.fit_transform(td)
    # è®­ç»ƒæ¨¡å‹
    clf = MultinomialNB(alpha=0.001).fit(tf, tl)
    # æ¨¡å‹é¢„æµ‹
    test_tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tt.vocabulary_)
    test_features = test_tf.fit_transform(testd)
    predicted_labels = clf.predict(test_features)
    # è·å–ç»“æœ
    x = metrics.accuracy_score(testl, predicted_labels)
    return x


# text classificationä¸ä»£ç åŒç›®å½•ä¸‹
train_documents, train_labels = load_data(&#39;.&#47;text classification&#47;train&#39;)
test_documents, test_labels = load_data(&#39;.&#47;text classification&#47;test&#39;)
x = train_fun(train_documents, train_labels, test_documents, test_labels)
print(x)</div>2019-02-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/a3/f9/9180d6d1.jpg" width="30px"><span>szm</span> ğŸ‘ï¼ˆ37ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>éœ€è¦å®Œæ•´ä»£ç ï¼Œä¸ç„¶çœ‹ä¸æ˜ç™½ï¼</div>2019-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg" width="30px"><span>Python</span> ğŸ‘ï¼ˆ18ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼Œèƒ½ä¸èƒ½åœ¨ç­”ç–‘çš„æ—¶å€™ç»™è¿™é“é¢˜çš„å®Œæ•´ä»£ç çœ‹çœ‹</div>2019-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/28/9c/73e76b19.jpg" width="30px"><span>å§œæˆˆ</span> ğŸ‘ï¼ˆ16ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>çœ‹è¿‡å¾ˆå¤šæœ´ç´ è´å¶æ–¯åŸç†å’Œåˆ†ç±»çš„è®²è§£æ–‡ç« ï¼Œå¾ˆå°‘èƒ½åƒå‰è¾ˆè¿™æ ·æ—¢æœ‰ç†è®ºï¼Œåˆæœ‰å®æˆ˜çš„è®²è§£ï¼Œè®©å¤§å®¶æ—¢äº†è§£äº†ç†è®ºçŸ¥è¯†ï¼Œåˆæœ‰ç›¸åº”å®é™…çš„æ“ä½œç»éªŒå¯å­¦ï¼ŒçœŸçš„å¥½æ£’ï¼Œè¿™ä¸ªä¸“æ ï¼Œå¿…é¡»å¤šå¤šç‚¹èµï¼Œä¸ºè€å¸ˆåŠ æ²¹ï¼ï¼ï¼</div>2019-01-30</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIlShPdVFbIaUu0wtcuSrlkG9r5zBedPuuN4Pyichof0QnMvr4J0G4kykyA4cgrlYibZ6wZ6NJNevFQ/132" width="30px"><span>Geek_z0wqck</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>https:&#47;&#47;github.com&#47;yourSprite&#47;AnalysisExcercise&#47;tree&#47;master&#47;%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB</div>2019-02-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/ee/b8/da245945.jpg" width="30px"><span>å‡ ä½•</span> ğŸ‘ï¼ˆ4ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>è€å¸ˆï¼Œå¼±å¼±çš„è¯´ä¸€å¥ï¼Œä»£ç æ„Ÿè§‰èƒ½çœ‹æ˜ç™½ï¼Œä½†æ˜¯ä¸æ˜ç™½çš„æ˜¯æ¨¡å‹æ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼Œ æ¯”å¦‚ä¸Šä¸€èŠ‚å’Œæœ¬èŠ‚ï¼Œéƒ½æ˜¯åªçŸ¥é“äº†å‡†ç¡®ç‡ï¼Œä½†æ˜¯å¯¹äºæœ‰æ–°çš„è¦å¤„ç†çš„æ•°æ®ï¼Œå¦‚ä½•åšï¼Œæ€ä¹ˆåšå¥½æ€»æ˜¯æ„Ÿè§‰å·®ä¸€ç‚¹ç‚¹ä¸œè¥¿ã€‚</div>2019-09-08</li><br/><li><img src="" width="30px"><span>Jack</span> ğŸ‘ï¼ˆ3ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>#!&#47;usr&#47;bin&#47;env python
# coding: utf-8

import os
import jieba
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
# 1. åŠ è½½æ•°æ®
# åŠ è½½åœç”¨è¯è¡¨
l_stopWords = set()
with open(&#39;.&#47;text_classification&#47;text_classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;) as l_f:
    for l_line in l_f:
        l_stopWords.add(l_line.strip())

l_labelMap = {&#39;ä½“è‚²&#39;: 0, &#39;å¥³æ€§&#39;: 1, &#39;æ–‡å­¦&#39;: 2, &#39;æ ¡å›­&#39;: 3}
# åŠ è½½è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
def LoadData(filepath):
    l_documents = []
    l_labels = []
    for root, dirs, files in os.walk(filepath):
        for l_file in files:
            l_label = root.split(&#39;&#47;&#39;)[-1]
            l_filename = os.path.join(root, l_file)
            
            with open(l_filename, &#39;r&#39;) as l_f:
                l_content = l_f.read()
                l_wordlist = list(jieba.cut(l_content))
                l_words = [item for item in l_wordlist if item not in l_stopWords]
                l_documents.append(&#39; &#39;.join(l_words))
                l_labels.append(l_labelMap[l_label])
                
    return l_documents, l_labels

l_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;train&#39;)
l_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;test&#39;)

# # 2. è®¡ç®—æƒé‡çŸ©é˜µ
l_tfidfVec = TfidfVectorizer(max_df=0.5)
l_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)

# for item in l_tfidfVec.get_feature_names():
#     print item
# print l_tfidfVec.get_feature_names()
# print l_tfidfVec.vocabulary_
print l_tfidfMatrix.toarray().shape

# # 3. æœ´ç´ è´å¶æ–¯æ¨¡å‹
# ## 3.1 æ¨¡å‹è®­ç»ƒ
l_clf = MultinomialNB(alpha=0.001)
l_clf.fit(l_tfidfMatrix, l_trainLabels)

# ## 3.2 æ¨¡å‹é¢„æµ‹
l_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)
l_testFeature = l_testTfidf.fit_transform(l_testDocuments)
l_hats = l_clf.predict(l_testFeature)

# ## 3.3 æ¨¡å‹è¯„ä¼°
from sklearn.metrics import accuracy_score
print accuracy_score(l_hats, l_testLabels)</div>2019-02-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/b4/76/edb4988e.jpg" width="30px"><span>Jasmine</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>è€å¸ˆï¼Œæˆ‘æƒ³è¯·æ•™ä¸€ä¸‹ï¼Œè®¡ç®—å•è¯æƒé‡æ—¶ï¼Œä¸ºä»€ä¹ˆtrain_featuresç”¨çš„fit_transformæ–¹æ³•ï¼Œè€Œtest_featureç”¨çš„æ˜¯transform</div>2020-11-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg" width="30px"><span>æ»¢</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>æœ€åé¢çš„ä»£ç å¤ªä¹±ï¼Œå¾ˆå¤šéƒ½ä¸çŸ¥é“ä»å“ªé‡Œæ¥çš„ï¼Œæ— æ³•é¡ºç€çœ‹ä¸‹å»~~~</div>2019-04-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg" width="30px"><span>Geek_hve78z</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div># -*- coding:utf8 -*-
# ç³»ç»Ÿï¼šmac 

# 1. åŠ è½½æ•°æ®
# åŠ è½½åœç”¨è¯è¡¨

l_stopWords = [line.strip() for line in open(&#39;.&#47;text_classification-master&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).readlines()]  
   
l_labelMap = {&#39;ä½“è‚²&#39;: 0, &#39;å¥³æ€§&#39;: 1, &#39;æ–‡å­¦&#39;: 2, &#39;æ ¡å›­&#39;: 3}
# åŠ è½½è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
def LoadData(filepath):
    l_documents = []
    l_labels = []
    
    for root, dirs, files in os.walk(filepath):
        for l_file in files:
            if l_file==&#39;.DS_Store&#39;:
                continue
            l_label = root.split(&#39;&#47;&#39;)[-1]
            l_filename = os.path.join(root, l_file)
            
            with open(l_filename, &#39;r&#39;,encoding=&#39;gbk&#39;) as l_f:
                try:
                    l_content = l_f.read()
                except Exception as err:
                    print(err)
                    print(l_filename)
                    continue
                generator = jieba.cut(l_content)
                words = &#39; &#39;.join(generator)
                l_wordlist=words.split(&#39; &#39;)
                l_words = [item for item in l_wordlist if item not in l_stopWords]
                l_documents.append(&#39; &#39;.join(l_words))
                l_labels.append(l_labelMap[l_label])
                
    return l_documents, l_labels

l_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;train&#39;)
l_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;test&#39;)

# # 2. è®¡ç®—æƒé‡çŸ©é˜µ
l_tfidfVec = TfidfVectorizer(max_df=0.5)
l_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)

print (l_tfidfMatrix.toarray().shape)

# # 3. æœ´ç´ è´å¶æ–¯æ¨¡å‹
# ## 3.1 æ¨¡å‹è®­ç»ƒ
l_clf = MultinomialNB(alpha=0.001)
l_clf.fit(l_tfidfMatrix, l_trainLabels)

# ## 3.2 æ¨¡å‹é¢„æµ‹
l_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)
l_testFeature = l_testTfidf.fit_transform(l_testDocuments)
l_hats = l_clf.predict(l_testFeature)

# ## 3.3 æ¨¡å‹è¯„ä¼°
from sklearn.metrics import accuracy_score
print (accuracy_score(l_hats, l_testLabels))</div>2019-04-05</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqMiaIuXLFmXvVlnP9Do2icudO3JV6l5ueicWYYFhZb2ftT9XSKHFHJWa33XLnUKlCSs0JhvI7omF8Mg/132" width="30px"><span>wzhan366</span> ğŸ‘ï¼ˆ2ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>å»ºè®® å¤§å®¶å…ˆåšè‹±æ–‡ç‰ˆæœ¬ï¼Œå› ä¸ºä¸­æ–‡çš„unicode encodeå’Œdecodeä¸æ˜¯å¾ˆå¥½å¼„ï¼Œä¸åˆ©äºä¸­é—´æ­¥éª¤çš„å¯è§†åŒ–ã€‚å¦‚æœå¯¹ä»£ç æœ‰ç–‘æƒ‘ï¼Œå¯ä»¥è¯•è¯•è¿™ä¸ªpipelineï¼Œ sklearn çš„ã€‚ ä¸è¿‡ï¼Œè¿™ä¸ªæ²¡æœ‰ç”¨NTLKã€‚</div>2019-02-06</li><br/><li><img src="" width="30px"><span>lemonlxn</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æ­¤å¤„æ²¡æœ‰å¯¹ sklearn çš„tfidfè¿›è¡Œè®²è§£ï¼Œä¸ºå®Œå–„ç»“æœï¼Œæ€»ç»“å¦‚ä¸‹ï¼š

ä¼ ç»Ÿ TF_IDF = TF * IDF

	TF  = è¯¥æ–‡æ¡£æŸå•è¯å‡ºç°æ¬¡æ•° &#47; è¯¥æ–‡æ¡£çš„æ€»å•è¯æ•°

	IDF = loge(æ–‡æ¡£æ€»æ•° &#47; è¯¥å•è¯å‡ºç°çš„æ–‡æ¡£æ•° + 1)


sklearn TF_IDF = TF * IDF

	TF  = è¯¥æ–‡æ¡£æŸå•è¯å‡ºç°æ¬¡æ•°
        IDF = loge((1 + n) &#47; (1 + dn)) + 1
       
        å…¶ä¸­ n  ä¸ºè®­ç»ƒé›†æ–‡æ¡£æ•°
               dn ä¸ºæµ‹è¯•é›†å‡ºç°è¯¥å•è¯çš„æ–‡æ¡£æ•°
        
        å¦‚æœnorm=None åˆ™ç»“æœä¸º 
          X.toarray()
        
        å¦‚æœ norm=&#39;l2&#39; åˆ™ç»“æœä¸ºï¼š
          X.toarray() &#47; np.sum(X.toarray() ** 2) ** 0.5
        
        å¦‚æœ norm=&#39;l1&#39;åˆ™ç»“æœä¸º
          X.toarray() &#47; np.sum(X.toarray())</div>2020-09-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/56/64/7d80093c.jpg" width="30px"><span>é»„äº‰è¾‰</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è¿™é‡Œç™¾æ€ä¸å¾—è§£ï¼Œ
è®­ç»ƒé›†ä¸­ï¼Œï¼ˆ4ä¸ªæ–‡ä»¶çš„è¯æ··åœ¨ä¸€èµ·ï¼‰æ¯ä¸ªè¯çš„ TF-IDFå€¼å’Œåˆ†ç±»éƒ½æ··åœ¨ä¸€èµ·äº†ï¼Œæ˜¯å¦‚ä½•å®ç°åˆ†ç±»å™¨çš„å‘¢ï¼Ÿ
</div>2020-09-10</li><br/><li><img src="" width="30px"><span>Viola</span> ğŸ‘ï¼ˆ1ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æœ‰å¤§é‡ä»£ç å®ç°çš„ ï¼Œèƒ½å¦å°†githubåœ°å€è´´ä¸€ä¸‹</div>2019-01-31</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/SAicxHNJGBiavTgLlYXydetlV4S1Lr1icEbVVCY7LCFK0WVnP8udTWCwkibevnclMWOnfREugguzLM11aBunicIicyCg/132" width="30px"><span>è®¸æ„¿å­—èŠ‚ä¸Šå²¸å†²å†²å†²</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼Œæ˜¯ä¸æ˜¯å¯ä»¥ç†è§£ä¸ºfitå‡½æ•°è®¡ç®—äº†idfçš„å€¼å’Œç‰¹å¾å•è¯ï¼Œtransformå‡½æ•°è®¡ç®—äº†tfçš„å€¼å¹¶ä¸idfç›¸ä¹˜ç®—å‡ºäº†çœŸæ­£çš„tf-idfã€‚æ‰€ä»¥train_featuresè°ƒç”¨äº†fit_transformè®¡ç®—å‡ºäº†idfï¼Œtest_featuresè°ƒç”¨äº†transformåˆ™é€šè¿‡test_featuresè®¡ç®—å‡ºçš„TFå€¼ä¹˜ä»¥äº†ä¹‹å‰é€šè¿‡train_featuresç®—å‡ºçš„idfå€¼å¾—å‡ºæ¥æœ€ç»ˆçš„TF-IDFçŸ©é˜µï¼Ÿ</div>2021-04-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/06/4d/430faf63.jpg" width="30px"><span>.</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>æœ€ååªæ˜¯å¾—å‡ºæ¥äº† åˆ†ç±»çš„å‡†ç¡®ç‡ï¼Ÿé‚£ä¹ˆ å†æœ‰ä¸€æ‰¹æ–°çš„æ–‡æ¡£æ•°æ® æ€ä¹ˆæŠŠåˆ†ç±»ç®—æ³•åº”ç”¨ä¸Šå»æŠŠæ–°çš„æ–‡æ¡£æ•°æ®è¿›è¡Œåˆ†ç±»å¼€å‘¢ï¼Ÿ</div>2021-01-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/86/79/066a062a.jpg" width="30px"><span>éåŒå‡¡æƒ³</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>äº¤ä½œä¸šï¼š
import jieba
from sklearn.naive_bayes import MultinomialNB
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score

BASE_DIR=&#39;&#47;home&#47;zjtprince&#47;Documents&#47;text_classification&#47;text classification&#47;&#39;

def cut_text(filepath):
    text = open(filepath,&#39;r&#39;,encoding=&#39;gb18030&#39;).read()
    words = jieba.cut(text)
    return &#39; &#39;.join(words) ;

def load_features_and_labels(dir , label):
    features = []
    labels = []
    files = os.listdir(dir)
    for file in files:
        features.append(cut_text(dir + os.sep + file))
        labels.append(label)
    return features , labels

def build_word_list_and_label_list(type_name):
    train_features1, labels1 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;å¥³æ€§&#39;, &#39;å¥³æ€§&#39;)
    train_features2, labels2 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;æ–‡å­¦&#39;, &#39;æ–‡å­¦&#39;)
    train_features3, labels3 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;æ ¡å›­&#39;, &#39;æ ¡å›­&#39;)
    train_features4, labels4 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;ä½“è‚²&#39;, &#39;ä½“è‚²&#39;)
    train_list = train_features1 + train_features2 + train_features3 + train_features4
    label_list = labels1 + labels2 + labels3 + labels4
    return train_list, label_list

def load_stop_words():
    stop_words = open(BASE_DIR+&quot;stop&#47;stopword.txt&quot;, &#39;r&#39;,encoding=&#39;utf-8&#39;).read()
    stop_words = stop_words.encode(&#39;utf-8&#39;).decode(&#39;utf-8-sig&#39;)
    return stop_words.split(&#39;\n&#39;)

if __name__ == &#39;__main__&#39;:
    stop_words = load_stop_words()
    train_list, label_list = build_word_list_and_label_list(&#39;train&#39;)
    test_list, test_labels = build_word_list_and_label_list(&#39;test&#39;)

    vec = TfidfVectorizer(stop_words=stop_words)
    vec.fit(train_list)
    train_data = vec.transform(train_list)
    test_data = vec.transform(test_list)

    bayes = MultinomialNB(alpha=0.001)
    ctf = bayes.fit(train_data, label_list)

    predict = ctf.predict(test_data)
accur = accuracy_score(predict,test_labels)
print(&quot;å‡†ç¡®ç‡ä¸ºï¼š%f&quot; , accur)</div>2020-11-21</li><br/><li><img src="" width="30px"><span>ä¸å›å…±å‹‰</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>é‚£ä¸ªçŸ©é˜µæ¯ä¸ªå…ƒç´ ä»£è¡¨ä»€ä¹ˆï¼Ÿæ¯ä¸ªå•è¯çš„TF-IDFå€¼å—ï¼Ÿæ€ä¹ˆæ„Ÿè§‰å¯¹ä¸ä¸Šã€‚ä¸åŒçš„è¡Œçš„åˆä»£è¡¨ä»€ä¹ˆå‘¢ï¼Ÿè¯¦ç»†è§£é‡Šä¸€ä¸‹è¿™ä¸ªçŸ©é˜µ</div>2020-10-29</li><br/><li><img src="" width="30px"><span>lemonlxn</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>IDF = log2(total_document &#47; (appear_document + 1 )) 

ç”±äº total_document æ˜¯å›ºå®šçš„ï¼Œå¦‚æœå¸Œæœ› IDF è¶Šå¤§ï¼Œåˆ™ appear_document è¶Šå°è¶Šå¥½ã€‚

appear_document è¶Šå°ï¼Œé‚£ä¹ˆIDFåˆ™ä¼šè¶Šå¤§ï¼Œè¯¥è¯åŒºåˆ†åº¦ä¹Ÿä¼šè¶Šå¤§ï¼Œè¯¥æ–‡æ¡£å¯ä»¥é€šè¿‡è¿™ä¸ªè¯ï¼Œæ›´å®¹æ˜“åŒºåˆ†å…¶ä»–æ–‡æ¡£</div>2020-09-24</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJAl7V1ibk8gX62W5I4SER2zbQAj3gy5icJlavGhnAmxENCia7QFm8lE3YBc5HOHvlyNVFz7rQKFQ7dA/132" width="30px"><span>timeng27</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æˆ‘å‡†å¤‡å…ˆå¤§è‡´è¿‡ä¸€éï¼Œé‡Œé¢çš„ç»ƒä¹ è¿˜æ˜¯ä¸é”™çš„ã€‚
ä¸è¿‡è¿™æ ·å¬ç€éŸ³é¢‘å­¦ä¹ ï¼Œè¿˜æ˜¯ä¸ä¹ æƒ¯ã€‚

ä¸‰ä¸ªæœ´ç´ è´å¶æ–¯:
1,é«˜æ–¯â€”â€”æ•°æ®è¿ç»­å˜åŒ–ï¼›
2,å¤šé¡¹â€”â€”å¸¸ç”¨æ–‡æœ¬åˆ†ç±»ï¼ŒTF-IDF
3,ä¼¯åŠªåˆ©â€”â€”0&#47;1</div>2020-04-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg" width="30px"><span>Ronnyz</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ3ï¼‰<div>ç»ƒä¹ é¢˜ï¼š
acc_score :  0.755 æ¯”è€å¸ˆçš„ä»£ç è¿è¡Œçš„ç»“æœå·®äº†è®¸å¤šï¼Œä¸çŸ¥é“æ˜¯ä»€ä¹ˆåŸå› å‘¢
æ€è€ƒé¢˜ï¼š
èº«é«˜ã€ä½“é‡ ï¼šè¿™æ˜¯è¿ç»­å‹å˜é‡ï¼Œé€‚åˆé«˜æ–¯è´å¶æ–¯
é‹ç ã€å¤–è²Œ ï¼šè¿™ä¸ªä¸€èˆ¬ç”¨ç¦»æ•£å–å€¼ï¼Œé€‚åˆå¤šé¡¹å¼è´å¶æ–¯
</div>2019-11-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f4/52/10c4d863.jpg" width="30px"><span>FeiFei</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ2ï¼‰<div>æˆ‘æœ‰ä¸ªç–‘é—®ï¼Œè´å¶æ–¯æ˜¯åŸºäºè®­ç»ƒæ•°æ®æ¥è¿›è¡Œåˆ†ç±»çš„ï¼Œè®­ç»ƒæ•°æ®å·²ç»æœ‰æ ‡ç­¾äº†ã€‚
ä½†åœ¨æ²¡æœ‰åˆ†ç±»å¥½çš„æ•°æ®å‰ï¼Œæ€ä¹ˆå»æŠŠä¸€äº›æ•°æ®æ‰“ä¸Šè¿™ä¸ªæ ‡ç­¾ï¼Ÿ
æ„Ÿè§‰æˆ‘é™·å…¥äº†ä¸€ä¸ªæ­»å¾ªç¯ã€‚
</div>2019-07-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg" width="30px"><span>å¬å¦ˆå¦ˆçš„è¯</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æˆ‘çš„ä»£ç ä½äºï¼šhttps:&#47;&#47;pastebin.com&#47;kqjXgy0c ï¼Œæœ€ç»ˆç»“æœ0.925
æ³¨æ„: ä¸­æ–‡åˆ†è¯ï¼ŒTfidfVectorizerå¢åŠ ä¸€ä¸ªå‚æ•°ï¼štokenizer=jieba.cut,ï¼ˆæ¥è‡ªgithub jieba issue: https:&#47;&#47;github.com&#47;fxsjy&#47;jieba&#47;issues&#47;332ï¼‰


train_contents=[]
train_labels=[]
test_contents=[]
test_labels=[]
#  å¯¼å…¥æ–‡ä»¶
import os
import io
start=os.listdir(r&#39;text classification&#47;train&#39;)
for item in start:
    test_path=&#39;text classification&#47;test&#47;&#39;+item+&#39;&#47;&#39;
    train_path=&#39;text classification&#47;train&#47;&#39;+item+&#39;&#47;&#39;
    for file in os.listdir(test_path):
        with open(test_path+file,encoding=&quot;GBK&quot;) as f:
            test_contents.append(f.readline())
            #print(test_contents)
            test_labels.append(item)
    for file in os.listdir(train_path):
        with open(train_path+file,encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;) as f:
            train_contents.append(f.readline())
            train_labels.append(item)
print(len(train_contents),len(test_contents))
 
# å¯¼å…¥stop word
import jieba
from sklearn import metrics
from sklearn.naive_bayes import MultinomialNB  
stop_words = [line.strip() for line in io.open(&#39;text classification&#47;stop&#47;stopword.txt&#39;).readlines()]
 
# åˆ†è¯æ–¹å¼ä½¿ç”¨jieba,è®¡ç®—å•è¯çš„æƒé‡
tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5)
train_features = tf.fit_transform(train_contents)
print(train_features.shape)
 
æ¨¡å— 4ï¼šç”Ÿæˆæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
# å¤šé¡¹å¼è´å¶æ–¯åˆ†ç±»å™¨
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)
 
æ¨¡å— 5ï¼šä½¿ç”¨ç”Ÿæˆçš„åˆ†ç±»å™¨åšé¢„æµ‹
test_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)
test_features=test_tf.fit_transform(test_contents)
 
print(test_features.shape)
predicted_labels=clf.predict(test_features)
print(metrics.accuracy_score(test_labels, predicted_labels))
 
# æœ€ç»ˆç»“æœ0.925

</div>2019-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/de/17/a3b8f785.jpg" width="30px"><span>å°è«</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è€å¸ˆï¼Œå®Œæ•´ä»£ç èƒ½è´´å‡ºæ¥å—ï¼Ÿ</div>2019-03-10</li><br/><li><img src="" width="30px"><span>ä¸‰ç¡åŸºç”²è‹¯</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>import jieba
import glob
import io
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

classification = [&quot;campus&quot;, &quot;female&quot;, &quot;sports&quot;, &quot;literature&quot;]
train_files_list = []
test_files_list = []
trainpathprefix = &quot;.&#47;text_classification&#47;train&#47;&quot;
testpathprefix = &quot;.&#47;text_classification&#47;test&#47;&quot;
pathsuffix = &quot;&#47;*.txt&quot;
train_label = []
test_label = []
train_docments = []
test_docments = []
stopword_path = &#39;.&#47;text_classification&#47;stop&#47;stopword.txt&#39;

for i in classification:
    trainpathstr = trainpathprefix + i + pathsuffix
    testpathstr = testpathprefix + i + pathsuffix
    trainpathlist = glob.glob(trainpathstr)
    lentrainlist = len(trainpathlist)
    train_label += [i for j in range(lentrainlist)]
    testpathlist = glob.glob(testpathstr)
    lentestlist = len(testpathlist)
    test_label += [i for j in range(lentestlist)]
    train_files_list += trainpathlist
    test_files_list += testpathlist

for i in train_files_list:
    f = open(i, &#39;r&#39;)
    content = f.readlines()[0]
    contentlist = list(jieba.cut(content))
    contentwithspace = &quot; &quot;.join(contentlist)
    train_docments.append(contentwithspace)

for i in test_files_list:
    f = open(i, &#39;r&#39;)
    content = f.readlines()[0]
    contentlist = list(jieba.cut(content))
    contentwithspace = &#39; &#39;.join(contentlist)
    test_docments.append(contentwithspace)

stopwords = [l.strip(&#39;\n&#39;) for l in io.open(stopword_path, encoding=&#39;utf-8&#39;).readlines()]
train_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5)
train_features = train_tf.fit_transform(train_docments)
clf = MultinomialNB(alpha=0.001).fit(train_features, train_label)
test_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5, vocabulary=train_tf.vocabulary_)
test_features = test_tf.fit_transform(test_docments)
predicted_labels = clf.predict(test_features)
print(metrics.accuracy_score(test_label, predicted_labels))
è¿åŠ¨çš„300.txtæ–‡ä»¶å› ä¸ºå­—ç¬¦é—®é¢˜æ‰‹åŠ¨ä¿®æ”¹äº†ä¸€ä¸‹ã€‚</div>2019-03-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/54/74/2959ff0b.jpg" width="30px"><span>é£Lisa</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>æˆ‘è§‰å¾—è€å¸ˆä½ è®²çš„ï¼Œå¤ªå¥½äº†ï¼ä»¥å‰çœ‹æœºå™¨å­¦ä¹ æ€»æ˜¯çœ‹ä¸è¿›å»ï¼Œçœ‹äº†ä½ çš„è®²è§£çœŸçš„è®©æˆ‘æèµ·äº†å…´è¶£ï¼Œä¸€ä¸‹å­çœ‹äº†åå‡ ç¯‡ã€‚ç„¶åè¯·é—®ä¸€ä¸‹è€å¸ˆï¼Œè¦ç»§ç»­é”»ç‚¼æœºå™¨å­¦ä¹ çš„å®æˆ˜èƒ½åŠ›çš„è¯ä½ æ¨èä»€ä¹ˆä¹¦æˆ–è€…è¯¾ç¨‹æˆ–è€…ç»ƒä¹ é¡¹ç›®ï¼Ÿ</div>2019-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/24/b0/a6e0b03a.jpg" width="30px"><span>ä¸€è¯­ä¸­çš„</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>å‰åçœ‹äº†3éï¼Œæˆ‘ç»ˆäºç†è§£äº†ï¼Œä¹Ÿå¯ä»¥è‡ªå·±æ•²å‡ºç»“æœäº†ï¼</div>2019-02-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8d/7e/85a3ff2c.jpg" width="30px"><span>ä¹”å·´</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>#code:utf-8
import pandas as pd
import numpy as np
import io
import os
import jieba

def preprocess(path_name):
    text_with_spaces=&quot;&quot;
    textfile=open(path_name,&quot;r&quot;).read() 
    textcut=jieba.cut(textfile)
    for word in textcut:
        text_with_spaces+=word+&quot; &quot;
    return text_with_spaces

def loadtrainset(path,classtag):
    allfiles=os.listdir(path)
    processed_textset=[]
    allclasstags=[]
    for thisfile in allfiles:
        path_name=path+&quot;&#47;&quot;+thisfile
        processed_textset.append(preprocess(path_name))
        allclasstags.append(classtag)
    return processed_textset,allclasstags

stop_words = open(&#39;D:&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).read()
stop_words = stop_words.encode(&#39;utf-8&#39;).decode(&#39;utf-8-sig&#39;) 
stop_words = stop_words.split(&#39;\n&#39;) 

processed_textdata1,class1=loadtrainset(&quot;D:&#47;train&#47;å¥³æ€§&quot;, &quot;å¥³æ€§&quot;)
processed_textdata2,class2=loadtrainset(&quot;D:&#47;train&#47;ä½“è‚²&quot;, &quot;ä½“è‚²&quot;)
processed_textdata3,class3=loadtrainset(&quot;D:&#47;train&#47;æ–‡å­¦&quot;, &quot;æ–‡å­¦&quot;)
processed_textdata4,class4=loadtrainset(&quot;D:&#47;train&#47;æ ¡å›­&quot;, &quot;æ ¡å›­&quot;)
integrated_train_data=processed_textdata1+processed_textdata2+processed_textdata3+processed_textdata4
classtags_list=class1+class2+class3+class4
print(integrated_train_data[0])
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)
train_features = tf.fit_transform(integrated_train_data)

train_labels=[0]

clf = MultinomialNB(alpha=0.01).fit(train_features, classtags_list)

test_textdata1,testClass1=loadtrainset(&quot;D:&#47;test&#47;å¥³æ€§&quot;, &quot;å¥³æ€§&quot;)
test_textdata2,testClass2=loadtrainset(&quot;D:&#47;test&#47;ä½“è‚²&quot;, &quot;ä½“è‚²&quot;)
test_textdata3,testClass3=loadtrainset(&quot;D:&#47;test&#47;æ–‡å­¦&quot;, &quot;æ–‡å­¦&quot;)
test_textdata4,testClass4=loadtrainset(&quot;D:&#47;test&#47;æ ¡å›­&quot;, &quot;æ ¡å›­&quot;)
integrated_test_data=test_textdata1+test_textdata2+test_textdata3+test_textdata4
classtags_list=testClass1+testClass2+testClass3+testClass4

test_tf = TfidfVectorizer( max_df=0.5)
test_features=tf.transform(integrated_test_data)
predicted_labels=clf.predict(test_features)

print(metrics.accuracy_score(classtags_list, predicted_labels))
</div>2019-02-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/03/3f/09308258.jpg" width="30px"><span>é›¨å…ˆç”Ÿçš„æ™´å¤©</span> ğŸ‘ï¼ˆ0ï¼‰ ğŸ’¬ï¼ˆ1ï¼‰<div>è¿˜æ˜¯å¸Œæœ›è€å¸ˆå¯ä»¥åœ¨githubåˆ†äº«ä¸€ä¸‹ä»£ç ï¼Œç»ƒä¹ é¢˜è¿˜æ˜¯æ²¡æœ‰åŠæ³•è§£</div>2019-02-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg" width="30px"><span>å¬å¦ˆå¦ˆçš„è¯</span> ğŸ‘ï¼ˆ5ï¼‰ ğŸ’¬ï¼ˆ0ï¼‰<div># ç”±äºè¯„è®ºä¸æ”¯æŒmarkdownï¼Œä»£ç æ”¾åœ¨https:&#47;&#47;pastebin.com&#47;kqjXgy0c

train_contents=[]
train_labels=[]
test_contents=[]
test_labels=[]
#  å¯¼å…¥æ–‡ä»¶
import os
import io
start=os.listdir(r&#39;text classification&#47;train&#39;)
for item in start:
    test_path=&#39;text classification&#47;test&#47;&#39;+item+&#39;&#47;&#39;
    train_path=&#39;text classification&#47;train&#47;&#39;+item+&#39;&#47;&#39;
    for file in os.listdir(test_path):
        with open(test_path+file,encoding=&quot;GBK&quot;) as f:
            test_contents.append(f.readline())
            #print(test_contents)
            test_labels.append(item)
    for file in os.listdir(train_path):
        with open(train_path+file,encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;) as f:
            train_contents.append(f.readline())
            train_labels.append(item)
print(len(train_contents),len(test_contents))

# å¯¼å…¥stop word
import jieba
from sklearn import metrics
from sklearn.naive_bayes import MultinomialNB  
stop_words = [line.strip() for line in io.open(&#39;text classification&#47;stop&#47;stopword.txt&#39;).readlines()]

# åˆ†è¯æ–¹å¼ä½¿ç”¨jieba,è®¡ç®—å•è¯çš„æƒé‡
tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5)
train_features = tf.fit_transform(train_contents)
print(train_features.shape)

æ¨¡å— 4ï¼šç”Ÿæˆæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
# å¤šé¡¹å¼è´å¶æ–¯åˆ†ç±»å™¨
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)

æ¨¡å— 5ï¼šä½¿ç”¨ç”Ÿæˆçš„åˆ†ç±»å™¨åšé¢„æµ‹
test_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)
test_features=test_tf.fit_transform(test_contents)

print(test_features.shape)
predicted_labels=clf.predict(test_features)
print(metrics.accuracy_score(test_labels, predicted_labels))

# æœ€ç»ˆç»“æœ0.925</div>2019-03-21</li><br/>
</ul>