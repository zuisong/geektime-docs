你好，我是 Tyler。

在上一节课中，你已经学习了自然语言处理（NLP）的预训练模型技术。经过持续不断的探索，NLP领域迎来了许多重大的突破。其中，Transformer模型及其衍生模型BERT和GPT系列就是最具代表性的例子。这些研究成果为NLP预训练模型的发展带来了曙光。

不过，这只是大语言模型波澜壮阔发展历史的开端。随后，自然语言处理（NLP）的预训练模型技术在短时间内取得了飞跃式进展，迅速迈入了技术爆炸的阶段。其中一个关键因素是Transformer的问世，其出色的性能和训练效率提升为大型模型技术的发展创造了必要的条件。

因此这节课，我就会带你啃下这个 Transformer 这个硬骨头，不过请放心，它的原理其实并不复杂。但想要理解它，请务必确保你已经认真学习了 [第12节课](https://time.geekbang.org/column/article/696734) 的内容， **深入理解上节课提到的 Seq2Seq 架构以及编码器和解码器的作用**。

因为没有这些基础的话，你学习 **Transformer 的过程就会有点像在听天书。** 当然了，我也会延续我们课程的风格，尽可能可以通过白话让你理解它。

## 注意力机制

[上节课](https://time.geekbang.org/column/article/698540) 我们提到了注意力机制。你可能已经发现，早期的注意力机制是需要附着在其他网络架构上才能发挥作用。

![](https://static001.geekbang.org/resource/image/92/33/92ecc911789d6aa174e20fff1b068e33.jpg?wh=1867x707)

![](https://static001.geekbang.org/resource/image/83/aa/834720936f6d00b71684912bb12dabaa.jpg?wh=1867x1050)

如图所示，我们已经在上节课学习了注意力机制的经典应用，也就是 LSTM w/ Attention。不过，这个机制不仅适用于此，还可以应用于视觉模型如CNN，甚至一些推荐系统算法中。这是因为，学会使用注意力是这些场景的共同需求。

![](https://static001.geekbang.org/resource/image/f1/2f/f12d7a87d6ce94030742b0d3b0b5a42f.jpg?wh=1867x851)

Google的研究人员认为，既然注意力机制这么有效，不如把它发挥到极致，也正是这个想法直接造就了 Transformer。没错，Transformer模型的主要灵感正式来源于注意力机制。接下来，我们来深入地聊一聊，Transformer 的工作流程。

首先，我们先来回顾一下Seq2Seq（Sequence-to-Sequence）架构所做的事情。

以机器翻译为例，这个过程里，编码器首先会将输入序列从源空间（例如中文的现实世界表达）投影到一个语义空间的向量表示中。接着，解码器将这个空间向量从语义空间映射回目标空间（例如英文的现实世界表达），生成一个新的序列作为翻译输出。

那么， Transformer 是怎么解决这几个空间之间的映射问题的呢？这就要涉及到 Transformer 的具体细节了。

在此之前，我想先教给你两个理解和掌握复杂事物的方法。首先，你需要自顶向下地去学习，而不要被裹挟到细节的漩涡之中。第二，我们要去拆解事物的每个组件，剖析各个组件的必要性，这样会帮助你理解，为什么它被设计成了现在这个样子。

所以，我们接下来先自顶向下地理解 Transformer，然后挨个拆解它的组件。如下图所示，Transformer 总体上是一个编码器-解码器的架构。

![](https://static001.geekbang.org/resource/image/d1/fb/d138a33aefd0yy3c90b549f6496f8efb.jpg?wh=1867x1050)

左侧是 Transformer 的编码器，它由很多个 Encoder 堆叠组成。右侧是它的解码器，由许多的Decoder堆叠组成。现在，我们先拿出放大镜看看编码器部分的内部构造吧，因为解码器其实和编码器是同构的，没有太大的区别。

## 进入高维空间

为了降低你的学习压力，我将通过一个小故事来串联 Transformer 各个部分的作用。先来听听故事的内容吧：故事发生在几千年后，由于那时地球的环境日益恶化，人类最终决定进行星际移民，离开这颗母星。

为了实现这个目标，我们需要让我们的星际舰队，探索宇宙中适合居住的新家园。为此，我们提出了一个名为Transformer 的搜寻计划，计划的目标是让人类尽快地找到目标星球。下面我们的计划开始了。

第一步，我们的舰队要先进入太空，才能开始星际之旅。这就好像Transformer模型，需要把我们的文本输入转化成高维向量一样，毕竟机器世界无法直接理解文本数据。

所以，Encoder 的第一层嵌入层会将输入的词汇转换为向量表示，也就是把输入映射到一个向量空间中，以便模型更好地理解输入内容。

这听起来很熟悉，对吧？没错，这就是我们 [第6节课](https://time.geekbang.org/column/article/689434) 里学习到的空间投影。所以，这里你可以使用独热编码对输入进行编码，让Transformer自学调整参数，或者使用像是Word2Vec预训练好的投影模型进行编码，让Transformer少走弯路。

### 组成编队：位置编码

很好，在完成第一步之后，你的舰队已经成功升空。接下来，为了确保舰船在无边无际的宇宙中，能够准确报告自己的位置并进行有效通讯，我们需要为每个成员分配独一无二的编号。这就是 Transformer 中 Endcoder 的第二层，也就是位置编码层所做的事情。

位置编码层会为嵌入层输出的这些向量添加位置信息，把位置信息融合到嵌入向量中，这能帮助模型区分不同词汇在输入中所处的位置。你可能会问，那么为什么RNN不需要位置编码呢？

这是一个很好的问题。要知道，RNN 是串行处理输入数据的，所以每个 RNN 单元其实都包含了所在位置之前的全部输入信息，所以对当前位置输入的处理是有状态的。

但是，Transformer会并行地处理所有输入的内容，所以各个并行单元会无状态地处理每个输入。因此，我们需要在最开始就给每个输入的嵌入向量一个位置编号，这样模型才能通过输入判断它在整体中的位置。

## 寻找目标星球

好了，在完成了这两步之后，舰船已经完成了升空和编队，下一步就要开始漫长的星际航行了。

### 宇宙航行：自注意力机制

随着太空舰队司令的一声令下，舰船纷纷启航进入广袤无垠的宇宙中，收集来自各个方向的信号。这个寻找目标的过程，在 Transformer中则是由自注意力机制来完成的。

自注意力机制会针对每个带有位置编码的输入向量，去计算和其他位置的关联程度，从而捕捉输入内部的上下文关联信息，形成一个注意力权重的分布作为后续层的输入，指导模型的学习过程。你可以参考后面的伪代码来加深理解。

```python
def self_attention(input_embedding):
    # 输入的嵌入向量矩阵，假设为input_embedding，维度为[序列长度, 嵌入维度]
    sequence_length, embedding_dim = input_embedding.shape
    # 通过线性变换得到Q、K、V
    Q = linear_transform(input_embedding)
    K = linear_transform(input_embedding)
    V = linear_transform(input_embedding)
    # 计算注意力分数
    attention_scores = Q @ K.T / sqrt(embedding_dim)
    # 进行归一化
    attention_weights = softmax(attention_scores, axis=1)
    # 将权重与V相乘得到上下文向量
    context_vector = attention_weights @ V
    return context_vector

```

在这里，有一个常被问到的问题，那就是：“注意力机制和自注意力机制的区别是什么？”

我来回答一下这个问题，帮助你区分、理解。注意力机制关注的是输入与输出之间的关联。比如模型对中译英训练数据 “我爱火锅” 译为 “I love hotpot” 的学习过程中，我们希望基于注意力机制的 LSTM 在翻译“我”的这个 Decoder 的位置上，注意力更多地集中在 “I” 上，而不是 “love” 上。

而自注意力机制的主要目标，则是确定输入词与词之间的内部关联性。比方说，在 “道可道非常道” 这句话中。我们希望 Transformer 可以通过自注意力机制，学到句中“非常道”指的是原句中的第一个“道”字而不是第三个。这种机制可以让 Transformer 学习理解这句话中更多隐含的信息和深意。

### 平行宇宙：多头注意力

在拥有了自注意力机制之后，你的舰队已经纷纷出发去搜寻目标星球了。这时，你的太空军司令在军帐中开始琢磨，是否有方法可以加快寻找目标的过程。恰逢这时候地球上发生了科技爆炸，人类获得了观测平行宇宙中事件的能力。所以，你的太空军司令决定在平行宇宙中收集舰队信息，以加速寻找目标星球的过程。

这个观测多个平行宇宙中舰队的航行情况的能力，正是 Transformer 中的多头注意力（Multi-Head Attention）模块所具备的。Transformer 模型会使用多组同构的自注意力（Self-Attention）模块，并行学习出多组不同的权重，每组权重表示了它根据输入信息所学习的不同自注意力权重。

最终通过将多组自注意力计算的结果拼接在一起，通过线性变换得到多头自注意力模块的联合输出。

总之啊，多头注意力机制类似于赛马机制，它有助于减少模型初始化的随机性对模型效果的影响。所以即使只留下一个注意力头也能使用，但这会导致模型的稳定性和多样性无法得到保障，进而造成模型的性能下降。

## 集体智慧决策

现在，太空军司令通过观察不同平行宇宙中的事件，就能不断汇总各个宇宙中舰队的信息。然而，随着平行宇宙的不断扩展，太空军司令发现自己疲于奔命，很难做出准确的决策，于是他开始想办法提高工作效率。

### 跨级沟通：残差归一化

首先，他决定授予各个舰队一定的权限，允许它们跨级沟通，以提高信息传递的效率。实际上，这就是之前我们学过的残差结构。

在Transformer的每个子层中，都使用了 **残差连接** 和 **层归一化** 来稳定训练过程。如果你记不起这部分原理的话，可以回顾一下 [第11节课](https://time.geekbang.org/column/article/692796) 中的ResNet。

### 掌控全局：前馈神经网络

在解决了信息传输的问题之后，为了提高信息汇总的效率，他又决定采用集体智慧决策的方式，通过设立总参谋部，分层汇总各个平行宇宙的舰队情况。这就是 Transformer 中多层感知机，也就是 MLP 的作用了。

从下图中你可以看出，在 Transformer 中，每个“头”都会输出到 MLP 中进一步的汇总信息，用来增加 Transformer 模型的拟合能力，提升决策的效率和效果。

至此，你已经掌握了 Transformer 中所有的核心组件以及它们的作用了。

![p5](https://static001.geekbang.org/resource/image/a7/bb/a700089a17a8833059c3905d3c377cbb.jpg?wh=1867x1050)

## 小结

今天的内容告一段落，我们做个总结吧。这节课我们讲述了一个关于星际舰队寻找新家园的故事。

在这个故事中，我们首先见证了舰队的升空启航，进入宇宙空间。这就好像 Transformer 模型中把文本输入数据转化成了高维空间向量的过程。

随后，为了确保每艘舰船在航行中，能够准确地报告自己的位置，并且进行有效的通讯，我们为每位成员分配了独一无二的编号，这也是 Transformer 中位置编码的作用。

随着太空军司令的一声令下，舰船们各自启航，寻找目标。这实际上就是自注意力机制所承担的任务，它负责在茫茫的宇宙中寻找目标。

同时，为了提高寻找过程的效率，太空军司令还拥有了一种“超能力”，那就是窥探平行宇宙中，各种可能的寻找路径。这就是多头注意力机制的巧妙之处，它可以极大地加速寻找的过程。

最后，司令汇总了所有舰船的探索结果，通过加权汇总的方式确定了目标星球的位置，就像 MLP 层在模型中的作用一样。

你可以对照后面的图来加深理解。是不是现在已经对 Transformer 各个部分的作用一目了然了?

![](https://static001.geekbang.org/resource/image/59/28/5935b6590b7733d8751c8fa40a6f8528.jpg?wh=1867x1050)

最初，Transformer只是机器翻译领域的一次尝试，但其惊人的效果和训练效率让它备受瞩目。在下一堂课中，我们将继续深入探讨Transformer最经典的变种，也就是GPT系列模型的原理和发展历程，敬请期待！

## 思考题

请你想一想，我们的模型里是否可以去掉位置编码？

恭喜完成我们第 14 次打卡学习，期待你在留言区和我交流互动。如果你觉得有收获，也欢迎你分享给你身边的朋友，邀 TA 一起讨论。