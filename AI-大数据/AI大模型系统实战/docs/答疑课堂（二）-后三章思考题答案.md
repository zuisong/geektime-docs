你好，我是Tyler。

又到了答疑课堂的时间了。之前我们讲过的内容你都学会了吗？建议已经忘记前面知识的同学在课前再回顾一下。这节课整理的题目会加大难度，直接和目前最前沿的大模型技术相关。

你做好准备了吗？让我们现在正式开始。具体的问题和答案你可以直接看文稿，每节课我也加入了超链接，方便你复习回顾。（这里我单独提一下第19节课的思考题，因为想清楚这节课的问题，对你理解提示语工程相当关键，我们在前面的课程中说过，我们是通过苏格拉底的产婆术来教会大模型思考的，而提示语工程就是大模型技术的产婆。）

# 第三章 技术原理篇

## [第11节课](https://time.geekbang.org/column/article/692796)

**思考题**

预训练模型和大模型之间的关系是什么？

**参考答案**

预训练模型（pre-training model）首先通过一批语料进行训练，然后在这个初步训练好的模型基础上，再继续训练或者另作他用。为了最大化模型复用的效果，往往使用参数量较大的模型作为预训练模型的网络结构。

## [第12节课](https://time.geekbang.org/column/article/696734)

**思考题**

1.这节课我们学习了如何给 LSTM 增加 Attention 机制，你可以思考一下，如果要给上节课学到的 CNN 增加这个机制，该如何做呢？

2.沿着课程中传声筒游戏可以“作弊”的思路想下去，你还能想出哪些作弊方法？越离谱越好！

**参考答案**

1.SE-Net: Squeeze-and-Excitation Networks

2.开放问题，答案越离谱分越高，因为这很可能是颠覆 Attention 的下一个科研 idea。

## [第13节课](https://time.geekbang.org/column/article/698540)

**思考题**

既然 BERT 的模型使用了大数据和大参数模型进行训练，那它是否属于大语言模型（LLM）呢？

**参考答案**

BERT属于大模型。它兼顾了参数量大（大型模型），训练数据量大（大量数据大规模训练）和迁移学习能力强（适应多种下游任务）几点，所以是一个大语言模型。

## [第14节课](https://time.geekbang.org/column/article/698985)

**思考题**

请你想一想，我们的模型里是否可以去掉位置编码？

**参考答案**

是可以的。实际上，许多视觉模型已经这么做了，因为去掉固定的位置编码可以使模型更加通用，不需要根据输入数据的长度来调整模型。然而，在自然语言处理领域，通常会保留位置编码，因为目前还没有找到更高性价比的方法。

## [第15节课](https://time.geekbang.org/column/article/700557)

**思考题**

你觉得 GPT-3 和你目前所使用的 ChatGPT 之间最大的区别是什么？

**参考答案**

GPT-3只使用通用数据进行训练，可以应用于多种不同的NLP任务，例如文本生成、文本分类、文本摘要等，但没有针对对话语料进行输入对齐。ChatGPT则是基于RLHF技术和人类的对话习惯进行了对齐，可以让大模型在与人类对话过程中发挥更多潜力。

## [第16节课](https://time.geekbang.org/column/article/701454)

**思考题**

1.由于 RLHF 只是单纯地根据已有的回答进行排序，是否会出现“自己吃自己”的循环，也就是用来训练模型的数据来自于模型自己生成的，“近亲繁殖”训练的模型水平是否会受影响？

2.RLHF 和 SFT 的关系是什么？

**参考答案**

1.会受影响。因此，OpenAI一直没有停止使用SFT进行增量训练。

2.SFT后的模型可以生成多个答案作为RLHF的输入，RLHF使回答更符合人的预期。其中更重要的是SFT，因为如果没有SFT，RLHF将成为无源之水。

## [第17节课](https://time.geekbang.org/column/article/701952)

**思考题**

1.在构建AI系统时，你会如何权衡模型规模和训练数据量，你会选择用更多的数据和小模型，还是更少的数据和大模型？

2.你认为涌现任务会如何影响人工智能的哪些应用领域？对于未来涌现任务的发展，你有哪些期待和疑虑？

**参考答案**

1.当前市面上大多数开放源代码的自然语言处理（NLP）模型通常具备相对适中的规模，在70亿到130亿个参数之间。虽然“大力出奇迹”是大模型技术快速发展的重要原因，但参数规模的增长不是大模型技术的银弹，各位同学还是要在训练数据，模型架构和大模型智能涌现原理的理解上多下功夫。

2.涌现的最大价值是让机器在远超人类的知识容量下，总结并产生新知，这会让人类在智力型任务上成为工具使用者，而不再是工具本身。这将大幅降低很多智力型工作的人力需求。

# 第四章 架构实战篇

## [第18节课](https://time.geekbang.org/column/article/702474)

**思考题**

不知道你是否已经关注到 GPT-4 再一次着重宣传了它的“微调”能力，首先我曾经说过，这个所谓的微调只是一个浅层的补丁，所以这里你一定要学会独立思考，理解 OpenAI 宣传这个能力背后的想法是什么。

如果各个领域的微调数据都汇聚到 OpenAI 是不是会形成强大的虹吸效应，让他们拥有众多的私域数据？

刚才我也提到了，各个系统之间最大的区别也就是生成模型能力的区别，更进一步说其实是数据质量的区别，希望你可以沿着这个方向做一些课后思考，在评论区说出你的看法。

**参考答案**

你或许以为 GPT-4 的目的是协助你进行模型训练，但实际上 GPT-4 需要的是你的数据。如果各家企业的相关业务负责人因业绩压力而不顾数据安全，大规模使用 GPT-4 进行所谓的微调，将导致数据流失，进一步巩固 OpenAI 的市场垄断地位，迫使你不得不依赖其服务，直至无法脱离。最后，微软再通过垄断获得的定价权收割各个客户。当然，这也需要监管和竞争政策来平衡，以确保数据和模型不被集中在某一家机构手中。

## [第19节课](https://time.geekbang.org/column/article/703597)

**思考题**

很多人认为提示语工程只是自说自话的试错游戏，你觉得他说得对吗？给出你认为正确或错误的原因。

**参考答案**

首先，你需要明白大语言模型自GPT-2以来就一直具备的一种能力，那就是通过与用户交互和提示语进行上下文学习（In-context Learning）。

在GPT-2中，提示语主要被用来 **向模型传递“指令”信息**，使模型了解其正在进行的任务。而在GPT-3之后，提示语的内容逐渐 **演变成为了包含“示例”和“指令”的形式**。其中，“示例”部分负责为模型提供任务场景的相关样例，帮助其掌握其中的规律；而“指令”部分被沿用下来，目的仍然是让模型明确自己的任务目标。

因此，提示语工程的本质其实是一种试图充分利用大语言模型上下文学习能力的方法。 **通过最佳的格式和最有效的示例及指令为模型提供指导**，使其能更好地理解和解决给定问题。在这个过程中，不仅需要考虑具体任务数据集的特性，而且也需要关注大语言模型本身的行为和表现。

为此，我们会运用一系列基于统计或基于监督学习的提示语工程技术，以提高模型的性能表现。所以提示语工程 **并不是自说自话的试错游戏**，而是非常依赖于实际数据和模型特性的数据驱动工作。

以下都是回答得很好的同学，他们分别从“上下文学习”的角度和“提示语引擎”的角度概括了系列课程的定位和重点，希望各位同学也能向他们学习，主动独立思考。

@一只豆 同学的答案。

> 提示语工程的本质，还是（领域）知识注入。好的提示词工程如果没有富含增量信息的知识作为外部记忆注入，如果没有提供与增量信息相匹配的推理逻辑，那就是没有注入知识。那就是一个死循环，或者说瞎尝试。面对 AI，拼的还是人类自身的知识/经验的积累+对 LLM 的 ICL 特点的深刻理解。老师攒了几个星期的内功，循循善诱到这个板块，终于排山倒海的开始降龙十八掌了。。。。。帅且飒！！

@周晓英 同学的答案。

> 提示语工程（Prompt Engineering）指的是为了使模型生成特定的输出而设计和优化输入提示的过程。在一些情况下，它可能看起来像是一个试错的游戏，因为工程师可能需要通过多次尝试来找到能够产生所需输出的正确提示。然而，提示语工程实际上是一种更复杂、更系统的过程，具有一定的技术和理论基础。以下是一些考虑点。

> **基于理论和经验**：提示语工程通常不是随机的。它通常基于对模型的理解，以及对特定任务和领域的知识。通过理解模型的工作方式和可能的局限性，工程师可以更有效地设计提示。

> **系统化和方法论**：提示语工程可以是一个系统化的过程，其中包括了测试、评估、和迭代。它可能包括了设计实验，收集和分析数据，以及使用统计和机器学习技术来优化提示。

> **数据驱动**：在很多情况下，提示语工程可以是数据驱动的。例如，可以通过分析模型在不同提示下的表现，或者收集用户或专家的反馈，来持续优化提示。

> **创造性**：提示语工程也需要一定的创造性和洞察力。通过理解任务的需求和模型的行为，工程师可以设计出能够引导模型生成有用输出的创新提示。

> **可扩展性和通用性**：高效的提示语工程应考虑到可扩展性和通用性，以使得找到的解决方案不仅仅是针对单一问题的，而是能够广泛应用于类似的任务和问题。

> **与模型训练和微调结合**：提示语工程通常不是孤立的。它可以与模型训练和微调相结合，以实现最优的性能。

## [第20节课](https://time.geekbang.org/column/article/704029)

**思考题**

如果只允许你为自己的大模型系统，添加一个这节课学到的提示语工程能力，你会选择哪个，为什么呢？

**参考答案**

我看到很多同学选择了 APE 方法，非常好，这确实是一个简单实用的提示语工程算法，但我更建议同学们考虑使用一些示例优选的方法。因为在提升大语言模型的回答质量方面，选择高质量的示例是最有效的方法。

@顾琪瑶 同学的答案。

> 如果按照性能来看, APE应该是最合适的……

@周晓英 同学的答案。

> 我可能会选择APE，因为我目前的项目中大体上是这样实现的，类似：假设我们正在构建一个基于 GPT-3 的自然语言处理系统，旨在帮助用户自动写作。用户可以输入一个主题，系统将为他们生成一篇相关的文章。为了实现这个目标，我们可能需要设计一个有效的提示以引导模型生成高质量的内容…

## [第21节课](https://time.geekbang.org/column/article/707992)

**思考题**

如果让你用前几节课学到的提示语工程方法构建一个智能体，让它融入这个社会，你有什么思路吗？

**参考答案**

答案是22节课的全篇内容，其中包括智能体反思过去和规划未来的能力。

## [第22节课](https://time.geekbang.org/column/article/709197)

**思考题**

通过修改配置和代码，将我们在前几节课学到的提示语方法用在人工智能小镇的智能体上。

**参考答案**

开放问题，只要能够基于小镇的开源代码，实现目标效果即可。

## [第23节课](https://time.geekbang.org/column/article/710697)

**思考题**

基于 AI 小镇的开源代码，完成以下任务（需改动处的相关代码，在前面课程中已经学习）。

1.为智能体添加更为复杂的思维链能力。

2.使用高级提示语工程方法，为智能体优化提示词模板（提示：默认的各种提示词模板均在 reverie/backend\_server/persona/prompt\_template下）。

3.为你的智能体添加使用外部工具（比如搜索引擎）的能力。

**参考答案**

1.开放问题，只要能够实现目标效果即可，比如在小镇的开源实现中添加生成记忆和决策逻辑，采用前几节课学到的 CoT 方法优化生成内容质量。

2.目前智能体的提示词模板是静态模板，此处使用前几节课学到提示语模板方法如 APE 等对其进行改造即可。

3.这里需要使用类似 Self-Ask 的方法，在生成具体内容时，提示大语言模型有工具可以使用，并且提供交互方式，获取外部记忆补充，注入外部世界信息，提高智能体的对话质量。当然要注意的是，这里可能会出现西部世界机器人看到真实世界后怀疑人生的精神状况，希望你能够重点观察他们在使用搜索引擎工具后的表现。

## [第24节课](https://time.geekbang.org/column/article/713603)

**思考题**

今天我给你留的思考题比较多，但建议你认真作答，积极参与思考有助于你更好地掌握大模型技术。第一道题目比较简单，后面三道更有挑战性。如果你暂时没有能力实现代码，也可以先聊聊你的思路。

1.结合我们课程里学过的知识，谈一下你对大语言模型外部记忆作用的理解。

2.结合 HA3 为你的知识库增加倒排索引的能力。

3.结合 HA3 为你的知识库增加多路召回的能力。

4.如何通过 HA3 来存储人工智能小镇智能体的记忆流？

**参考答案**

1.大语言模型外部记忆主要有以下几个作用，第一是针对特定的任务提供优质的示例，第二则是存储大模型需要使用的海量外部知识，第三是帮助大模型存储能用到的外部工具，第四是记录大模型的会话信息或者智能体的记忆。

其他：开放问题，涉及到基于 HA3 的多路召回开发，只要能够实现目标效果即可。

## [第25节课](https://time.geekbang.org/column/article/713908)

**思考题**

1.根据这节课学习的知识，你认为使用 Alpaca 增强数据微调模型的上限是什么？

2.使用 Alpaca 方法，为它提供红楼梦的原文，让它帮你扩充后四十回的故事内容。

**参考答案**

1.Alpaca 或者说 self-instruct 的各种变种，本质上是在做模型对齐，如果你使用 GPT-4 来生成增强语料，则是在对齐目标任务领域上，你的模型与 GPT-4 之间的能力，所以上限就是你所选择对齐的那个模型。

2.基于 Alpaca 代码改写，使用红楼梦前八十回的内容制作种子数据，生成后四十回的内容（开放问题，达到目标效果即可）。

## [第26节课](https://time.geekbang.org/column/article/717818)

**思考题**

1.通过你对 LoRA 的学习，分析一下在使用 LoRA 微调的过程中，可能会存在哪些问题？

2.请通过 AutoML 的方法自动化 LoRA 的调参过程。

3.结合你对前面学习的知识，辨析一下 LoRA 方法和向量检索中的经典 ANN 算法 PQ 之间有何联系？（这是一道我曾面试AI大模型相关业务候选人的题目）

**参考答案**

1.可能存在 LoRA 的参数空间过大，无法完全测试，所以要引入一些超参数搜索的方法，比如 AutoML 的策略。

2.你可以使用 Amazon [这篇论文](https://arxiv.org/pdf/2305.16597v1.pdf) 的方法，或者用到了 LoRA 和 AutoML 技术都是合理的。

3.两个方法都是低维映射的矩阵来近似替代原矩阵，来提高工作效率。

## [第27节课](https://time.geekbang.org/column/article/718657)

**思考题**

根据前面学习的GPT系列原理的知识，想一想构建一个规模达到100B及以上的模型需要怎么做。

**参考答案**

认真学习了课程的你一定知道，GPT-3 是在 GPT-2 的基础上进行了参数量和训练数据规模的提升，所以一定程度上 GPT-2 是一个小型的 GPT-3。同时，GPT-2 也是 OpenAI 最后一次完整公开的 GPT 系列代码开源工作，之后已经渐渐成为 CloseAI 了。这也是为了让我们看到GPT宇宙早期的影像，对我们了解 OpenAI 的工程细节很有帮助。

因此如果你学有余力，强烈推荐你结合我们 GPT 系列课程所讲内容，跑通 GPT-2 的开源实现 [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)，加深对其技术原理的理解。

## [第28节课](https://time.geekbang.org/column/article/719033)

**思考题**

概括工业级AI大模型系统与原型验证工具（如LangChain和AutoGPT）之间的区别。并且总结出你心中的大模型技术和AI大模型系统是什么。

**参考答案**

开放式答案，参考答案参照第4节课内容。

# 第五章 前沿拓展篇

## [第29节课](https://time.geekbang.org/column/article/719696)

**思考题**

你所在的公司或业务处于创新中的哪个阶段？你认为目前所在业务的前景如何，会往哪个方向发展？

**参考答案**

开放式答案，结合自身实际，言之有理即可。

## [第30节课](https://time.geekbang.org/column/article/720462)

**思考题**

你认为这一轮大模型竞争中，国内外分别有哪些公司会成为最后的赢家？

![图片](https://static001.geekbang.org/resource/image/ed/b1/ed5f53b4846c50a18705b3c3054bceb1.png?wh=1088x418)

**参考答案**

开放式答案，但是要解释原因，分析逻辑，言之有理即可。

比如回答微软的这位同学，你应该说出微软的优势和不足有哪些，这些在课程中做过充分讨论。我们知道，收购并不一定能帮助大公司建立优势，比如阿里收购了市场排名前两名的优酷和土豆，目前视频领域的前两把交椅却被腾讯和爱奇艺占据；阿里收购的高德后，却是一直保持该领域的市场领先地位；阿里收购雅虎中国后则让雅虎消失在中国。

你如果能够分析这几次收购有何不同，以及微软对 OpenAI 的收购更像是哪一种，则能得出答案了。当然这只是一个例子，这里还是抛砖引玉，希望看到大家更好的答案。

## [第31节课](https://time.geekbang.org/column/article/720951)

**思考题**

我们知道，手机4G流量成本的下降催生了短视频产品的蓬勃发展，当大模型的使用成本可以忽略不计时，你预测会有哪些应用横空出世？

**参考答案**

开放式答案，你的答案应该涉及一个迫切需要大模型技术，但目前对成本十分敏感的产品或领域。

同学们会发现，随着课程的深入，我们的课后思考题也变得更为开放，希望你可以参与其中，积极思考。以上就是这次加餐的全部内容。欢迎你继续在留言区发表自己的见解或者提出疑问，沉淀自己的想法。