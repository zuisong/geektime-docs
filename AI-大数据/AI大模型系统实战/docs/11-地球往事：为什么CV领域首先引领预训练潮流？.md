你好，我是Tyler。

今天我们要开启一个全新的章节——技术原理篇。从今天开始，我们正式进入AI大模型的学习。在这节课中，主要学习内容是视觉预训练模型（Pre-Training Model，PTM）。

你可能会问，为什么要先讲预训练模型呢？很好的问题，我们所说的AI大模型，其实就是一种预训练模型。预训练模型会使用海量的数据对模型做长时间的预先训练，这能让它拥有强大的知识储备，更好地解决各个领域的问题。

我会带你深入学习“预训练技术”和“大模型技术”的发展历程，帮助你理解它们的来龙去脉，以便更好地学习大语言模型中的预训练方法。

## 视觉模型的发展之路

在开始预训练技术的学习之前，我们先来了解一下“大模型”的发展历程。

大模型这个概念在视觉领域并不陌生，第一代大模型技术就来自视觉领域。故事的开始还要从1943年说起，那一年神经生物学家 MeCulloch 与青年数学家 Pitts 合作，基于生物神经网络的工作特点，提出了第一个人工神经网络模型。

在此之后，神经科学成为了人工智能最重要的交叉学科之一。随着这两个学科交叉的不断深入，涌现出了各种伟大的仿生神经网络，其中最经典的网络就有我们即将学习的卷积神经网络（Convolutional Neural Networks, CNN）。

### CNN：初出茅庐

我们人类在处理视觉内容时天赋很高。相信你也会有这样的体会，看过一个东西之后，脑海中的图像记忆，远比声音和文字要深刻得多。所以研究人员总隐隐觉得生物的视觉机制更加精巧和智能，一直在尝试各种方法，用计算机去对图像处理的能力进行建模。

幸运的是，这个问题由于神经科学的新发现有了转机。什么？想要解答一道计算机应用题，首先要学的竟然是生物学知识？我这就带你看看这背后的故事。

1981 年，哈佛大学的休伯尔和维泽尔发现了视觉中的简单细胞和复杂细胞这两种类型的细胞。

- 简单细胞是在视网膜中发现的，它们是视觉系统的第一级神经元，对特定方向和空间位置的光敏性很强。有助于提取观测对象各种形状的边缘信息，可以拼凑勾勒出不同物体的轮廓。
- 复杂细胞是在初级视觉皮层中发现的，它们是在简单细胞之后的第二级神经元，在信号的传递过程中不会改变特征的位置和方向。

休伯尔和维泽尔的一系列研究获得了诺贝尔奖，也为卷积神经网络的发展奠定了基础。随后 LeCun 基于这项发现，提出了卷积神经网络（CNN）。

- CNN 的卷积层类似于简单细胞，通过将卷积算子作为滤波器。提取物体的视觉边缘信息，形成局部特征，并只与上一层局部区域相连接。
- CNN 的池化层则类似于复杂细胞，通过提取信号最强的部分，对底层的边缘特征进一步的聚合，同样确保了信号在传递中不改变位置和方向。

我在后面准备了一张CNN的示意图，方便你直观对比两类细胞和 CNN 网络之间的关系。

![](https://static001.geekbang.org/resource/image/a2/ff/a2f95d583f6c0244f88ec6c8f9e2d3ff.jpg?wh=3900x2194)

通过这种分层的特征传递方式，即使视觉目标在图像中移动位置，顶层的特征表示也不会改变。我们不由感叹生物构造进化的奇妙，这种平移不变的特性特别适合解决视觉任务中的各种问题。

早在 1998 年 LeNet-5 就成功使用下图中的 CNN 网络解决了手写数字识别的任务，并定义了 CNN 的基本结构。然而，由于当时训练 CNN 所需的计算资源非常庞大，学术界和产业界都没有进一步研究它的想法，只是把它看成实验室中的一种“玩具”封存了起来。

![](https://static001.geekbang.org/resource/image/91/40/9175419b7ceb7290d68a9901c0a78140.jpg?wh=3900x1594)

### AlexNet：一鸣惊人

直到 2009 年，李飞飞等研究员在 CVPR 2009 上发表了 [ImageNet: A Large-Scale Hierarchical Image Database](https://paperswithcode.com/dataset/imagenet)，这篇论文正式拉开 ImageNet 挑战赛的历史序幕，推动了整个计算机视觉领域的发展。

这个时期的 GPU 性能不断提高，人们的视野被拉回到那些性能开销大但值得一试的方法，革命性的方法呼之欲出。

直到 2012 年，Hinton 教授的团队终于通过利用一些沿用至今的训练技巧（包括 ReLU 激活函数、Dropout 正则化和数据增强）以及两块 GTX 580 GPU，让下图中 CNN 架构的 AlexNet 取得了突破性的成果，在 ImageNet 竞赛中一举夺冠。

![](https://static001.geekbang.org/resource/image/bc/35/bc8831cfbc8fdf9fd3e2e2f4b336da35.jpg?wh=3694x1614)

AlexNet 的成功，为深度学习领域带来了革命性的变化，从此深度学习成为了计算机视觉领域的主流算法。

值得一提的是，在这个时期，一家伟大的公司也在冉冉升起，那就是NVIDIA。NVIDIA 早期在 CUDA 上的布局让他迅速在深度学习的高性能计算（HPC）领域形成了绝对优势。如今，NVIDIA 已经成为市值过万亿美元的巨头公司了。

### ResNet：冲破瓶颈

也是在 AlexNet 之后，人们开始尝试使用更多的 GPU，来训练更深的神经网络。这时人们发现，在模型的网络层数不断增加后，出现了梯度消失和梯度爆炸的情况。这让研究人员在工作中变得畏首畏尾，不敢大规模地增加模型的层数，深度学习有些“深”不下去了。

这时候深度残差网络（ResNet）应运而生。ResNet 是 CNN 图像史上的一件里程碑事件，它可以让模型在大幅增加层数的同时，突破梯度爆炸的瓶颈，达到更高的准确性，这个能力让它在 ImageNet 和 COCO 等比赛中拿到了五项第一的好成绩。

那 ResNet 的解题思路是怎样的呢？它通过设计特殊的直通通道，引入了许多旁路直接连接到后续层。这就相当于让领导和员工直接见面，避免传话过程中信息的丢失。这个设计让模型可以绕过深层网络中的梯度爆炸和消失问题，更好地学习复杂的图像特征。

你可以先看看后面的示意图，再听我讲解 ResNet 残差模块的特别之处。

![](https://static001.geekbang.org/resource/image/93/c6/93cff55a4cde7a8360637749b78615c6.jpg?wh=3900x1635)

左图是常规的残差模块，由两个3×3的卷积核组成。然而，随着网络进一步加深，这种残差结构在实践中效果比较有限。

而右图的“瓶颈残差模块”（bottleneck residual block）效果更好。这是因为，ResNet 的瓶颈残差模块由 1x1、3x3 和 1x1 的卷积层依次堆叠而成。其中，1x1 的卷积层可以起到降维或升维的作用，使得 3x3 的卷积操作可以在相对较低维度的输入上进行，从而提高计算效率。

![](https://static001.geekbang.org/resource/image/b4/78/b466fe3b91edb73dc55f8c9407aa1478.jpg?wh=3900x1859)

ResNet 这篇论文成为了 CV 领域乃至整个 AI 领域引用最多的文章之一，它的价值可想而知。

ResNet 的成功为深度学习领域带来了革命性的变化，随后基于ResNet诞生的 DenseNet、Inception 等网络结构的层数越来越深，参数量过亿已司空见惯。

超大参数模型的出现，让 ImageNet 这类公开大规模数据集发挥了未曾预料的作用。之后又有越来越多普惠的视觉 PTM 发布出来。

所谓的大模型技术在这时已初显雏形，这些工作都为后续大模型发展奠定了坚实的模型工程基础，积累了宝贵的实践经验。

## 视觉算法的社会价值

刚刚我们学到的视觉大模型的发展，对预训练模型技术的推动作用不容小觑。然而，视觉模型的高速商业化进程，也给视觉预训练技术的发展带来了重要影响。

十二年前，几名来自清华大学的学生共同创立了四小龙中最早成立的旷视科技。这时以人脸识别算法（旷世的英文名就是Face++）为代表的视觉算法进入了快速发展阶段，更多的人开始直观地感受到人工智能技术的价值，各种AI概念的创业公司陆续涌现。

那时社会上几乎所有的 AI 领域投资都在向 “产业互联网 = 视觉算法 + AIoT” 的模式靠拢，商业需求这只巨大的手推动着视觉算法的发展。

然而，产业互联网的算法研发模式和之前大不相同。例如，推荐算法只要有一个固定的算法团队，对若干场景的模型做长期稳定的调优即可。

但这在产业互联网中则不行。例如，如果把为银行准备的视觉安防模型拿给餐饮店去使用，这肯定不现实。因为客户对数据隐私保护的需求十分强烈，你不能使用各个客户的数据去交叉训练同一个模型。

因此，为了满足特定的需求，算法团队每天都在重复使用新场景的数据来训练定制的模型。这将带来无尽的重复工作，消耗本来就稀缺的算法工程师资源。

如果这时候你能够通过单一模型快速适应新场景，就能帮你的公司赢得巨大的成本优势。于是，研发人员开始广泛采用 **公开数据集 + 大模型预训练的方法，配合领域微调的技术**，用来降低面向不同业务场景定制化开发的成本。

因此，随着产业互联网的发展，大模型预训练+领域微调这种方式，逐渐成为了视觉算法研发的主流。

## 视觉预训练模型原理

接下来，我将以人脸检测算法为例，带你学习视觉预训练模型的特点。

预训练大模型在视觉领域兴起还有一个至关重要的因素，那就是预训练技术在视觉领域的可解释性更好。你可以先结合后面这张图观察一下。

![](https://static001.geekbang.org/resource/image/bc/48/bc7b70a235134d3c8673948b44505248.jpg?wh=3900x2194)

可以看到，这个人脸检测算法的构成其实是有一定的分层特点的。

- 左侧的底层网络正在构建基础的“边缘检测能力”。
- 中层的网络则利用这些边缘信息，来组成更高层次目标的局部特征，中间这张图显示的五官信息，就是一个典型的表现。
- 最上层则会出现和我们的目标分类任务最相关的一些实体，在这个例子中就是人脸的内容。

这种分层结构非常适合用来做模型的领域微调，只需要使用较少的数据，更新最上层也就是负责分类功能部分的模型参数，就能够获得良好的效果。

在搞清原理之后，后面模型领域微调和我们上一章学过的模型训练没有太大的不同。

这里我只提一个关键点，假设你已经在开源社区得到了一个使用 ImageNet 训练好的预训练模型，你需要将最后一层的连接替换成你的输出格式再进行训练（详见下图和代码）即可。当然你也可以再往后面加上几层网络，来增加模型的深度和拟合效果，这个留作你的课后作业进行练习。

![](https://static001.geekbang.org/resource/image/0f/ef/0fe143023ba562a87fe0700efce2d8ef.jpg?wh=3900x1525)

```python
# … 加载数据

# 加载预训练的ResNet-50模型
model = models.resnet50(pretrained=True)

# 替换最后一层全连接层
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, len(image_datasets['train'].classes))

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# 将模型迁移到GPU上（如果可用）
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# … 训练模型

```

虽然随着大模型的发展，针对各种大模型微调的工具越来越多，在 AI 大模型上已经不再直接使用这种方式去微调（其实也可以就是费钱），但是总体的思路是完全相同的。在下一章中，我将带你学习如何高效地微调一个大规模参数的 AIGC 模型，敬请期待。

## 小结

学到这里我们做个总结吧，其实计算机视觉预训练大模型的发展受到了商业和技术因素的双轮驱动。

- 在商业上，人脸识别等人工智能应用的发展，吸引了许多投资者对相关产业进行投资建设，提供了持续的需求，也促进了成本更低和更准确可靠模型的不断推出。
- 在技术上，ResNet 和 ImageNet 等视觉领域的创新发展，使视觉大模型的训练成为可能，为视觉大模型的出现提供了必要条件。

除此之外，你还学习了模型预训练相关的知识。

- 首先，在预训练阶段，你需要让模型学习通识知识（如ImageNet），为后续专业知识的学习打下基础。
- 随后，在微调阶段，你需要为模型提供各个专业领域的定制化知识（如人脸识别），这就像高中的文理分科教学，或大学阶段的专业学科教育一样。

事实上，像 ChatGPT 这样的 NLP 预训练大模型，也是通过使用大量通识教育数据先训练学习基础的“语言知识”，然后通过各个领域的专业资料学习“世界知识”。

这听起来似乎很简单，但将视觉预训练模型的这些经验应用到自然语言处理（NLP）领域经历了漫长的过程。你可以先猜一下它是如何实现的，接下来的课程里，我将为你揭晓答案。

## 思考题

预训练模型和大模型之间的关系是什么？

恭喜完成我们第 11 次打卡学习，期待你在留言区和我交流互动。如果你觉得有收获，也欢迎你分享给你身边的朋友，邀 TA 一起讨论。