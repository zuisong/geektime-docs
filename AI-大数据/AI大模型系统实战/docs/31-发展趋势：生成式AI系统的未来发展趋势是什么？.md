你好，我是 Tyler。

前面的两节课，我们深入研究了OpenAI的创新历程、国内大模型在产业界的发展趋势以及其背后的技术和商业逻辑。

今天，我将结合自己的观察，与你分享人工智能的发展趋势，我总结出了以下几个关键趋势，分别是多模态、混合专家模型和模型小型化。

## 多模态模型

我们先来说一下第一个趋势——多模态模型。

为了理解这一趋势，我们需要先了解大模型的行业现状。正如我们之前提到的，微软收购了OpenAI，你有没有想过，他们该如何才能收回这笔庞大的投资？

答案很简单，他们需要进一步地商品化OpenAI，基于OpenAI先前积累的用户和技术能力，建立微软的商业壁垒。然后再通过这一壁垒创造超额利润，以收回成本。

这是一个可以预见的必然趋势，从技术角度来看，这是一个迎合商业需求的过程。OpenAI现在需要忘记早期由马斯克制定的“造福人类，改变世界”的愿景，只想尽早将大模型这项无休止投入的技术纳入商业闭环，把用户需求当作牵引，让技术更加健康地发展。

为此，他们必然要更大规模的商业化。而想要追求这个目标，就要完成这三个主要任务：第一个是增加用户规模，第二是增加用户使用时长，第三个任务则是提高付费用户的转化率。只有这样才能增加每个用户的生命周期价值（LTV），扩大业务的营收和利润。

前两个任务最直接的办法，就是让用户在使用你产品的时候“变懒”或“变笨”。比方说啊，如果大模型技术可以处理文字、图像、视频、音频，还有各种办公文档等日常工作中常用的信息格式，自然会吸引更多人使用ChatGPT。

了解了这一逻辑后，我们可以理解为什么GPT的下一个发展方向是多模态领域。这也解释了为什么OpenAI没有急于推出GPT-5，而是首先推出了GPT-4V。

从OpenAI GPT-4V的 [那篇报告](https://cdn.openai.com/papers/GPTV_System_Card.pdf) 里，就证明了他们已经很好地解决了大多的视觉多模态的问题，那为什么他可以做得这么好呢？其实这件事的本质是一个视觉问答（Visual Question Answering，VQA）的问题。

该问题的早期方案都是基于两阶段的方式去做的，第一个阶段是图生文，第二阶段是文字理解。那图生文做得最好的方法当然也就是OpenAI 的 CLIP，文字理解最好的模型则是GPT-4。这个过程只不过是整合他们两个本已具备的一些能力。稍微概括一下就是，前期的技术积累让OpenAI当仁不让地成为了多模态模型的佼佼者。

当然，我们看到OpenAI在论文中介绍了一类视觉任务，他们处理得非常不好，他们自己也没有避讳去说这件事情，那就是复杂图像的理解。

这也证明了GPT-4V虽然可以解决80%的问题，但是他们没有精力和资源去完成剩下20%的事情，这也给各个领域的创业者，或者是垂直领域的工作者，留下了一些空间。

大象踩不死蚂蚁，你完全可以结合你的领域知识，在你的领域当中去做这种复杂图像的理解，这个是GPT目前所不能完成的，比如在生物医疗领域的复杂图像理解，就是一个待解决的问题，也是一个很有价值的创新业务方向。当然这里我只是举一个最直观的例子，相关的场景还是需要你自己去发掘。

## 混合专家模型

我们都知道，原始OpenAI的GPT模型已经非常庞大，推理成本很高。前面提到的多模态和商业化的双重背景将导致推理成本进一步上升，多模态功能的加入会增加模型参数，提高单次推理成本。进一步商业化的要求也会扩大用户规模，增加服务调用次数，提高推理需求总量。

两个因素的共同作用下，大模型技术的开销与日俱增，甚至可能到达目前技术所不能支撑的一个程度，或者说这个成本没有任何一家公司能够承担。

因此，大模型也进入了我们在之前课程中提到的“分久必合，合久必分”的阶段。为了让大模型可以支持更多垂直领域，更好地回答用户特定问题，OpenAI也进行了拆分大型原始模型的工作。根据谷歌的公开研究报告，GPT-4实际上已经成为混合专家模型。当然这是包括我在内的一些AI工作者之前也都在猜想的一个事情，不过谷歌的 [这一篇文章](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure) 帮助大家印证了自己的猜想。

这对于各个领域的创业者和垂直领域从业者都是好消息，因为这证明了在目前的阶段，很难出现一个通用大模型来完成所有任务，“超级智能”还需要很长时间才能实现。这为各个领域的垂直大模型提供了创新的想象空间，让大家可以去发现各自领域中的蓝海，完成自己的创新和商业模式的构建。

我们在前面的课程中提到了，因为在一些相关度不高的任务上，很难增加“涌现”的效果。因此，对于这些隔离度较高的任务，我们可以使用不同的领域模型来完成工作。

比如，将文生图、内容理解和内容摘要的模型做成一个模型，当然是可以的。但是，如果想让这个统一的模型达到生产级的效果，它的参数规模至少要达到 100B。

这三个任务如果分别使用三个 80B 的模型来完成，就可以节省大量的推理成本。拆分主要有两种方法，第一种是根据领域专家的知识来进行拆分。除此之外，还有一种方法就是让模型自己去决定如何拆分，自动生成一些可解释性不是那么强的专家模型来完成工作。

可以看出混合专家模型的专人专事模式会带来两个好处。

- 第一个好处是，提高了每个模型的专业性，这往往会带来模型表现的提升。
- 第二个作用是，降低了“用人标准”，也就是每个模型参数量的降低，减少单次推理成本。

大模型技术作为一个早期的技术，还没实现普及和大众化，因为成本高昂，业界也正在探索除了混合专家模型之外，进一步降低技术成本的方法，下面就来详细说说这些方法。

## 大模型技术平民化

商业关注的重点在于降低成本，而模型小型化技术的目标就是优化硬件成本，降低推理功耗，因此我也比较看好大模型小型化的这个趋势。

其实，模型小型化并不是一个新领域，这项技术在人工智能领域已经广泛使用了多年。

最初，这项技术主要用于物联网设备，如边缘计算设备，因为在这些算力紧缺的设备上运行模型推理任务非常困难。最早期的模型小型化方法，主要集中在将中等规模的模型裁剪成更小规模的模型，以便在物联网设备上使用。

这和将大模型缩小到中等规模模型的任务之间存在一定的差异，但在本质上，使用的技术和方法是相似的。这也是为什么大模型刚刚推出后不久，就出现了各种模型小型化方法，使人们能够在笔记本电脑甚至树莓派这类设备上运行大模型。

在未来很长的时间内，模型小型化都还会是让大模型技术普适化的主要技术，模型小型化技术还有许多待开发的空间，包括训练中量化、剪枝和压缩等一系列AutoML技术，异构计算的技术，还有大模型的架构和硬件架构的co-design技术，这些都是未来重要的发展方向。

即使你只是该领域的应用者，也需要时刻关注可以降低商业成本的模型小型化方法，这会给你带来实际的成本优势，让你悄悄开挂，超过他人。

## 总结

ChatGPT成为家喻户晓的用户产品后，它已经无法脱离用户需求和商业业绩的限制。尽管GPT技术是一个令人振奋的技术人的胜利，但即使它生而自由，如今也无往不在商业的枷锁之中。新技术孕育了新的商业模式，商业收入反哺于技术研发，这是大家在互联网行业蓬勃发展的这些年中都无法打破的客观规律。

因此，ChatGPT被收购后，首要任务是如何满足更多用户的需求，以确保其技术优势和市场优势能够转化为商业收入，然后像一切“伟大”的公司一样，建立护城河，封堵后来者。为了更好地接近客户，ChatGPT必须让客户在使用产品时变得更便捷或更简单。因此，多模态大模型成为最迫切的功能需求，这也是OpenAI推出GPT-4V的原因。

然而，我们也必须认识到GPT-4V的局限性。多模态功能会增加模型训练和推理成本。所以，我们需要思考如何进一步降低这些成本。而我们谈到的一系列模型小型化技术，正是为此服务，这些技术包括混合专家模型、蒸馏、量化、剪枝、压缩等等。

![](https://static001.geekbang.org/resource/image/ef/f5/ef7f68764639a2084db86d060bd8aaf5.jpg?wh=4000x2250)

## 思考题

我们知道，手机4G流量成本的下降催生了短视频产品的蓬勃发展，当大模型的使用成本可以忽略不计时，你预测会有哪些应用横空出世？

恭喜你完成我们第 31 次打卡学习，期待你在留言区和我交流互动。如果你觉得有收获，也欢迎你分享给你身边的朋友，邀 TA 一起讨论。