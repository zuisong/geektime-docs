你好，我是Tyler。

还记得第一节课的思考题吗？我们再回顾一下。

> 前一段时间，马斯克曾联合上千位人士签署联名公开信，“以担心人工智能系统将达到不可控程度，且会造成不可预知的风险为由，呼吁暂停训练更强大的人工智能6个月”。你认为他说得对吗？他所说的不可控的程度是什么？

你可能会问，为什么我突然提到了之前的问题？其实前面思考题反映了我们对大模型的“智能”非常担心。担心的主要原因是，现在还没有人真正了解大模型产生智能的原理。

首先，目前的智能反应是难以预测的，这使得模型的行为变得不可控。其次，这些能力可不总是朝着有益的方向发展，可能会引发不良后果。

因此在学术界产生了各种各样的猜想和实验，试图弄清楚大模型为什么会产生智能。毕竟长远来看，如果每个数字应用都发展为智能体理论上是可能的。就算使用大模型技术时都会有风控策略的参与，但是很难避免大模型首先服务人类，之后随着时间拉长开始互相对话建立共识，最后架空人类。

只有解开大模型智能来源的谜底，我们才有可能牢牢“控制”大模型，避免有坏人利用它做坏事。这节课，我们就一起来探寻大模型智能产生的机制。

## 涌现是什么

想要推测大模型为什么会产生智能，我们需要先了解“涌现”这个概念，它是大模型区别于之前人工智能方法的最鲜明特征。

维基百科里是这样定义“涌现”的，我觉得定义得比较准确所以这里引用一下。

> 涌现（英语：emergence）或称创发、突现、呈展、演生，是一种 [现象](https://zh.wikipedia.org/wiki/%E7%8F%BE%E8%B1%A1)，为许多小实体交互作用后产生了大实体，而这个大实体展现了组成它的小实体所不具有的特性。

涌现在复杂系统理论中起着核心作用。例如，生物学中的生命现象是化学的一个涌现，或者足够多的蜜蜂聚集在一起可能形成“蜂巢”的结构，足够多的人聚集在一起，可能逐渐形成“国家”的实体。这些都是涌现的例子。

同样地，足够多的神经元和数据的聚集也可能产生智能。当模型的参数和数据量达到一定水平时，它们才开始表现出人们难以预料的能力，这些能力似乎是 **在模型内部自发地涌现出来的**。

## 参数规模的影响

你也许听说过“可控核聚变”，如果人类能够掌控这项技术，就能让恐怖的核能为我们所用，大幅地降低能源使用成本。

搞清楚智能涌现的原理，其实就是为了推动AI领域的“可控核聚变”。显然，涌现的出现和模型参数的规模有着直接的关系，随着大语言模型的规模不断增长，参数规模对下游任务的影响也会变得更加显著。

我们观察到不同类型的任务在参数增加时，表现出了三种不同的特征，分别是伸缩、涌现和U形曲线效应。

### 伸缩法则

首先我们来看 **知识密集型任务。** 这些任务主要会考察名词解释、概念掌握，不太需要推理能力，因此对模型内蕴含的知识量要求较高。

为了理解伸缩法则，我们先来看一下现在大语言模型的“智力”是如何记忆在模型参数中的。大语言模型从海量自由文本中学习了大量知识。这些知识分为语言和世界两大类，包括语法、语义、历史事件等。这些知识的分布与模型的深度有关，随着深度的增加，模型能够学习到的知识数量也会增长。

在论文 BERTnesia: Investigating the capture and forgetting of knowledge in BERT 的结论中也能看出，不同的知识在 Transformer 的存储是存在分层特点的，这与我们在视觉预训练模型 [那节课](https://time.geekbang.org/column/article/692796) 学到的人脸识别算法的分层特点很像。

![](https://static001.geekbang.org/resource/image/4c/5b/4c69ae5736c556ddfa70c01b0b96b15b.jpg?wh=3900x2194)

在论文 Scaling Laws for Neural Language Models 中，OpenAI 提出了“伸缩法则”（scaling law）。如下图所示，大语言模型性能分别受训练时长、模型参数量和数据集大小的影响，而大模型性能与每个单独的因素都有正相关性，体现为 Test Loss 的降低，这意味着模型性能的提升。

![](https://static001.geekbang.org/resource/image/a8/b3/a84b60e64843fe7020bc256d9b5276b3.jpg?wh=3900x1964)

不过，这些结论目前都还符合我们的“常识”，那就是 More Pain, More Gain。而我们更感兴趣的还是“大力出奇迹”中的那个“奇迹”。我们接下来要讨论的大模型的涌现能力，其实就是那个“奇迹”的代名词。

### 涌现能力

涌现能力的典型特征就是，在小规模的时候，模型基本上没有任务解决能力，而在模型规模达到一定的临界值之后，模型能力就会迅速提高。这种现象，在处理多步骤复杂任务的时候尤其明显。这种新能力的涌现，让模型在处理复杂任务的时候，具备了更高的性能和解决能力。

你可以在下面这张图中看出，各项任务的性能都在参数规模达到了 10^10 时发生了骤变，攀升到了一个很高的数值。这便是涌现在模型参数量和性能的相关性数据上所呈现的特征。

![](https://static001.geekbang.org/resource/image/06/ca/06d7ccdd908dea8e135aff21df30deca.jpg?wh=3900x2194)

### U形曲线效应

Google 在 Inverse scaling can become U-shaped 提出了 U形曲线的任务，研究人员们开始对下图中这种奇怪的现象进行研究。

![](https://static001.geekbang.org/resource/image/d8/3f/d873bdffa9c16360d3338a6e1ab53b3f.jpg?wh=3900x1915)

这类工作非常具有研究价值，因为模型性能随着参数规模上升而下降的现象，很可能会让大多的研究人员 **过早地错误放弃掉** 一些有价值的研究内容。

如果我们能搞清楚哪些场景下有“再坚持一下试试”的必要，那将有效地提升研究人员的工作效率，加快新理论的发展速度。

Google 的研究人员初步发现了这类任务的特点，这类任务往往在内部包含了两种不同类型的子任务，一种是真正的任务，另一种是干扰任务。

这有点像是英语的听力题，里面经常会增加一些干扰的内容，让你做出错误选择。比如论文中下图的例子，大模型还很小时，可能会直接接受提示词中给出的错误内容，盯着 5 美金和 50 美金绕圈子，识别不出其中的逻辑关系，导致给出的结果还不如随机生成的值，进而掉落到 U 形的底部。

![](https://static001.geekbang.org/resource/image/e2/af/e2ca662def15260872dd359e4caf68af.jpg?wh=3900x2194)

但是如果模型的规模进一步变大， 则会绕过这个“陷阱”，识别并忽略干扰任务，从而提高真正任务的准确率，远超最初的随机结果。

研究人员经过多组类似的观察后归纳总结，将这种随着模型规模的增长，刚开始模型效果下降，但当模型规模足够大时，效果反而会提升的现象称为“U形曲线效应”。因为如果把参数规模作为横坐标，把模型性能作为纵坐标，那么就会出现一个U型曲线。

在包含多种子任务，而且模型规模较大的情况下，就会经常出现“U型曲线效应”。

## 涌现任务的特点

涌现能力是指大语言模型在特定任务上，表现出超出其训练数据范围的能力。

接下来，我们就来了解两个最典型的涌现能力表现——ICL和CoT。

- In Context Learning (ICL)：大语言模型从少量的示例中学习，而无需微调参数的能力。
- 思维链 (CoT)：大语言模型能够理解和执行复杂的推理过程。

### ICL

这是我们上节课学到的一种技术，ICL许大语言模型从上下文示例中学习。在 ICL 中，我们只需给模型提供几个示例，它就可以在许多下游任务中表现出色，甚至超过了经过监微调的小型模型。

它的工作原理仍然是个谜。有学者认为 ICL 无法从示例中学习到任何东西，而另一些学者则认为大模型可能会进行一种隐式学习。虽然他们都还没有说服对方。但是目前都在一个结论上达成了一致，那就是具有涌现能力的模型，大部分都具备 ICL 能力。

### CoT

CoT 本质上也是一种 Few-Shot 的提示方法，它允许用户为复杂的推理问题提供详细的推理过程，并将其提供给大语言模型。这使得模型能在执行复杂的推理任务（包括数学问题和符号推理等）过程当中表现出色。

![](https://static001.geekbang.org/resource/image/8b/8e/8bfd624e382c5d09da23746d7dd3168e.jpg?wh=3900x2120)

CoT 的核心思想是将大问题分解成小问题，并一步步进行推理，而不是一次性解决整个问题。通过提供详细的思维链提示，可以引导模型给出思考过程，这样可以提高推理的准确性。

CoT 已经在各种领域取得了成功，特别是在数学推理方面。例如，GPT-4 可以从 CoT 提示中学习如何解决复杂的数学问题，如微积分和线性代数。

### 涌现能力的证明

ICL 和 CoT 都是大语言模型具有涌现能力的证据。然而，为了证明涌现能力，主流的 LLM 大多采用了人类最古老的方法，那就是“考试”。

GPT-4 已经在很多人类的考试中取得了高分，比如 SAT、AP、GRE 等等，甚至还通过了模拟律师考试，分数在应试者排名前 10% 左右。这当然是对涌现能力最直观的证明。

ChatGPT 是一个典型的涌现能力模型。它可以通过与用户的互动，学习和适应用户的语言习惯和偏好。ChatGPT 还可以生成不同的文本格式，如诗歌、代码、脚本、音乐作品等。

总而言之，涌现能力是大语言模型的一个重要特性。涌现能力使大语言模型能够在特定任务上表现出超出其训练数据范围的能力。

## 涌现智能的开关

那么，多大的模型规模才能出现涌现现象呢？

根据 Google、Stanford 和 DeepMind 的研究，经验判断是模型参数至少需要达到 68B（十亿级别）的基本门槛，最好超过 100B。

不过，这与任务类型和具体模型有关。在 ICL 情况下，一些简单任务，如三位数的加减法，只需要 130 亿参数，而较复杂的任务，如多义词判断，则需要 5400 亿参数。总之，68B 是一个基础门槛，目前效果最好的大语言模型通常超过 100B。

然而，大模型的训练成本是相当高的。如果把模型做得太小，又可能无法展现涌现能力，造成资源的浪费。

对于这个两难问题，DeepMind 的 Chinchilla 为我们提供了一个很好的实验。把模型的参数模型控制在了 70B，随后通过增加数据量，在性能上达到了大模型 Gopher 相似的效果，大幅降低了推理成本和部署门槛，让大模型可以在更小的硬件上实现应用。

![](https://static001.geekbang.org/resource/image/9b/9c/9babab1689a46a12ca486d8yy07d479c.jpg?wh=3900x2105)

这引发了一个核心问题：如何分配算力？显然答案并不唯一：

- OpenAI 的结论是优先增加模型参数，然后再增加训练数据量。
- DeepMind 认为，训练数据量的重要程度并不排在参数量之后。
- Meta 则是直接动手，使用更小参数的 LLaMA 模型和更多的训练数据勾调出了涌现能力。

这些研究表明，在给定算力预算下，小型模型在更多数据上训练也可以实现不错的性能，而且还能降低推理的成本。因为当前大模型的参数可能还没有得到充分的利用，所以将模型做小并充分训练，然后逐步增加模型规模，似乎是一个可行的办法。

## 总结

学到这里，我们来做个总结吧。

今天，我们学习了模型参数对模型的影响有哪几种，以及涌现智能任务的特点，还有我们对“涌现”所需要参数量的思考。

我个人对弄懂涌现的原理并不抱太大的希望。自从深度学习技术诞生以来，对于它的可解释性的研究就一直没有停止。然而，从目前的观察来看，这明显是一个“三轮车追汽车”的过程。

![](https://static001.geekbang.org/resource/image/92/4d/92133d3ae76f4052a5dcf8dd65620a4d.jpg?wh=3900x1626)

理论的可解释性已经因为技术发展的速度越落越远。或许人类永远也不会得到“涌现原理是什么”的答案。但这可能恰恰成为了大模型技术可以走得更远的有利条件。

说到这里多聊几句题外话，可能恰恰 **因为大模型足够复杂，太过难以被理解，反而永葆生命力。** 这就好比，如果有一天我们解开了人类基因的全部密码，或者真正了解了地球上第一个生命的诞生原理，这并不一定是一件好事，甚至可能会让人类面临前所未有的挑战和灾难。

就好像奥本海默领导研发的原子弹，看似是人类科技进步的一大里程碑。然而，这项“伟大”的成就，却让人类掌握了毁灭自己母星的力量。所以换个角度来看，“生命诞生”和“智能产生”这些事物难以解释的性质，反而为这个星球上的生命提供了最好的加密保护，让他们免遭针对性的破坏。

这一章的课程告一段落，此刻你已经掌握核心的大模型理论了。下一章，我们将利用前面学习的所有知识，来构建一个AI大模型系统，敬请期待。

## 思考题

1.在构建AI系统时，你会如何权衡模型规模和模型的训练数据量，你会选择用更多的数据和小模型，还是更少的数据和大模型？

2.你认为涌现任务会如何影响人工智能的应用领域？对于未来涌现任务的发展，你有哪些期待和疑虑？

恭喜完成我们第 17 次打卡学习，期待你在留言区和我交流互动。如果你觉得有收获，也欢迎你分享给你身边的朋友，邀 TA 一起讨论。